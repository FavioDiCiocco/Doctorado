------------------------------------------------------------------------------------------

22/03/2024

Al final terminé de revisar los archivos del barrido Beta-Kappa, sólo había problemas en la iteración
90 a 99. Así que tendré que en Oporto mandar a correr dos conjuntos de datos separados. Uno que recorra
todo el espacio para iteraciones entre 90 y 99 y otro que recorra el espacio para Beta mayor a 1
para iteraciones entre 50 y 69.

Voy a aprovechar la mañana para preparar el main cosa de que cuando se termine la corrida de Oporto,
que va a terminar en uno o dos días, ya lo pueda mandar a correr rápido. Después de comer me pondré
con el análisis de la ecuación dinámica del modelo. Bien, ya modifiqué el Instanciar. Lo que tengo
que hacer es mandar las simulaciones de 50 a 69 en Oporto mañana a la tarde.

------------------------------------------------------------------------------------------

26/03/2024

En la mañana fui a llevar el documento de matrimonio de mamá al estudio, pero al final el documento
no estaba actualizado. Tendré que conseguir la actualización del registro civil y de ahí
volver a intentar.

Las simulaciones en Algarve terminaron, así que podría mandar a resolver las simulaciones entre
80 a 99.

Hoy la verdad me agaró un sueño imposible, no avancé mucho. Pero estuve mirando lo que me pasó Hugo
y lo que tengo yo de la cuenta, logré armar la misma expresión para la derivada de la perturbación.
Queda entonces ver dónde queda efectivamente el valor de Beta de transición. Lo interesante es que
aparece un factor 2 que antes no estaba. Vamos a ver cómopuedo hallar entonces el valor de Beta de
equilibrio.

------------------------------------------------------------------------------------------

27/03/2024

Lo primero que hice en la mañana fue revisar mails, después mandé a correr las simulaciones entre
90 y 99 en el barrido beta-kappa en Oporto. Con eso voy a tener todas las simulaciones en ese espacio
y voy a poder armar los gráficos con la estadística de 100 simulaciones. Eso debería durar de acá a
la semana que viene. Quizás.

Mientras estoy haciendo el análisis de la ecuación dinámica para definir el Beta tal que el estado de
polarización ideológica transiciona de ser inestable a estable.

------------------------------------------------------------------------------------------

04/04/2024

En la mañana revisé las carpetas de Oporto y demás para ver que todo estuviera corriendo bien, mandé
a armar gráficos en Oporto ahora que tengo las 100 simulaciones por punto y estuve intentando pensar
cómo armar el gráfico que Pablo me pide que haga sobre el espacio Beta-Kappa.

Mientras tanto, se siguen resolviendo las simulaciones que faltan del espacio reducido Beta-Cos(delta).
Esas simulaciones van a terminar cerca de tres o cuatro días.

Lo que tengo que hacer ahora primero es ver cómo armar un gráfico que muestre clarametne la composición
de estados en el espacio Beta-Kappa. A partir de eso, podría pensar un mapa para el espacio de
Beta-Cos(delta).

Hoy apenas miré las cosas, no hice mucho más. La verdad, siento que perdí buena parte del tiempo. Dios,
como odio estos días. Y eso que vine temprano y todo.

------------------------------------------------------------------------------------------

05/04/2024

Hoy estuve todo el día armando los gráficos de distribución de estados en el espacio de parámetros.
Logré hacer los gráficos tanto para el espacio Beta-Kappa como para el espacio Beta-Cosd.
Siento que no son la gran cosa, pero Pablo me dijo que haga algo así, o mejor. Yo saqué esto.
Tengo que ser mejor que esto, espero estar mejorando con el tiempo, no estar estancándome.

Bueno, no nos matemos, mañana partida, el domingo tarea de Alemán y del curso de aplicaciones
a becas. Lunes retomamos con ganas y fuerzas.

------------------------------------------------------------------------------------------

08/04/2024

Hoy llegué más o menos temprano, los trenes están tardando mucho ahora, temas de despidos.
VLLC. A la mañana voy al curso de inglés. Revisé las simulaciones en el espacio reducido.
Ya casi están todas terminadas, queda una sola en la carpeta de algarve. Así que después de
comer, junto todas las simulaciones de Algarve en Coimbra y desde ahí armo los gráficos del
parámetro de espacios.

Veamos qué simulaciones tengo en cada pc. En Coimbra tengo dos carpetas.
) Datos: 0-9.
) Zoom_beta-Cosd: 30-39, 50-79.

Algarve.
) Zoom_beta-Cosd: 10-29, 80-99

Oporto.
) Zoom_Beta-Cosd: 40-49

Ahora lo junto todo. Ahí mandé a correr para que se armen todos los gráficos del espacio reducido.
La pregunta es qué hago ahora. Para mi tiene interés revisar qué es lo que pasa con el sistema
para el punto fijo de cuatro extremos, ese análisis de la ecuación dinámica, que creo que
razonablemente dará que Beta crítico es 1, yo creo que será una investigación interesante.

Otra cosa que puedo hacer esperando a la reunión con Pablo, es armar la presentación para el
miércoles. Aunque eso lo puedo hacer mañana. Lo tercero sería ponerme a leer algún paper en
el tiempo hasta juntarme hoy con Pablo.

Hablé con Pablo, la idea es armar los gráficos indicando las regiones no con colores, sino delineando
los contornos, usando números romanos para indicar las regiones, luego colocar cerca gráficos que
indiquen estados finales característicos y explicar cómo es que terminé definiendo esas regiones.
En particular, si habría alguna forma de identificar mejor la curva que surge en el espacio Beta-Kappa,
la cuál yo definí puramente a ojo.

Ahí armé los gráficos que me dijo Pablo, me falta colocarle los números romanos al segundo. Mientras
se están terminando los gráficos en la región reducida. Tengo mala espina.

------------------------------------------------------------------------------------------

09/04/2024

Revisé Coimbra, se hicieron los gráficos del espacio reducido. Se ve algo interesante respecto
de los estados con anchura, y se ve que coinciden los resultados con lo observado en el espacio
total Beta-Cos(delta).

Sigamos con el trabajo de Pablo hasta eso de las 11 y algo. Después a las 12 voy a comer, a las
13 voy al abrazo en el 0+inf y después a cursar.

Terminé de cursar, trabajé un poco en la presentación y después me fui a casa temprano para que
no me agarre la lluvia.

------------------------------------------------------------------------------------------

10/04/2024

A la mañana llegué y mandé a armar gráficos de Histogramas 2D. Después tuvimos la reunión de grupo.
Comí en la oficina y rápido terminé de armar lo que pude de la presentación para la
gente de GOTHAM. Pablo hoy no pudo venir, lo mismo Jesús. La charla la tuve yo con Hugo
y Mario.

De la charla saqué los siguientes puntos:
1) En el espacio de parámetros Beta-kappa, la curva que diferencia estados de Consenso Radicalizado
respecto de los estados Polarizados, en principio Hugo la estudió bastante desde varias perspectivas.
Su conclusión es que es un resultado de la saturación de la función de tanh. Una propuesta que
me hizo es intentar ver si reemplazando la función tanh por una función signo, entonces
debería ver si esa curva desaparece y sólo veo una línea constante en Beta aprox. 0.2.
2) Hugo me dice que esos estados de polarización descorrelacionada que estoy observando
si los dejara correr más tiempo, posiblemente vería que convergen a otro tipo de estados,
quizás Pol 1D con anchura o Pol 1D nomás, yo no creo que sea el caso. Vamos viendo.
3) Algo interesante sería intentar ver si el modelo 2D para el caso de Cos(delta) = 0 se puede interpretar
como dos modelos unidimensionales independientes. Una forma de comprobar eso sería tomar un
punto del espacio de parámetros en el cuál surjan estados de polarización descorrelacionada
y comparar esos estados con estados polarizados en el caso unidimensional. Si el caso 2D con
Cos(delta)=0 se pudiera interpretar como dos simulaciones independientes, una para cada tópico,
entonces lo que debería poder observar es que la polarización simultánea de ambos tópicos debería
ocurrir con una probabilidad que es la probabilidad al cuadrado de que cada tópico polarice. Y
eso se debería obtener a partir de realizar simulaciones 1D. Para ellos esto es una pregunta
interesante respecto de cómo el modelo 2D no es sólo el modelo 1D 2 veces, sino que trae algo
propio a la formación de opiniones por la distancia entre opiniones. (Creo)
4) Cuando vaya a hacer los cálculos analíticos, probar hacer unas simulaciones primero, para
ver que efectivamente lo que quiero ver se da como yo planteo y no perder un montón de tiempo
en cuentas que no llevan a nada.
5) Al hacer las simulaciones con las que vamos a trabajar, no guardar todos los datos, sino guardar
sólamente el histograma de datos. Y considerar que ese histograma de datos tiene que tener
una cantidad de cajas que coincidan con la cantidad de cajas de las preguntas que voy a mirar,
porque algunas preguntas tienen 6 cajas y otras 7.

------------------------------------------------------------------------------------------

11/04/2024

A la mañana hablé con Pablo sobre lo que fue la charla con la gente de GOTHAM. El plan ahora
es armar una lista de cosas para hacer en estas tres semanas hasta la próxima reunión. Ahora
mismo no estoy realizando simulaciones. Debería considerar arrancar con eso, para que ya
esté funcionando antes de hacer el resto de cosas.

En la mañana estuve organizando cosas, mandando mensajes, imprimiendo comprobantes y así.
Mañana voy a ir a buscar las fotos de mi recibida. Para eso, tengo que llamar antes. Entonces,
a eso de las 14 llamo y después a las 17 salgo para allá, total es tomarme el tren hasta Retiro,
pedalear 10 minutos desde ahí y después volverme.

Hagamos la lista de orden de cosas por hacer esta semana y la que viene:
(Una idea que propusimos es ya fijar Kappa=10 y dejar de preocuparnos)
-----------------------------------------------------------------------

Es importante arrancar con esto porque requiere simulaciones, así que eso puede ir corriendo
mientras hago otras cosas.
########################################################################################################
1) Podría arrancar con las simulaciones del sistema usando la función signo en vez de la función
tanh. Eso me parece importante para arrancar porque va a requerir varias simulaciones. Digamos
que lo armo con 30 iteraciones, eso debería alcanzar para ver algo. Y el espacio que voy
a barrer debería ser de Kappa [1,10], con Beta de [0,1]. Después puedo ver de agregar
más Kappa, pero lo importante es hacer un barrido bien rápido de la cosa, variar grueso,
con Kappa variando de a 1 y Beta variando de a 0.1. Esto ya lo podría mandar hoy.
2) Empezar a descargar los datos de la ANES, ir construyendo un conjunto de datos con 10000
agentes cosa de tener para ir comparando nuestras simulaciones con los datos extraídos de las
encuestas. La idea sería armar datos en la región reducida de Beta-Cos(delta). Además, tengo
que asegurarme de guardar los datos, no guardando las opiniones de TODOS los agentes finales,
sino guardar el histograma final de los agentes. Hugo me dió un consejo de cómo guardar los
datos de forma de poder comparar mis resultados en el caso de que la opinión tenga 6 respuestas
o de que tenga 7.
3) Hugo me dijo de hacer simulaciones en un punto del espacio de Parámetros donde observe
varios casos de simulación y que por un lado corra simulaciones en el modelo 1D y por el otro
corra simulaciones en el modelo 2D. La idea es ver si la probabilidad de observar estados polarizados
2D es igual al cuadrado de la posibilidad de que polarice un estado 1D. Esto tendría alguna comparación
en el caso en que Cos(Delta)=0, siendo que en ese caso la ecuación del tópico 1 queda casi
completamente desacoplada de la ecuación del tópico 2. Hay todavía un pequeño acople en lo que
a los pesos refiere, así que no debería ser lo mismo. Digo yo. Esto puedo mandarlo por un lado
en Algarve y por el otro en Coimbra, con 10000 agentes. Creo que habíamos discutido que era
una buena idea usar esa cantidad de agentes.
########################################################################################################

4) Antes de continuar las cuentas analíticas sobre el valor de Beta que cambia la estabilidad de las soluciones,
conviene intentar primero hacer simulaciones con redes completas y con la pequeña perturbación
propuestas, para ver si el resultado corrobora lo esperado. Así nos damos una idea del resultado
final. Después de esto puedo retomar mis cuentas. Eso significa, PRIMERO, revisar que me quedó
una cuenta pendiente del caso en el que considero la solución ideológica. LUEGO, continuo la cuenta
del caso de polarización descorrelacionada.

Cosas más opcionales:
---------------------
5) Hugo me había dicho la otra semana que revise si pasaba que partiendo de un estado que yo
sé que finaliza en un estado de polarización radicalizada, al ir aumentando el cos(delta) el
estado se transforma en un estado de polarización ideológica. La idea está buena y no requiere
mucho esfuerzo. Creo.
6) Me dijeron de revisar si encuentro encuestas en Argentina que nos permitan observar
polarización, porque en las encuestas que buscaron en España, la mayoría no tiene casos de polarización.
7) Podría ver de continuar con el clasificador neuronal, aunque en principio no es ni de
cerca necesario ahora.

Después de haberme distraído un poco, resolví el punto 1. Cambié la función en el archivo de avanzar,
armé una nueva etapa llamada Func_sign y mandé en Oporto a correr este barrido en la región que 
determiné antes.

------------------------------------------------------------------------------------------

12/04/2024

A la mañana fui a realizar el trámite del Banco Provincia. Llegué a la facultad a las 13.

Ahora estoy revisando la lista de cosas por hacer. Entre lo que tengo que hacer, me parece que
es mejor empezar por el punto 3 y no por el 2. Más que nada porque el punto 2 va a tardar tanto
que empezarlo ahora o el lunes da lo mismo, el otro punto en cambio me parece que me va
a dar resultados más rápido.

Estuve muy distraído hoy, pero armé el programa que simula en 1D. Más tardé con el armar una función
que en C me construya mi histograma. No era tan difícil, sólo soy un boludo que está desatento.
Ahora tengo que subir esto a Algarve o Coimbra y mandarlo a correr. Fijate que esto no contempla
estar en una carpeta de src. Pero creo que no es problema, simplemente tengo que compilarlo y
mandarlo. Voy a tener que modificar el archivo de Instanciar quizás, pero nada grave. O el
de Metainstanciacion. Espero que cuando arme la función para el caso 2D esto sea más fácil.
Vamos para casa. No es temprano en realidad.

------------------------------------------------------------------------------------------

15/04/2024

Al final el finde no hice la tarea que planeaba hacer, pero no fue tan terrible, la semana
arrancó igual y estamos todos bien. A veces no tiene tanto sentido hacerse la cabeza con
eso.

Hoy lo que tengo que tener terminado es el armado de las opiniones en cajas al final de la simulación,
cosa de ya mandar eso y entonces de los siete puntos que marqué para hacer, tener ya dos
resueltos.
 Repasé la función para el caso 1D, ahí logré hacer que funcione. El caso 1D está planteado
para redes con N=1000, Beta=0.7, Kappa=10 y Cosd=0. Cada simulación tarda 215 segundos.
Así que si corro 30 simulaciones en ese punto, no debería costarme más de 20 minutos,
siendo que esas simulaciones las voy a repartir en 10 hilos. Después tengo que subir
el archivo de opinion.e a Algarve y desde ahí modificar el Instanciar para mandar a correr
la simulaciones 1D.

Ahora me pongo con las simulaciones 2D. Logré que compile. Mañana lo mando a correr en las
pc's de la facultad. O quizá hoy en casa si llego temprano y no estoy para jugar ranked.
Perdí un poco de tiempo en esto, pero al final del el armar estas funciones que me construyan
los histogramas en el código en C es importante para el siguiente punto a trabajar.
Es cierto que faltan dos cosas importantes:
1) Probar que la función esa esté laburando bien. MUCHO MUY IMPORTANTE.
2) Organizar correctamente el cómo interpreto los números que la cosa esta
me tira. Así identifico correctamente qué va dónde. Armar las cajitas de 6*6 o las de 7*7
va a ser un bardo. Bah no, mentira, va a ser una boludez.

------------------------------------------------------------------------------------------

16/04/2024

No puedo mandar a correr las simulaciones en la pc de Algarve, ya fue, lo corro en la pc
de la facultad. Mientras eso corre, mandé las simulaciones en Coimbra, eso no fue tan simple.
Había un error medio boludo en la función de clasificación. Ahora me voy a poner a estudiar
alemán. Tengo que después hacer caso a mis notas sobre revisar que todo funcione bien,
no puedo moverme a ciegas como estoy haciendo ahora. Si bien la función está copiada
de algo que funciona, tengo dudas.

Fui a cursar Alemán.

Mandé en la pc de Coimbra para realizar 1000 simulaciones. Eso debería tardar unos días.
Mientras, voy a necesitar simulaciones en la pc de la facultad que lleguen hasta 1000 también
de 1D.

Repito para que no se olvide, es mucho muy importante que pruebe que las funciones de Clasificación
están funcionando bien. Para eso voy a tomar algunas simulaciones, usar sus semillas y simular en
la pc de la facultad. A esas simulaciones les voy a guardar las opiniones finales, y las distribuciones.
Voy a comparar que las distribuciones me den lo mismo que las opiniones finales.

------------------------------------------------------------------------------------------

17/04/2024

Mandé en la pc de la facultad cuatro procesos para resolver 250 simulaciones en cada terminal.
En la pc de Coimbra ya está casi terminada la tanda de simulaciones para el punto del espacio
Kappa=10, Beta=0.7 y Cosd=0. En la carpeta de Oporto está terminado el barrido en el espacio
Beta-Kappa para la función signo. Así que ya podría trabajar en ver que efectivamente se arma
el gráfico que esperamos.

Yo creo que hoy tendría que hacer dos cosas. La primera es ver de descargar datos de la ANES,
así ya puedo mandar a correr datos. Lo segundo es ver que efectivamente la función Clasificación
funciona bien. Para eso voy a descargar algunas simulaciones, rearmar esas simulaciones en esta
pc usando las semillas, guardar las opiniones finales y comprobar que los gráficos que puedo
recrear usando las distribuciones que surgen de la función Clasificación son iguales a las
distribuciones que surgen de las opiniones finales.

Tengo unas dudas sobre qué hacer con el tema de los datos de la ANES. Vengo revisando mis archivos,
no encuentro algo de lo que me decía Hugo de tener cuidado al revisar los datos de la ANES.
Lo que voy a hacer, luego de debatirme y pensar qué corno quiero, es hacer gráficos de distribución
de opiniones confrontando todas las preguntas entre ellas. 

Estoy revisando las etiquetas. Labels_pre tiene 20 etiquetas mientras que labels_post tiene 9 etiquetas.
Juntos suman 29 etiquetas. Pero dict_labels tiene apenas 23 etiquetas. Es decir que de las 29 etiquetas
que mira, sólo 23 están identificadas. Debería esforzarme por identificar las otras 6.
Ahí vi, las seis que están removidas en labels_pre y labels_post son 6 preguntas en labels_pre
que tienen 4 respuestas, en vez de seis o siete. En labels_post sacaron la pregunta sobre si Obama es
musulman, pero no la de ubicación propia a lo largo del eje conservador/liberal. Supongo que para arrancar,
por hoy, puedo dejarla.

Observando los gráficos, encontré claros casos de: Consenso Radicalizado, Polarización Ideológica, Polarización
1D con anchura, Polarización descorrelacionada, y quedaría definir qué cuenta como casos con anchura
o algunas cosas más. Creo que lo que estaría bueno es poder definir qué pares de preguntas vamos a
revisar y con qué vamos a comparar esos casos. También vale notar que algunos casos resulta que tienen
algunas casillas vacías. Quizás para no hacer un barrido demasiado grande y poder observar estos
resultados, estaría bueno moverse con Beta entre [0.6,1.2] y Cos(delta) entre [0.05,0.15]. Y para
eso hacer un barrido BIEN grueso, total no busco con eso caracterizar alguna curva, sólo quiero tener
gráficos para comparar con lo observado en la ANES.

Voy a descargar entonces cinco archivos de Probas_Pol en 2D así ya mañana arranco con los gráficos
para revisar que la Clasificacion está funcionando bien. Descargué cinco simulaciones al azar, las
cinco pareciera que convergen a estados de Consenso Radicalizado. Voy a tener que armar las simulaciones
mañana usando esas semillas y ya después veo qué pasó.

------------------------------------------------------------------------------------------

18/04/2024

A la mañana fui a llevar el acta de matrimonio al estudio de traducción, y volví al mediodía. Después
me puse a ver los datos del barrido en el espacio Beta-Kappa. El gráfico pareciera dar lo que yo busco,
pero no se ve muy claro. Debería probar extender un poco la región y hacer un poco más fino el barrido.
Mañana lo hablo con Pablo cualquier cosa.

Ahora me voy a poner a estudiar Alemán. Después me pondré con lo de escribir la carta de motivación y el 
CV. Por último, con algo de tiempo, quizás lea un paper. Mañana repasaré el tema de las simulaciones
en el espacio Beta-Kappa, y de las simulaciones en un punto del espacio de parámetros.c

------------------------------------------------------------------------------------------

19/04/2024

Lo primero que voy a hacer es comparar el gráfico de Func_sign en el espacio Beta-Kappa
con el gráfico que tengo de esa misma región hecho en la etapa de opinión actualizada.
Para eso voy a tomar la función que arma el gráfico del espacio de parámetros de Entropía
y reducir la región de los ejes X e Y.

Mientras se resuelve eso, voy a probar lo de revisar que las funciones de Clasificacion
estén laburando bien. Aunque me descargué sólo gráficos con estados finales de Consenso.
Busquemos primero dos o tres gráficos en estados de polarización. No encontré estados de
polarización, pero encontré estados que tardaron mucho en resolver. ¿Estará funcionando
mal la función de Clasificación?

Voy a identificar los estados según su número de iteración.
.) Iter=100: Los agentes convergen a un estado de Consenso radicalizado, en el punto [10,-10].
Pero la función de clasificación dice que están todos en el lugar final. Lo cuál no tiene
sentido, porque si el valor de Y es -10, en la asignación el valor de "fila" debería ser 0
y por tanto la casilla marcada debería ser una de las primeras 42, no la última.
.) Iter=110: Convergen al punto [10,-10]. Van al mismo lugar que los anteriores
.) Iter=120: Convergen al punto [10,10]. Este sí le corresponde marcar en la última casilla
del vector de Distribución. Pero claramente parece que la función está funcionando mal.
.) Iter=130: Convergen al punto [10,10].

Es al pedo seguir con esto, efectivamente la función de Clasificación está mal, pero lo está
por un error muy boludo, y es que el ancho que usaba para ver si un número iba a una fila u otra
era 0, entonces al dividir por cero daba infinito y todos los agentes iban a parar al mismo
lugar. Así que eso está mal y tengo que rehacer todas las simulaciones corrigiendo simplemente
ese detalle.
 De paso, increíblemente, para el caso 1D sí hice bien el cálculo del ancho. Genial, eso significa
que eso está bien hecho y no necesito rehacerlo. Lo cuál es genial, porque estas simulaciones
eran más complejas de hacer porque no podía hacerlas en las pc's del cluster.

Voy a cerrar el día de hoy con tres cosas.
1) Mando a correr en Coimbra las simulaciones de un punto del espacio de nuevo. Esta vez corrijo
la función de Clasificacion para que funcione correctamente.
2) Mando en Oporto unas simulaciones extras de la región Beta-Kappa para intentar delinear mejor
lo que observo. La idea es expandir la región, no la cantidad de simulaciones.
3) Armo una carpetita con algunas simulaciones separadas del tema de datos de la ANES, así me queda
más sencillo para mostrarle a Pablo después lo de algunas distribuciones razonables.

Para el punto 2 hago un barrido extra, agregando valores de Kappa en los puntos intermedio.
1.5, 2.5 y así.

------------------------------------------------------------------------------------------

22/04/2024

A la mañana fui a la última clase del curso de "Becas:Help!". Tomé algunas notas sobre cómo
mejorar el CV y discutimos con los demás consejos para buen CV.

Lo que voy a hacer ahora es armar el gráfico del mapa de entropía para la función signo,
y después ver cuál es la proba de que un estado esté polarizado para el caso 1D y para el
caso 2D.
 Miré los archivos armados. Por motivos que no comprendo, faltan archivos, otros están armados
a la mitad. Hubo un corte, es imposible que no haya habido un corte.

Tengo descargados los archivos de agentes en un punto del espacio de parámetros, en los cuales
me guardé la distribución de opiniones finales, no las opiniones finales. Lo que tengo que hacer
ahora es con Python, levantar los datos, reordenarlo en el formato de una matriz y desde ahí
construir los gráficos de histogramas. Además de eso, lo otro que tengo que hacer es calcular
la proba de que un estado sea polarizado en el caso 1D y de que polarice en ambos tópicos
en el caso 2D.

1) Armo histogramas a partir de la distribución de opiniones.
2) Clasifico esos estados según los criterios definidos previamente.
3) Calculo la proba de tener estados polarizados en 1D y la proba de tener estados con
polarización descorrelacionada en 2D. (¿Debería comparar con polarización descorrelacionada?
¿Cómo es la proba si considero polarización 1D?)

De estas tres cosas que quería hacer, no hice ninguna. Siento que hoy podría haber hecho
más, pero estuve trabado toda la tarde. También hice muchas pequeñas cosas. Voy
a ponerme a pensar mejor cómo trabajar esto y ya el miércoles veré de resolverlo más claro.
Mañana voy a estudiar para el final de Mininni y después marcha.

Hablando con Pablo, él me dijo de que descargue yo los datos de la ANES para revisarlos,
así voy trabajando eso por mi cuenta. Debería separar preguntas políticas y no políticas,
así como separarlas según sus distribuciones. Después con eso ver qué tipo de gráficos
me quedan así tengo una buena idea de qué cosas espero observar. Mirar el paper de Baumann
y el de Hugo vendría bien para repasar qué es lo que estoy queriendo buscar.

Hoy y mañana me voy a poner a pensar cómo construir mis nuevas funciones en Python, cosa
de ya cuando em vuelva a sentar, ponerme a escribirlo de una y no volver a trabarme con
el cómo armar las funciones en el caso de Probas_Pol.

Por último, Pablo me propuso que arme más estadística a las simulaciones de Func_Sign,
no que barra más fino. Debería cancelar lo que mandé a correr y mandar de nuevo
con más estadística. Más que nada porque sino voy a tener un problema al querer armar
el gráfico porque algunos puntos van a tener más estadística y otros menos.
Lo que voy a hacer es mañana borrar las simulaciones con valores de Kappa fraccionarios.

------------------------------------------------------------------------------------------

23/04/2024

Ya borré esos archivos con valores de Kappa fraccionarios, mientras en Oporto se completan
simulaciones para llegar a tener 100 en ese barrido.

Ayer me quedé pensando en lo que dijo Sebas. Quizás es mejor seguir guardando las opiniones
finales de los agentes. Y además, me hice un quilombo queriendo usar la función de Hugo para
las simulaciones 1D. ¿Por qué no usar mis simulaciones? Si dan similares los resultados,
no tiene sentido diferenciar unas de otras. Podría usar mis simulaciones y con eso armarme
en Coimbra las 1000 simulaciones para el caso 1D y para el caso 2D. Mandemos hoy las 2D 
guardando las opiniones finales y después veo qué hago mañana.

Hablando con Pablo, los dos pilares para la próxima charla es trabajar con los datos y poner
en orden el análisis teórico. Así que por lo visto, no es central esto que estoy haciendo.
Pero como ya empecé, tengo que aprovechar y presentarlo.

------------------------------------------------------------------------------------------

24/04/2024

A la mañana lo primero que hice fue mandar a correr la tanda de simulaciones 1D de Probas_Pol
en la pc de Coimbra. Lo siguiente que voy a hacer es repasar las ecuaciones dinámicas cosa de tener
algo que presentarle a Pablo mañana y organizar los datos para la presentación.

Estuve avanzando con esto, aunque en eso vamos. Mañana me voy a poner en la mañana a estudiar
un poco de lo de Mininni y después seguiré con esto un poco más.

------------------------------------------------------------------------------------------

26/04/2024

En el día de ayer estuve trabajando en el análisis de la ecuación dinámica básicamente todo
el día. Fui a una reunión de grupo y llegué tarde por el tema de estar esperando el link para
la charla por el Repositorio de Investigación.

Hoy llegué, revisé el tema del análisis dinámico, charlé con Sebas y me propuso considerar un
análisis matricial similar a lo que veíamos en Mate 3. Igual creo que lo mejor sería probar
hacer lo que dijo Hugo, de hacer simulaciones sobre el análisis perturbativo y ver qué da eso.

Las simulaciones en Coimbra están terminadas. Las de Oporto todavía no. Debería por un lado
armar lo de Coimbra, cosa de tener por lo menos ese resultado. Pero creo que puedo dejar
eso para el próximo lunes sin dramas. Yo hoy me pondría a ver el tema de descargar datos de la
ANES. Voy a revisar la ANES, ver qué se puede descargar, qué preguntas hay y hacer esa separación
que me dijo Pablo de preguntas políticas y preguntas apolíticas, cosa de tener un conjunto
saludable de preguntas a revisar.

Ahí estuve viendo un poco de alemán, ahora sigamos con lo del modelo de opiniones. Descargué
los datos de la ANES, habrá que ver de armar el conjunto de preguntas que querramos observar.
Eso tomará un tiempo. 

------------------------------------------------------------------------------------------

29/04/2024

Llegué al horario usual. Me puse a revisar mails y cosas que quedaron pendientes. Ahora que
resolví eso, me pongo con las cosas que dejé corriendo.

1) Ver lo de la entropía en el espacio Beta-Kappa para la función Signo.
2) Determinar la fracción de estados polarizados en el caso 1D y en el caso 2D.
Usaré el algoritmo que tengo para eso y compararé con tomar 200 gráficos de histogramas
3) Leer un paper
4) Seguir laburando los datos de la ANES.

El punto 1 ahora no voy a poder, no terminó de correr eso.
Para el punto 2 necesito armar funciones que trabajen con un sistema 2D y un sistema 1D.

Logré armar uno de los gráficos de torta que quería, pero claramente esto no es lo más
interesante para charlar en la próxima reunión. Lo que deberíamos charlar en la próxima
reunión es el trabajo con los datos, de los cuales apenas armé unos gráficos nuevos
y después descargué los datos de la ANES de 2020. Así que la cosa viene lento en
ese tramo. Habrá que hacer nuevas cosas si quiero avanzar. Ahora voy a leer rápido
el paper de Sebas respecto a lo de Granovetter, al final no recuerdo si ese fue el
paper que leí o no. O si leí un paper en algún momento. Dicho eso, lo que sigue es
ponerme a revisar las preguntas y cosas del modelo. Así que hagamos esto y preparemos
para ir a casa temprano hoy. Iré a buscar el documento que necesitaba la semana que viene.

La idea sería primero que nada tomar las preguntas que tenemos y separarlas en función de
si son políticas o no. ¿Qué definimos como preguntas políticas? La respuesta es que las
preguntas políticas son aquellas que los demócratas o republicanos responderían distinto,
mientras que las apolíticas son aquellas que no apelan a esa identidad partidaria.

Anotemos esto en el código. Ahí hice la separación, básicamente todas las preguntas
son políticas. Razonable. Veré mañana si puedo encontrar unas ocho o diez no
políticas y de ahí veré cómo resuelvo esto. Pero el armado de los gráficos de histogramas
funciona de una.

------------------------------------------------------------------------------------------

30/04/2024

No llegué más temprano que lo usual, pero casi. Armé los gráficos de las distribuciones
de las opiniones diferenciando preguntas políticas y no políticas. Habiendo charlado con
Pablo, la pregunta sería ver si estas preguntas políticas cruzadas con ellas mismas me
generan gráficos distintos a cruzarlas con las preguntas no políticas. Aunque siento
que las preguntas no políticas apenas tengo casos que mirar, quizás valga la pena revisar
la ANES nueva y de ahí ver qué puedo extraer. Al final del día, la forma de levantar
datos que usa Hugo me podría servir para la nueva ANES creo yo, simplemente tengo que levantar
el csv y convertirlo en un pandas.

Lo siguiente que puedo hacer entonces son los gráficos de la distribución en el espacio de
opiniones y de ahí ver qué puedo rescatar. Comparemos preguntas políticas con otras políticas
y apolíticas con apolíticas. De ahí vamos armando los gráficos. Una vez que tenga los gráficos,
ampliar el conjunto de preguntas, porque no me satisface lo que tengo. No creo que tenga
suficientes apolíticas.

Antes que esto, Pablo me dijo de aplicar la medida de la distancia Jensen Shannon. Olvidemos
lo anterior, hagamos esto YA. No tengo una distribución para comparar, pero podría aprovechar
dos distribuciones que tengo acá y medirles su distancia Jensen-Shannon, ver que la idea se
puede y que da razonable. Bien, ya vi que calcula y cómo hacer la cuenta. En su momento será
un bardo ver cómo clasificar los gráficos según si son preguntas de 6 casilleros o 7.

Hice la cuenta sobre los datos de la ANES 2020, aprox. tiene 1200 preguntas. Veamos de levantar
los datos de la ANES 2020. Bien, los datos parece que se levantan sin problemas.

Preguntas Políticas 2020:
-------------------------
1) V201200: SCALE LIBERAL-CONSERVATIVE SELF-PLACEMENT (7 Opciones)
2) V201225x: VOTING AS DUTY OR CHOICE (7 Opciones)
3) V201231x: PARTY IDENTITY (7 Opciones)
4) V201246: SPENDING & SERVICES: SELF-PLACEMENT (7 Opciones)
5) V201249: DEFENSE SPENDING: SELF-PLACEMENT (7 Opciones)
6) V201252: GOV-PRIVATE MEDICAL INSURANCE SCALE: SELF-PLACEMENT (7 Opciones)
7) V201255: GUARANTEED JOB-INCOME SCALE: SELF-PLACEMENT (7 Opciones)
8) V201258: GOV ASSISTANCE TO BLACKS SCALE: SELF-PLACEMENT (7 Opciones)
9) V201262: ENVIRONMENT-BUSINESS TRADEOFF: SELF-PLACEMENT (7 Opciones)
10) V201342x: ABORTION RIGHTS SUPREME COURT (7 Opciones)
11) V201345x: FAVOR/OPPOSE DEATH PENALTY (4 Opciones)
12) V201356x: FAVOR/OPPOSE VOTE BY MAIL (7 Opciones)
13) V201362x: FAVOR/OPPOSE ALLOWING FELONS TO VOTE (7 Opciones)
14) V201372x: HELPFUL/HARMFUL IF PRES DIDN’T HAVE TO WORRY ABOUT CONGRESS/COURTS (7 Opciones)
15) V201375x: RESTRICTING JOURNALIST ACCESS (7 Opciones)
16) V201382x: CORRUPTION INCREASED OR DECREASED SINCE TRUMP (7 Opciones)
17) V201386x: HOUSE IMPEACHMENT DECISION (7 Opciones)
18) V201405x: REQUIRE EMPLOYERS TO OFFER PAID LEAVE TO PARENTS OF NEW CHILDREN (7 Opciones)
19) V201408x: SERVICES TO SAME SEX COUPLES (6 Opciones)
20) V201411x: TRANSGENDER POLICY (6 Opciones)
21) V201420x: BIRTHRIGHT CITIZENSHIP (7 Opciones)
22) V201423x: SHOULD CHILDREN BROUGHT ILLEGALLY BE SENT BACK OR ALLOWED TO STAY (7 Opciones)
23) V201426x: WALL ON BORDER WITH MEXICO (7 Opciones)
24) V201429: BEST WAY TO DEAL WITH URBAN UNREST (7 Opciones)



Preguntas Apolíticas 2020:
--------------------------


------------------------------------------------------------------------------------------

01/05/2024

Volví del cumpleaños de Estaban. Ahora queda ponerme a hacer algunas cosas, empezar a preparar
la presentación de mañana. Se me hace importante primero revisar que las preguntas que
arme tengan gráficos útiles. Si eso es así, lo siguiente será ir viendo quizás de empezar
la presentación. Si no es así, me pongo a buscar algunas preguntas más.

Por algún motivo que no comprendo, mi función me está tirando errores, a pesar de que no
tiene nada distinto a lo que tenía ayer. Resolveré esto mañana en la facultad.

Hagamos un esquema de la charla:
--------------------------------

1) Les comento que estuve mirando las preguntas que estaban en el archivo que me pasó Hugo
y revisando esos gráficos que producen para ver cuáles gráficos podríamos querer comparar
con nuestros datos, en caso de querer observar tal o cual estado.
 Muestro varios gráficos y coloco al lado las distribuciones con las que digo que se podrían
comparar.
2) No creo que de ninguna forma me dé el tiempo, pero podría intentar medir la distancia
Jensen-Shannon de algún gráfico en particular con la de una de las distribuciones obtenidas
de la ANES.
3) Les cuento que bajé los datos de la ANES 2020, logré leer los datos, los cuáles tienen un
formato muy similar, y construir gráficos parecidos. Para eso estuve revisando la guía de
datos y de ahí seleccionando preguntas que podrían ser relevantes o útiles.
¿Por qué queremos la encuesta más nueva?
4) Debería definir una región en la cuál realizar las simulaciones de 10000 agentes.
Eso debería poder hacerlo posiblemente si tengo una idea de qué gráficos quiero buscar.
En Beta razonablemente tenga que ir entre [0,1] y Cos(delta) entre [0,0.15]. Un barrido
medio crudo sería de a 0.05 para Beta y 0.01 para Cos(delta). Necesito saber cuánto va
a tardar esto.
5) Si queda lugar, mostrar alguno de los otros resultados.

------------------------------------------------------------------------------------------

02/05/2024

A la mañana preparé la presentación para la gente de GOTHAM. Armé los gráficos de las
distribuciones 2D y de ahí fui armando alguna idea de las cosas importantes.

Después de la reunión, fui al coloquio de Darío, aunque no pude ver mucho, volví para
anotar algunas cosas y después me fui a casa. Estuve como cuatro horas viajando para 
ir a buscar el documento que faltaba por el tema de la nacionalidad italiana. Todavía
sin suerte.

No pude mandar a correr el barrido en el espacio de parámetros para el caso de 10000
agentes. Mañana es lo primero que mando a hacer en la mañana.

------------------------------------------------------------------------------------------

03/05/2024

No estoy pudiendo mandar a correr simulaciones en la pc de Coimbra, no sé qué error está
encontrando el código. Voy a rehacer un código único desde la pc de la facultad y subir
eso a las tres pc's y de ahí mando a correr todo. A este código le falta las funciones
de Clasificación y los vectores asociados.

Mandé a correr en mi pc un archivo de 10000 agentes, veamos cuánto tarda en resolverse.
Es un estado con Beta 0, debería converger a un estado de Consenso Radicalizado. Está
tardando más de una hora esto. Es un muy mal presagio esto.

Mientras termina de correr el programa con 10000 agentes, voy a descargar los datos de
la ANES de años anteriores. Descargué las encuestas de 2012, 2008 y 2004.
La de 2012 parece que tiene un formato muy similar a las últimas 2, esa creo que se
podría agregar a lo visto sin mucho problema. Pero la de 2008 y la de 2004 tienen un
formato muy distinto y no estoy seguro de tener los archivos correctos para observar
las preguntas consideradas.

La de 2008 tiene un codebook separado para la encuesta pre y otro para la encuesta 
post. Habría que repasar el codebook para separar las preguntas. Lo bueno es que
parece que también usan números negativos para respuestas fuera de formato, pero
algunas preguntas tienen MUCHÍSIMAS respuestas con números por fuera de los primeros
10, habrá que revisar si son un problema o si se pueden ignorar.

NO TERMINÉ DE VER CÓMO ES EL FORMATO DE LA ANES 2004. TAMBIÉN TENGO QUE VER DE LEVANTAR
LOS DATOS, VER SI ME TIRA ALGÚN ERROR.

Justo terminó el código en la pc de la facultad. Por lo visto corre y llega a un resultado
razonable. El problema es la cantidad de tiempo que tarda. Tardó 11813 segundos en resolver
7000 pasos. Una locura es eso. Para 1000 agentes tardó simplemente 40 segundos. Las escalas
son completamente distintas. Veamos si funciona con los datos de clasificación. De paso,
el archivo apenas ocupa 900 kb. Casi un mega. Si tengo 100 simulaciones, para 36 puntos,
eso es aprox. 3 GB. Puedo simplemente guardarme las opiniones finales por ahora.

El código con la clasificación funca perfecto en la pc. Igual, quedémonos sólo con lo
importante, que serían las opiniones finales y la semilla. Comitteo esto y ya mañana
mando a correr todo. ACORDATE DE USAR UNA SOLA CARPETA DE REDES DE ADYACENCIA.

------------------------------------------------------------------------------------------

04/05/2024

Hoy a la tarde mandé a correr las simulaciones en las tres pc's, pero esto va a tomar
una eternidad. Hablar con Pablo y contarle que voy a meterme un momento en el código a ver
si hay algo que pueda hacer para hacerlo correr más rápido.

------------------------------------------------------------------------------------------

05/05/2024

Mirando el código, encontré algo que claramente está gastando mucho tiempo. Estoy calculando
varias veces el mismo peso. Tengo que hacerlo de forma tal de calcularlo una sola vez. Eso
va a reducir en mucho el tiempo de simulación. Ese error está en la función de GENERAR_SEPARACIÓN.

Otra cosa que no tiene que ver con tiempos de simulación pero que estaría bueno cambiar
es el hecho de que la matriz que guarda las tanh se llama Exp. Habrá quedado de otro código,
cambiar eso.

Lo tercero que se me ocurre que puede servir para achicar los tiempos es reducir primero
el ancho de la ventana de promedio, de 1000 a 500, y después cambiar el número de iteraciones
extras que el sistema necesita para cortar a 1500. En especial porque muchas simulaciones que
cortan rápido hacen 7000 pasos totalmente al pedo, cuando con 4000 o 3000 habrían terminado.

Estoy pensando en quizás cambiar la cantidad de pasos_maximos, pero Hugo y demás usaban esa
misma cantidad de pasos máximos, me parece que cambiar sería una mala idea.

------------------------------------------------------------------------------------------

06/05/2024

Ahí hice el cambio sobre el cálculo de los pesos. Veamos en comparación con el cálculo que se hizo
el otro día. La otra vez tardó 11813 segundos. Voy a usar la misma semilla y correr el mismo
programa, a ver cuánto menos tarda ahora. No voy a cambiar el tema las iteraciones extras o el
ancho de la ventana todavía, así la comparación es directamente con eso.

Ahí está corriendo eso. Mientras, voy a hacer las otras dos correcciones. Ya cambié el uso del
vector Exp por el vector Tanh. Es un cambio  de nombre para ser más claro.
 También modifiqué la cantidad de iteraciones extras y el ancho de ventanas para que esto corte
más rápido en los casos en que el sistema deja de variar rápido.

Mientras termina de correr la simulación que mandé, voy a preparar la presentación del miércoles.

Memes y cosas para introducir en la charla:
-------------------------------------------
1) Descripción distópica y útopica del tema con imágenes de Megaman battle network.

Después continuo esto.

El cambio que hice en los cálculos de las exponenciales no parece tener ningún efecto.
Es raro eso, no tiene mucho sentido. A menos que haya algo que está mal anotado en el
código, revisar eso. Porque tendría que reducirse mucho la cantidad de cuentas.

------------------------------------------------------------------------------------------

07/05/2024

Hoy quería ponerme a estudiar alemán, no creo que lo haga. Primero voy a mandar a correr de nuevo
los barridos en las pc's, pero modificando el instanciar para que no tenga el cero de cos(delta)
duplicado y modificando el ancho de ventana e iteraciones extras, eso debería modificar
algunos tiempos de simulación. Después voy a probar un poco más el tema de la modificación
en el código sobre el cálculo de las exponenciales, el cuál parece no tener efecto.

IMPORTANTE: EN ALGARVE ALGUNAS SIMULACIONES NO COMPLETARON PARA BETA=0 Y COS(DELTA)=1. REVISAR
DE MANDARLO DESPUÉS. AHORA MANDÉ PARA QUE ARRANQUE DE NUEVO A PARTIR DE BETA=0.2.

Ya mandé a correr todo de nuevo, haciendo las modificaciones que hice en la pc de la
facultad. Veamos ahora el tema de si puedo hacer correr esto más rápido. Necesito que
así sea, porque estas cosas están tardando tiempos imposibles.

No encontré la forma de hacer que esto vaya más rápido. Por ahora lo voy a dejar, voy
a mandar a correr simulaciones con redes de 5000 agentes a ver si puedo usar eso para
realizar las comparaciones. Con 5000 agentes el sistema me tarda 15 minutos. Ya fue,
trabajemos con 5000 agentes por ahora. Es mucho más razonable.

Cargué los archivos en las tres pc's, cuando terminen las corridas previas mando a correr
las nuevas simulaciones. No debería tardar mucho. A 15 minutos por simulación, me puedo
dar el lujo de hacer un barrido un poco más fino. Recorriendo 11 puntos en cada eje, son
121 puntos en el espacio de parámetros. Con 15~20 minutos por simulación, en el peor caso
eso es 40 horas por iteración. Así que en 2 o 3 días debería tener resuelto esto, considerando
que para los Betas cerca de 1 los tiempos de simulación se van a disparar. Espero que salga
bien la cosa, tendré 30 simulaciones para mirar.

Voy a ponerme a preparar la presentación para mañana.

------------------------------------------------------------------------------------------

08/05/2024

Revisé los programas que están en las pc's y están tardando aprox. 40 minutos cada simulación.
Recién resolvió dos betas de los 11. Así que eso va a estar un buen rato, con algo de suerte
va a estar terminado para el viernes. La pregunta importante es:
¿Me pongo hoy a estudiar Alemán o mañana? Yo diría que mejor mañana, así mañana cualquier
cosa tengo algo para mostrarle a Pablo. Hoy pongámonos a armar el Excel que decíamos con
Pablo de preguntas de las ANES.

------------------------------------------------------------------------------------------

09/05/2024

Por el paro, hoy estoy trabajando en casa. Estoy mirando la ANES y clasificando preguntas.
En la encuesta ANES 2020 hay preguntas de estereotipos sobre que tan trabajador o violentos
son según grupo étnico. Esas preguntas van desde V202515 hasta V202526. Todas estas preguntas
tienen 7 posibles respuestas. A partir de la pregunta V202580 hay muchas preguntas que parecen
tener 7 respuestas, varias que no son políticas. Pero son demasiadas y poco interesantes
realmente. Quizás si hace falta las podría revisar, pero en principio no es importante.

Ya revisé toda la ANES 2020. ¿Debería seguir con este trabajo?

------------------------------------------------------------------------------------------

10/05/2024

El 9 trabajé desde casa, pero por lo que veo no lo comitee, quizás genere problemas.
Tengo problemas más serios. Resulta que lo que estoy mandando a correr está tardando una eternidad.
Es una eternidad más corta que el caso de 10000 agentes, pero aún así el de 5000 está tardando
una eternidad siendo que las simulaciones que polarizan tardan 36 horas aprox. en resolverse.
Tengo que ver de buscarle una vuelta a eso. Simplemente es inviable trabajar así.
¿Debería resolver eso ahora?

Lo que habíamos charlado con la gente de GOTHAM era catalogar preguntas, armar a partir de eso
un conjunto de preguntas interesantes y quizás ver a lo largo de varias ANES cómo evoluciona
la polarización de esas preguntas. Creo que puedo conseguir más resultados si sigo catalogando
preguntas. Pero en ese caso, ¿Qué buscaría mostrarle a la gente de GOTHAM el miércoles que
viene? Con suerte, podría mostrarles el subset de preguntas que pensamos mirar y algunos 
gráficos donde veamos cómo fue evolucionando esa polarización. Porque mostrarles cuáles son
los tipos de gráficos que planeamos comparar no tiene sentido.

Sabiendo que los datos de simulaciones no van a estar como para hacer un estudio razonable
de acá al miércoles, yo propondría primero ir comparando lo que tengo con datos obtenidos
de 1000 agentes, como los que armé para el espacio reducido en Beta-Cosd, y de ahí armar algo
que funque y que en principio sea un puntapié al análisis, total una vez que tenga los datos
es una cuestión de cambiar el N del código de 1000 a 5000 o 10000. Entonces primero seguiré
catalogando preguntas, y ya a la tarde o después de hablar con Pablo definiremos los siguientes
pasos en estos días de acá al miércoles. Después habrá que volver a ver qué podemos hacer para 
que el código corra más rápido, porque simplemente no se puede vivir así. La primer idea que
surge es hacer una Tabla con datos para la función exponencial e interpolar. Tengo que pensarla
de alguna forma correcta, cosa de que el argumento sea la distancia al cuadrado del vector
y no la distancia, así aprovecho y me ahorro la raíz cuadrada.

Estoy mirando el guidebook de 2012 y estoy notando dos cosas importantes. La primera, es que el
formato es mucho más feo que las versiones anteriores, va a tomar mucho revisar las preguntas.
La segunda es que este guidebook no parece tener separados los datos en códigos como los otros
dos. Eso me preocupa.

De acá para el lunes es importante mandar a correr mi código para 10000 agentes en una dimensión
y el código de Hugo para 10000 agentes en una dimensión. Corrido eso, vamos a tener una buena
idea de la diferencia de tiempos y si los tiempos del código de Hugo es razonable o viable.

------------------------------------------------------------------------------------------

11/05/2024

Mandé a correr el código mío y tardó 7600 segundos. El código de Hugo en cambio tardó 47 segundos.
Hay una diferencia de dos órdenes de magnitud en el tiempo que tarda mi código y el de ellos.
No comprendo por qué. Eze me dice que quizás sea un tema de la cantidad de fors que estoy usando.
No creo que sea eso, pero se puede probar.

Mañana habrá que hacer gráficos con las preguntas que tengo de la ANES, armar una clasificación
clara y después preparar algo para presentar con eso.

------------------------------------------------------------------------------------------

13/05/2024

Soy un reverendo pelotudo, cinco veces pensé en commitear el trabajo de casa y no lo hice.
Ahora voy a tener un bardo entre los archivos y cosas. En la facultad hoy voy a
ponerme a trabajar en los archivos de Python para armar la clasificación de las preguntas
y los gráficos asociados.

El sueño que tengo me está complicando laburar fuerte. Ya no es joda hermano. No sé si
es la ropa que tengo puesta o el cansancio general, simplemente estoy deteriorándome. Y
el invierno no me gusta.

Anoté las preguntas que voy a mirar, armé los gráficos y los estuve repasando un poco,
pero no me parece muy interesante lo que estoy viendo. Pablo me propuso mirar directamente
las distribuciones 1D y con eso ir descartando cuáles son las preguntas que quiero mirar y
cuáles no.

¿Qué criterios usé para remover preguntas? Saqué las que tuvieran una gran cantidad de agentes
en una opinión neutra o que tuvieran distribuciones muy homogéneas.

Hice un primer filtro de cuáles voy a querer y cuáles no. En el drive voy a marcarlas como
descartadas, mientras que en Python las voy a borrar directamente.

Las preguntas que clasifiqué y separé me parece que son un buen filtro, me dejan los gráficos
que inicialmente quería mirar. Anotemos las preguntas descartadas en el drive. Después de eso,
empiezo a ver cómo medir la distancia de jensen-shannon.

Hice las anotaciones que quería en el drive, tengo clasificadas las preguntas que voy a querer
revisar. Mañana veré de armar el conjunto de preguntas y mostrárselo a Pablo, así como mostrarle
de estas preguntas cuáles podrían tener una cierta trazabilidad hacia atrás.

Lo que voy a hacer ahora es descargar los datos del barrido de Zoom Beta-Cosd y a partir de esos
ver cómo comparar esos datos con los datos de las distribuciones. Voy a elegir dos o tres
gráficos para comparar y a partir de esos voy a intentar armar algunas comparaciones.

Descargué los archivos a la carpeta de Comparacion_datos. Lo que necesito es un archivo de Python
que arme la Clasificación de las opiniones finales y compare eso con los datos de la encuesta.
Tengo algo que hace eso creo, que lo preparé para la semana pasada. Así que simplemente tengo
que lograr hacer un for. Y después voy a tener errores porque obviamente tengo huecos en mis
distribuciones T.T.

------------------------------------------------------------------------------------------

14/05/2024

Ayer llegué a casa y tampoco comitee mis datos. Soy un re boludo. Voy a copiar los datos de la
ANES en la carpeta de Comparación_datos y ahí voy a trabajar la parte de las comparaciones de
las distribuciones usando distancias Jensen-Shannon.

Ya tengo los datos del barrido reducido y tengo los datos de la ANES para comparar. Lo siguiente
sería empezar a comparar los datos. Voy a probar contra una distribución que tiene una buena
forma de polarización ideológica, el gráfico que sale de "Govt. Asssitance to Blacks" vs
"Guaranteed Job Income", que son los códigos "V201258" vs "V201255".

Por lo que leí en Wikipedia, debería revisar un poco más el tema, la idea es que al calcular
la distancia Jensen-Shannon, la distribución p, que es la primera que le paso a la función,
es la distribución medida. Es decir que primero le paso la distribución obtenida de la encuesta.
La segunda distribución, la distribución q, debería ser mi aproximación o mi resultado simulado.

Estoy armando el código para calcular la distancia entre dos distribuciones. Tengo una primera gran
duda: ¿Las distribuciones que estoy comparando, elemento a elemento, corresponden entre ellas?
La respuesta es que sí, se corresponden. Eso es genial, no puedo creer que haya funcionado.

Lo siguiente ahora es armar una función que calcule esta distancia y arme gráficos en el espacio
de parámetros. Ahí lo mandé a correr, funcionó. Y da razonable. Charlar eso con Pablo mañana.

------------------------------------------------------------------------------------------

15/05/2024

Pablo mandó un mensaje para posponer la reunión hasta la semana que viene. Así que para
la semana que viene tengo que tener algo más sólido armado. Para eso tengo que trabajar
en las siguientes cosas:

1) Acondicionar el código de Hugo para que trabaje con dos tópicos.
2) Continuar estudiando la distancia Jensen-Shannon en el espacio de parámetros para otras
configuraciones de preguntas. (Esto va a implicar adaptar la función de Clasificación para
los casos con tamaños que no sean 7x7). También debería estudiar un poco más la función
que calcula la distancia Jensen-Shannon.
3) Revisar los resultados analíticos y armar un plan de qué falta hacer. Quizás hacer
análisis de las ecuaciones en papel mezclado con simulaciones.
4) Revisar si quedaron ideas que valga la pena estudiar del tema de las simulaciones.
5) Probar cómo hacer que mi código funcione tan rápido como el de Hugo.

Vuelvo a mirar mi código, no veo nada raro. No comprendo. Pero bueno, empecemos a modificar
el código de Hugo para que funcione como yo quiero. Mientras, pongamos a prueba los tiempos
una vez más. Mandé a correr los dos códigos, sólo para comparar los tiempos.

Mandé mi presentación al curso de Verano ese en Uruguay que Pablo había compartido. Por otro
lado, no terminé de adaptar el código de Hugo. Mañana termino eso. Quedé a mitad de
adaptar la parte donde evoluciona y calcula la variación promedio. Lo que me queda revisar
también es agregar el cálculo de la norma no ortogonal. Creo que hechas esas dos cosas,
debería estar resuelto esto.

De paso, mandé a correr los códigos para comparar los tiempos, sigue ocurriendo que el
código de Hugo es 100 veces más rápido que el mío.

------------------------------------------------------------------------------------------

16/05/2024

Llevo toda la mañana y un poco de la tarde en terminar de adaptar el código de Hugo para que
corra como yo espero. Veamos si realmente funciona. Crucemos los dedos.

Corre y resuelve en poco tiempo. Queda hacer una comparación sobre si mi código genera el
mismo resultado. Suponiendo que el código funciona bien, tengo que reemplazar lo que ya 
está en funcionamiento y mandar a correr este nuevo código. Vayamos preparando eso mientras
se resuelve el código que mandé en la pc de la facultad que compara lo que hace mi código
con lo que hace el de Hugo.

Ya está corriendo el código de Hugo adaptado, el cuál corre mucho más rápido que el mío.
Lo que voy a hacer ahora es probar que el cálculo de la distancia de jensen-shannon sea
razonable, que esa cosa calcula correctamente.

Revisé la función de Jensen Shannon comparando dos distribuciones normales para ver cómo
varía esta distancia. Parece funcionar perfecto, incluso trabajando sin drama ante la idea
de tener 0 en partes de la distribución. Parece que no va a generar ningún problema
esto.

Mañana podría ponerme a modificar la función de Clasificación para poder agrupar las opiniones
en arrays de 6*7.

Ahora lo siguiente que necesito hacer para presentar es armar una función que primero
aisle la región de datos en la cuál quiero armar mi ajuste, y que luego haga ese ajuste.
Como sea que tenga que ser eso. Yo diría que por hoy estoy en horario, guardaría todo,
iría para casa y luego veré cómo construir eso el finde. O el lunes, creo que el lunes
tendría que salir eso.

------------------------------------------------------------------------------------------

20/05/2024

Estoy trabajando desde casa, hoy no me siento en condiciones de ir a la facultad.
Lo que quiero hacer es el ajuste de los parámetros. Para eso recorto una región en
la cuál observe un mínimo. A esa región, le hago un ajuste por cuadrados mínimos a la
Distancia de Jensen Shannon en función de los parámetros Beta y Cos(delta), ajustando
con la función de un paraboloide.

Armé la función que hace el ajuste de cuadrados mínimos. Después tengo que ver cómo obtener
el error del ajuste. Pero lo importante es que con esto podría calcular los coeficientes
de mi paraboloide con el que ajusto este espacio.

Digamos que Beta lo ajusto entre [0.5,0.72] y Cosd entre [0.04,0.15]

------------------------------------------------------------------------------------------

21/05/2024

Ya logré armar el cálculo del mínimo del valor 

Lista de tareas:
1) Chequear dónde se demoran mis simulaciones.
2) Resumen de la parte numérica. Qué está hecho y qué faltaría.
3) Resumen de la parte teórica. Qué está hecho y qué faltaría.
4) Resumen de la selección de datos y comparación del modelo.

Para la charla de mañana, quizás ver de hacer un ranking y también 
mostrar el espacio clasificado según las simulaciones.

Estoy teniendo un problema horrible con el cálculo de los gráficos
de distribución de opiniones de las encuestas. Es ese problema que me ocurre sólo en casa,
y que no logro comprender por qué. ¿Será mi versión de Win Py? Buscaré cómo actualizar eso.

Mandé a correr las simulaciones de la 30 a la 50 en Coimbra. En Algarve y Oporto no terminaron
todavía.

Las dos primeras preguntas que revisé son: 'V201258' vs 'V201255'. Son preguntas dudosas
Las variables óptimas obtenidas fueron: [Cosd = 0.104, Beta = 0.602].
Revisé 'V201420x' vs 'V201231x'. Son preguntas políticas
Revisé 'V202320x' vs 'V202320x'. Son preguntas No políticas

------------------------------------------------------------------------------------------

22/05/2024

Para bien o para mal, hoy es la reunión. Hay que hacer funcionar esto. Logré graficar 
las distribuciones de las encuestas usando matplotlib. Gracias por tanto Hugo.
Lo que tengo que hacer es elegir tres pares de preguntas. Uno ya lo tengo elegido,
es el que le mostré a Pablo, creo que Govt. Assistance to Blacks vs Guaranteed Job
Income.

De las preguntas políticas tengo dos opciones que me parece que podrían resultar
interesantes. La primera es "Birthright Citizenship vs Liberal-Conservative Self placement"
(V201200 vs V201420x). Se parece un poco a polarización descorrelacionada porque no va
tanto a extremos sino que llena un poco más las zonas intermedias. La segunda opción es 
la de "Less or more Government vs Birthright Citizenship" (V201420x vs V202255x).
Esta no me convence tanto porque tiene bastante vacío el medio, eso se parece más a una
polarización descorrelacionada sin anchura. Aunque debería igualmente dar mejor 
cerca de Beta =1.

En el caso de las no políticas, observo que son todas básicamente casos de consenso radicalizado.
Si ploteo lo que observo en la salida del plt.hist2d, lo que obtengo no es algo normalizado, es una
distribución con números más grandes que uno en muchos casos. Pero son todos floats, no es que
es la cantidad de agentes en cada región exactamente.
Me parece que razonablemente puedo proponer como ejemplo del de "Government action about opiod
drug addiction vs Economic Mobility compared to 20 years ago" (V202320x vs V202350x). Todos los
pares de preguntas parecen razonablemente observarse como consensos, ese par me parece preguntas
claramente apolíticas.

El par de preguntas apolíticas que estoy mirando está dando muy mal, por algún motivo me queda la
distribución al revés de lo que esperaría ver. No, ya lo pensé, ahí lo entiendo todo, la forma en
que calculo la distancia claramente está generando problemas. No estoy rotando las distribuciones.

Probemos cómo da el caso de Vaccines in Schools vs Best way to deal with Urban Unrest. 
(V201429 vs V202341x).

.) Considerar que la gente responde nulo cuando no se le da la opinión de no responder.
.) Usar agentes neutros a la fuerza. Es una forma de resolver el tema de la caja de agentes
 con opiniones. La otra es anular el elemento que corresponde a la caja central
.) Normalizar los gráficos de la distribución de la ANES.
.) Salvar los ceros en las regiones que faltan agentes.
.) Considerar que quizás tenemos dos Betas. Pensar en temas que tengan Betas distintos quizás,
 como el del Muro con México y el ..., para ver si tienen dos mínimos.
.) Arrancar con las preguntas que vieron ellos en el paper que sean bimodal o unimodal.
.) Comparar gráficos con igual resolución.

------------------------------------------------------------------------------------------

23/05/2024

Hoy lo primero que quiero hacer es organizar un poco mis archivos, y después resolver alguno
de los elementos que charlamos con la gente de España.

1) Normalizar los gráficos de distribuciones de ANES.
2) Armar reversiones de mis gráficos 
3) Rearmar la función de Clasificación para que tome preguntas de 6 respuestas en vez de 7.
4) Finalizar los ajustes, así como el graficado scatter de las distancias.

Al final solo logré hacer un poco de orden en las cosas que tenía, no avancé en nada del
trabajo por el tema de cómo me está resultando la operación. Roguemos que todo va a mejorar.

------------------------------------------------------------------------------------------

24/05/2024

Hoy estuve con el tema de la inflamación. Recién a la tarde arranqué a laburar. Pablo me va
a proponer usar zealots neutrales en las simulaciones. Mi proposición sería que no porque 
eso es algo a implementar en el código y más fácil sería hacer lo que dijo Hugo, simplemente
retirar el elemento del centro de la distribución.

Creo que normalizar los gráficos de la ANES es la primer prioridad. 
Segunda es modificar la función de Clasificación,
Tercero es graficar los histogramas 2D de mis simulaciones con la misma resolución que mis distribuciones.

Si eso está hecho, podría ver de pasar a ver cómo hacer lo de comparar distribuciones que tengan
betas diferentes para comparar. Para eso necesitaría la info que Hugo me dijo que me iba a compartir.
Si hoy no está, se la pido ya el finde. 

También voy a tener que repasar la parte teórica del modelo y charlar eso como para hacer una
presentación interesante al respecto. ES IMPORTANTE QUE PARA DENTRO DE DOS SEMANAS TENGAMOS
MEJOR PRESENTADO EL AJUSTE HECHO EN LOS DATOS, ASÍ COMO UN RESUMEN DE LA PARTE TEÓRICA Y
LA PARTE DE SIMULACIONES.

Empecemos por la parte de normalizar las distribuciones. Con ponerle un density = True parece
que alcanza. Aunque me genera un poco de duda eso, por cómo funcionan los pesos. La idea
es que el histograma cuenta cuántas son las personas que opinaron X o Y. Pero al pesarlo,
¿Afecta ese valor según un número? ¿No debería ser más bien como que la cantidad de gente
que cuenta se ve afectada? ¿Está el plt.hist2d haciendo el segundo trabajo o el primero?
Yo ya no estoy seguro de qué está haciendo.
 Si esto funciona bien, lo que debe estar haciendo es primero definir donde va cada "evento",
y luego no los cuenta como unidad, sino como su peso. Entonces si el peso es 0.97, no
cuenta como una persona, cuenta como 0.97 persona. Bien, es razonable. Normalizar eso
es una tontería después.
 En particular, al normalizar me queda algo tal que si sumo todos los valores me da 1. Eso
es porque cada cajita mide 1 de lado. 

------------------------------------------------------------------------------------------

26/05/2024

Ahí modifiqué el armado de la función de Clasificación, no lo probé, confío en que está bárbaro.
Acabo de ver que en la función que lee los datos de la ANES ya estaban los nombres de las
preguntas. Debería ver de extraer eso de ahí y listo.

Revisé la función que arma los histogramas 2D, para conseguir mis gráficos de 7x7 como quiere Pablo,
necesito simplemente cambiar el número de bins, ni hace falta que arme una función nueva.
Pero sí aproveché a modificar la función de Clasificación, todos los usos que encontré de eso, y 
por tanto los de Diccionario Metricas y de Calcular Entropia.

------------------------------------------------------------------------------------------

27/05/2024

Llegué no tan temprano, pero con motivos. Organicé cosas, revisé que el armado de gráficos con Seaborn
siga funcionando, aunque pasé a armar los gráficos con matplotlib. Ahora voy a ponerme a hacer
los gráficos de histogramas de opiniones 2D de 7x7.

Mientras está corriendo el armado de los gráficos de Zoom_Beta-Cosd voy a revisar que se estén armando los
gráficos correspondientes en las computadoras del cluster. Eso sigue corriendo todavía. En especial en
Algarve y Oporto creo que están todavía en la primera vuelta, falta la segunda. En cambio en Coimbra
ya casi está terminado. Mañana voy a ver si ya está terminado y mando a correr de nuevo.
Estos gráficos se podrían rearmar de forma de que los cuadrados en los que divido el espacio de opiniones
representen la proba de tener agentes en esa región. Actualmente representan otra cosa porque los cuadrados
miden 2.8...

Bien, ya resolví las cosas que me anoté el viernes. ¿Qué es lo próximo que tengo que hacer?
Hay dos cosas que se me ocurren hacer. Lo primero es revisar el tema del estudio analítico.
Completar el análisis de la polarización ideológica con el análisis de la polarización descorrelacionada.
La idea sería utilizar alguna simulación para ver la estabilidad del modelo en función de Beta
para ver si el estado de polarización descorrelacionada se rompe a medida que Beta crece.

La segunda opción es ver cómo puedo rotar las distribuciones de opiniones para poder comparar eso con las
distribuciones de ANES. Una vez hecho eso, puedo volver a realizar los ajustes que dejé truncados en
la última reunión.

------------------------------------------------------------------------------------------

28/05/2024

En la mañana estuve trabajando un rato en mi cuaderno en armar un código que rote la matriz.
Después me puse a estudiar alemán, fui a cursar y a la vuelta trabajé un poco más antes de
irme a casa.

------------------------------------------------------------------------------------------

29/05/2024

En la mañana me escribí el código para rotar matrices que necesitaba. Ahora que tengo eso,
puedo ponerme a hacer lo segundo que es lo que me dijo Pablo, ver de remover elementos de
las distribuciones. Podríamos removerlos de dos formas. La primera, es remover sólo el elemento
central. La segunda es remover TODO agente que haya respondido con las preguntas centrales
an alguna de las preguntas. Vamos a probar hacer las dos cosas y de ahí vamos a ver qué resulta
mejor.

Pero antes de eso, ¿Tengo resuelto la rotación de mis matrices? Lo que hice funciona bien si mi
matriz es cuadrada. Pero en caso de no serlo, ¿Cómo puedo rotar mi matriz? ¿Tiene sentido rotar la matriz?
Creo que por ahora no me voy a preocupar demasiado por esto. Avancemos con lo otro, y cuando surja
ese problema, lo enfrentaré.

Probaré primero sacando un elemento en el centro. Me parece que va a ser más fácil de implementar.
Al mismo tiempo implementaré una corrección para el hecho de que las distribuciones no tengan ningún
punto nulo.

Para remover el punto central, eso es bastante directo. Si mis preguntas son de 7x7, significa que
tengo que eliminar el punto en la posición [3,3]. O lo que sería lo mismo, de mis 49 elementos de
mi array con la distribución de la encuesta, tengo que eliminar el elemento 24.

Admito que me gustaría además que el gráfico que hago de la encuesta tenga ese elemento bloqueado.
Debería ver de pedirle al pandas que al graficar ese punto esté tapado.

Bien, para remover los agentes del centro o para remover la cruz ya Chat GPT me dió una linea fácil de
cómo hacer eso con Pandas, probé graficarlo y efectivamente lo que se grafica es lo que estoy buscando.
También miré las distribuciones generadas. Tienen la forma correcta. Lo siguiente es entonces
Ya vi que eso funca, lo que me queda es hacer lo de que si la distribución de la simulación tiene ceros,
entonces tengo que agregar agentes en esa distribución. Lo que puedo hacer en ese caso es sumar agentes
de a 1. Luego, según la cantidad que sumé, restar esos agentes del lugar donde haya más agentes.

Si pongo 1 agente en cada lugar vacío, la distancia Jensen Shannon da distinto que si no tengo agentes
ahí. En particular, tomé el ejemplo del par de preguntas políticas y lo comparé con una distribución de
Consenso Radicalizado. Para esta distribución con ceros, la distancia JS da 0.824, en cambio cuando 
relleno los lugares con cero obtengo una distancia de 0.774. Habrá que hacer esto de rellenar los lugares.

Me queda hoy probar lo de remover el punto central. Al remover el punto central, la distancia da
distinto a si dejo un cero en ese punto. Ok, definitivamente removeré ese punto cuando haga las cuentas.
Ya tengo armado el esqueleto para construir el código que quiero hacer, ahora queda nomás pasarlo
al código de funciones que se encarga de armar el mapa de colores de Distancia JS.

------------------------------------------------------------------------------------------

30/05/2024

Hoy el plan es hacer dos cosas importantes. La primera es lograr ver la distancia de JS de pares de
preguntas removiendo el centro y removiendo la cruz central. Tengo más o menos armado el código para
hacer eso, ahora tengo que organizar todo en carpetas razonables. Creo que lo que voy a hacer es
preparar tres carpetas. Una para el caso normal, una para el caso sin el centro y otra para el caso
sin la cruz. El caso normal lo voy a llamar "Base". El caso sin el centro lo voy a llamar "Sin Centro"
y razonablemente el último caso va a ser "Sin Cruz".

Hecho eso, puedo empezar a revisar pares de preguntas, medir las distancias JS y hacer los gráficos
correspondientes.

Lo segundo que voy a querer hacer hoy es mandar las simulaciones con los Zealots. Eso lo tengo
que mandar a correr en las pc's de la facultad. Igual eso lo voy a mandar a correr recién mañana,
hoy no voy a poder porque la pc de la facultad todavía está corriendo el barrido en Beta-Cos(delta).

Probé varias cosas, pero ninguna parece resultar para cerrar esa región de la cruz. Así que por ahora
dejemos eso y sigamos con el resto del laburo. Para lograr esto creo que la idea va a ser en la misma
función de cálculo de la Distancia de JS hacer las modificaciones para que se calcule correctamente la
distancia y desde ahí armar los dos gráficos de una sola vez. Eso va a ser lo mejor.

Hice todas las modificaciones como para poder correctamente armar los gráficos. Fue un bardo, va a
tirar 15 errores, pero bueno, ahora estoy en condiciones de probar a ver qué da. Va a tardar una eternidad
más, porque ahora hace muchas más operaciones. Además está todo mezclado en un sólo paquete para trabajar
sólo con preguntas de 7x7. Va a ser un bardo desarmar eso para que trabaje con preguntas de 6x6 o 6x7.
Voy a tener que ver cómo se automatiza eso para trabajar en esos casos. O, quizás, sólo quizás, cuando
trabaje con esas preguntas mejor simplemente me pongo a armar una función distinta que ya sepa de antemano
la forma de la función que le voy a mandar. Veré, no sé honestamente qué es mejor.

Ahora voy a trabajar con tres pares de preguntas que me resultan las más simples de utilizar:
1) V201255 vs V201258. (Govt. Assistance to Blacks vs Guaranteed Job Income.)
2) V201200 vs V201420x (Birthright Citizenship vs Liberal-Conservative Self placement)
3) V202331x vs V202341x (Background checks for gun purchases vs Vaccines in Schools)

Logré que haga los cálculos, pero no da muy lindo. Mañana armaré algunas preguntas más en la facultad
como para ver qué es lo que da y si ahora el caso de las preguntas no políticas que parecen un consenso
radicalizado dan mejor.

Lo siguiente es ponerme a trabajar en el código actual para introducir el uso de Zealots cuyas
opiniones se mantienen fijas en (0,0). Entiendo que la idea es que una fracción de esos
agentes se mantengan en esa opinión. Supongamos que el plan es que un 10% se mantenga en ese
número.

Ahí hice lo de los Zealots, la verdad fue bastante rápido, buenísimo. Bueno, ya son las 18:30,
creo que puedo dejarlo acá por ahora. Mañana seguiré sacando gráficos de distancia JS
y veré qué tal quedan.

------------------------------------------------------------------------------------------

31/05/2024

Ahora estoy en la facultad. Lo primero que voy a hacer ahora es mandar a armar los gráficos
para los pares de preguntas que tengo ahí considerado, así se ve mejor si esto funca bien
o no.

Armé gráficos de distancia JS en el espacio de parámetros. Dan un poco mejor que antes, pero
tampoco la gran cosa. Habrá que ir viendo qué tal. Se me ocurre que puedo agarrar algunos
pares de preguntas más, revisarlos y después de ahí sacar conclusiones.

Lo que voy a hacer ahora es armar un poco una charla, unas pocas diapositivas para armar
la idea de lo que vamos a charlar para el miércoles que viene. Había malentendido yo, 
esto es para tener un esquema de trabajo nuestro en el cuál tengamos bien claro lo que estamos
queriendo hacer y lo que falta del trabajo. Tengo anotado el armar simulaciones extendiendo la
región de simulación del espacio Beta-Cos(delta). La idea está bien, vamos a tener que considerar
seriamente cuál es el nivel de granularidad de ese barrido cosa de que sea consistente en toda la
región.

Para el miércoles lo ideal sería llegar con más laburo hecho del lado de los ajustes y las
distancias calculadas de JS. Para eso necesito entonces armar un conjunto de seis pares
de preguntas que me interese revisar. Fijemos algunas preguntas como me dijo Pablo y de
ahí vamos viendo cuáles combino y como.

Algo para aclarar de paso, noté hace un rato que el cálculo de la distancia JS con las simulaciones
más similares está mal hecho porque no estoy haciendo un sort según distancia. Así que eso lo tengo
que corregir. Otra cosa que voy a tener que hacer es corregir el cómo se hace el ajuste, porque eso
no tiene la parte de rotar los gráficos. También estaría bueno agregar al mapa de colores de 
distancia JS algo que saque el gráfico que más se parece a la distribución de la encuesta considerada.

Una cosa más a revisar es el tema de cómo está dando la distancia JS. Parece que nos está dando
muy alto. Pero justamente la palabra clave es parece. No tenemos claro si es un número alto o
no, necesitamos más claridad respecto a qué es un gráfico similar o no a lo que estamos viendo.
Así que voy a tener que experimentar un poco con eso para ver cómo sale. Por último, voy a tener
que ver de normalizar los gráficos de histogramas 2D de forma tal que la suma de las fracciones
graficadas de 1.

Bien, tengo bastante claro el laburo a hacer. Y tengo bastante laburo. Arranquemos. Primero
las preguntas. Me armé un conjunto de preguntas. Suficientemente chico como para no matarme
revisando 30 gráficos. La idea es armar seis, siete gráficos de pares de preguntas y con eso
empezar a revisar cosas.

------------------------------------------------------------------------------------------

01/06/2024

Obviamente me olvidé de subir al drive los gráficos de histogramas que había elegido ayer.
Porque obviamente me iba a olvidar de eso. Ahí armé los gráficos, los hice con matplotlib.
Por ahora voy a trabajar sólo con preguntas que tengan seis respuestas.

Bien, ya tengo seleccionadas las preguntas que creo van a resultar útiles. Ahora armo las
distribuciones 2D de esas preguntas para decidir cuáles 6 preguntas quiero mirar.
Los gráficos que esoty viendo no me gustan, no hay nada que se parezca a lo que queremos
ver. Sí, ya sé que tampoco lo que veíamos se parecía tanto, pero igual no me gusta. Por
ahora dejemos eso así, avancemos con las otras cosas, total si lo otro está armado, simplemente tengo
que elegir nuevos pares de preguntas y volver a intentar.

Ya corregí lo de la similaridad. Era tan simple como poner un np.sort. Lo siguiente que se me
ocurre es armar una función que me muestre el gráfico más y menos similar. El tema de
armar una función aparte es que va a tener que volver a calcular todas las distancias JS
y eso tardaría mucho para mostrar un simple gráfico. Creo que lo mejor sería intentar aprovechar
la función del Mapa de colores DJS. Hagamos eso.

Suponiendo que está todo bien, se arman los histogramas 2D de mínima y máxima similaridad
para cada encuesta sin centro y sin cruz. Aunque no estoy haciendo el histograma 2D de la
simulación sin la cruz o sin el centro. Tengo que ver cómo hacer que esos gráficos respeten
eso. Lo cuál no es obvio porque en las simulaciones las opiniones no son valores enteros.

------------------------------------------------------------------------------------------

02/06/2024

Revisé los gráficos de distribución de la ANES, en el centro de opiniones existen como mucho
un 12% de agentes, siendo más bien en general entre un 6% y un 8%. Así que mandar un 10% de agentes
como Zealots parece una buena idea. Lo que tengo que hacer ahora entonces es cargar el archivo
de opinion a todas las pc's, armar la nueva carpeta de los Zealots y ya mandarlo a correr.

Mandé a correr 10 hilos en Coimbra y 10 en Oporto. Iba a mandar en Algarve, pero todavía le
queda un hilo a Algarve, entonces para no hacer macanas corté lo que estaba mandando ahí y volví
a cargar el archivo de opinion que tenía previamente y lo compilé para que ese ejecutable sea el que
esté armado. Con algo de suerte, no arruiné nada.

Bueno, si corrijo el cómo hace el cálculo de JS la función de ajuste, entonces habré hecho todo lo
que me anoté el viernes para hacer. Que era un montonazo. Me parece un gran plan.

------------------------------------------------------------------------------------------

03/06/2024

En la facultad acabo de rehacer los gráficos de distribuciones de ANES. Sostengo que los gráficos
hechos con las preguntas elegidas no me gustan. Voy a tener que elegir algunas preguntas nuevas.

Ahora voy a mandar en Algarve a correr el armado de las simulaciones con Zealots. Ya mandé eso a
correr. Dentro de todo parece correr bien, los primeros agentes tienen opinión nula mientras que el
resto de los agentes está convergiendo a algún lugar.

Lo siguiente es mandar a hacer los gráficos de Distancia JS. Los gráficos se hicieron, parece funcar
bárbaro. Las distancias dan bastante menor, cercano a 0.35 en promedio. Tengo que corregir una cosa
de esto, y eso es cómo se arma el histograma de máxima y mínima similaridad. También tengo que 
cambiar esos títulos, son confusos. Pero cuestión que tengo que cambiarlo para que esos gráficos
tengan efectivamente 0 en los lugares correspondientes, ya sea en el caso en que no tiene el
centro o el caso en que no tiene la cruz.

Creo que en vez de hacer el histograma, es mejor si simplemente hago el gráfico usando la función de
clasificación y un pcolormesh. Bien, ahí hice algo, espero que funcione. No me gusta mucho cómo están
armadas las funciones, después en algún momento tendré que encontrar la oportunidad para rearmarlas y
escribirlas de forma más prolija. Pero mientras funcionen por ahora, estoy hecho.

Lo siguiente es lograr que la función de ajuste funcione claramente. Que creo que lo está haciendo.
Así que si eso funca, tengo que seguir con la función de Ajuste. Aunque ahí trabajé la función de Ajuste,
si todo está bien ya hace lo que debe hacer. Entonces lo que necesito es tomar los gráficos de 
Distancia en el espacio de parámetro que calculé y calcular los rangos en los que voy a hacer los ajustes.
Otra cosa interesante para hacer sería juntar todos los datos de 10000 agentes y empezar a laburar con
eso, mandar a correr este programa en la máquina de Coimbra. O quizás Algarve. Hoy pareciera que Algarve
es una mejor opción, tiene más hilos libres, como para mandar a correr esto ahí. Empecemos a juntar los
archivos de Comparacion_datos ahí.

El armado del histograma de forma tal que se vea bien normalizado me está costando horrores. No puedo creer
que esté perdiendo tanto tiempo en esto. Ahora creo que lo armé de una manera más razonable, Dios quiera
que esto funque mejor así.

Con esto ya tengo armado todo lo que me anoté el viernes. Debería ahora ir preparando la charla. Una vez que el
código corra bien en la pc de la facultad, voy a ver de pasarlo a Algarve así mando a armar los gráficos ahí.
Aparte de eso, debería ver de implementar correctamente el ajuste. Para eso necesito tener más o menos armados
los rangos de Beta y Cdelta para cada par de preguntas. Si estoy con tiempo, hoy reviso algún par de preguntas
más y veo de armar gráficos con eso.

El código funciona hasta el final, arma el gráfico del paraboloide, hace el mapa de colores de Distancia JS y
hace el ajuste para el caso de las distribuciones sin centro y sin cruz. Ya puedo mandarlo a correr en la pc
de Algarve. Voy a tener que hacer algunos ajustes para eso. En estos últimos 20 minutos, armemos el esqueleto
de la idea de lo que quiero mostrar, mañana lo mando a correr esto mientras voy mirando alguna que otra pregunta
que me parezca más valiosa.

------------------------------------------------------------------------------------------

04/06/2024

Estoy armando la presentación de mañana. Estoy viendo que tengo bastante para mostrar como trabajo,
no tanto como resultados positivos. Siento que buena parte del problema es que estamos mirando
combinaciones de preguntas que dan terrible. Igual vamos a charlarlo con Pablo y ver qué opina al respecto.

No lo encontré a Pablo. Intentemos mejorar los resultados. Para eso hagamos lo siguiente:
1) Corregir el tema de los gráficos de histogramas, que la distancia anotada sea un número con
dos cifras significativas.
2) Aumentar un poco más la distancia de los labels en el gráfico de la superficie 3d.
3) Asegurarme que la distancia de JS graficada se grafique con hasta dos cifras significativas, sino se chocan
los números con las palabras.

Esas últimas dos cosas las puedo dejar para después y ahora ponerme con la parte de buscar pares de preguntas
que me resulten más interesantes.

Ok, estoy sinceramente estancado, tengo que hablar con Pablo para ver qué opina. No sé exactamente cómo encarar
esto.

Necesito una discusión conmigo mismo para organizar mis ideas. El plan es contar a la gente de GOTHAM que hicimos
TODO lo que charlamos la última vez. Normalicé los gráficos, armé histogramas 2D, removí el elemento central de las
distribuciones de las encuestas, hice cálculos de las distancias tanto para casos sin el elemento central como sin
la cruz central. De esto puedo concluir que se puede trabajar ambos casos casi indistintamente, no dan muy distinto.

También considero que no es necesario mostrar seis preguntas, es un montón de bardo y gráficos, con tres estamos más
que hechos. Realicé algunos ajustes, y la verdad los resultados para mostrar son un poco feos.
Estoy teniendo un problema por un lado de que me está quedando los mínimos muy cerca del borde de la
región de ajuste, entonces el valor de Cos(delta) que ajusta está dando mal. No sé si convendría simular
un poco para ese lado cosa de que esos datos ubiquen correctamente el mínimo en 0 o si debería más
bien imponer que el mínimo valor de Cos(delta) sea cero.

Estoy pensando, ¿Tengo una buena respuesta sobre cuál es una distancia chica respecto de dos gráficos?
¿Cómo mido eso rápido? Podría tomar una distribución en dos ejes que arranque igual e ir aumentando
su diferencia migrando todos los agentes a un sólo lugar y ver cómo me da eso. Eso lo puedo probar de
hacer ahora y ver de graficarlo como para tener una idea qué tal da la distancia entre gráficos.

Los ajustes dan un poco más lindo si no encierro tanto el ajuste. Démosle un poco de espacio, quizás
esta ajustando mucho sobre una zona plana.

Hablé con Pablo, organizamos bastante la charla. Me dijo que en vez de mostrar los que más se parecen
y los que menos, muestre el que más se parece y el décimo que más se parece.

------------------------------------------------------------------------------------------

07/06/2024

El 05 estuve preparando la charla con la gente de España. A la mañana tuvimos la reunión de grupo.
No estuve en la pc para anotar lo que charlamos en la presentación. Problemas de internet.
Después de la charla me reuní con Pablo y empecé a armar un cálculo de tiempo de simulación que iba
a tomar el armar el barrido en los espacios de parámetros.

El 06 llegué tarde porque tuve problemas para arrancar el auto. Después también tuvimos la charla
de Diego. No llegué a terminar el cálculo del tiempo de simulación, pero casi.

Hoy a la mañana terminé el cálculo del tiempo de simulación de ambos espacios de parámetros. El
barrido no resulta fino, es algo que tendremos que discutir.

Después me puse a estudiar un poco de alemán.
Lo siguiente que voy a hacer es enviarle a Hugo un mail sobre cómo es el cluster que tienen, cuántos
hilos tienen y cuántas cosas podríamos mandar a correr.

------------------------------------------------------------------------------------------

09/06/2024

Hoy lo que quiero hacer es mandar a correr en las pc's de la facultad el nuevo barrido que hablamos con Pablo.
Para eso, ya que también le voy a pedir a la gente de GOTHAM que mande a correr esto, creo que es importante
modificar el formato de los archivos de salida, cosa de que pesen lo menos posible. Para eso, en vez
de devolver el estado final del sistema, voy a hacer que devuelvan la clasificación de opiniones. Voy
a hacer que además no escriban ni las opiniones iniciales ni la variación promedio. Voy a dejar el dato
de la semilla y la matriz de adyacencia, por si acaso.

Estoy mirando la forma en que se construye la distribución de mis distribuciones de opiniones finales, y me
parece que la forma en que se construye con la función de Clasificación NO es igual a cómo se construye
la distribución de la encuesta. Me parece raro, porque los había comparado y había visto que eran iguales.
Bueno, había entendido todo mal, soy un gil. No puedo ser tan pelotudo. Efectivamente las distribuciones
se arman distinto, la que tengo hecha de las simulaciones está traspuesta respecto a la de las encuestas.
Corrijamos eso ahora. Y estemos atentos a cuál es la forma correcta de hacer esto.
Lo corregí en Python, ahí lo corrijo en el clasificador de C también.

Bien, ya lo armé y funca bien por lo visto. Ahora lo voy a poner a correr en las 3 pc's. Voy a asegurarme
de mandar a correr el espacio que charlamos con Pablo.

Estoy mandando a correr en cada pc 20 simulaciones, por lo que tendré en total 60 simulaciones. Lo que estoy
barriendo son las regiones que etiqueté como B+C+D+F+G+H. La región E la dejo para barrerla al final.

Lo mandé a correr en Algarve y Coimbra, en Oporto no porque faltan 3 simulaciones de lo de los Zealots para
terminar. Faltan dos para el peso siempre boludo. Bueno, si tengo suerte, antes de irme a dormir puedo mandar
a correr en Oporto eso que falta.

------------------------------------------------------------------------------------------

10/06/2024

Lo primero que hice en la mañana es mandar a correr desde Oporto las 20 simulaciones que
faltaban para completar 60. Las últimas 40 voy a ver si se las puedo pedir a Hugo.

¿Qué debería hacer hoy? De lo que tengo anotado como primordial, ya mandé a correr simulaciones
en la pc de la facultad y modifiqué la salida de los datos para que los archivos sean lo más
livianos posibles.

Como prioritario me queda hacer:
1) Realizar ajustes con los conjuntos de simulaciones más similares, armando un ranking agrandando
el conjunto de a poco.
2) Armar un archivo con el análisis teórico hecho para pasárselo a Hugo.
3) Charlar con Capuzzi sobre el cluster de Dirac.

El trabajo interno quizás lo iré revisando ya el miércoles o jueves. El martes podría hacer
lo del análisis teórico. Hoy me pongo con el punto 1 totalmente. Antes que exactamente eso,
lo primero que voy a hacer es organizar el código, hacer que se vea más prolijo, agregar las
funciones al archivo de funciones generales y si todo queda más lindo, ahí hago lo que dijo
David que se arma en dos o tres líneas.

Lo que tengo que hacer es borrar la parte de laburar borrando el centro, sino que tengo que
acomodar para que la función trabaje con preguntas de seis respuestas.
 En la función hice que la distribución de la encuesta siempre salga como una matriz de 6x6.
Independientemente de si lo que recibe son preguntas de 7 respuestas o preguntas de 6 respuestas
o mezclado.

No terminé esto, pero vamos bastante bien. Mañana veré de terminar esto.

------------------------------------------------------------------------------------------

11/06/2024

Estuve trabajando sobre la función que realiza los mapas de colores de forma tal que trabaje
sobre preguntas que tengan seis o siete respuestas y para que además arme los gráficos que hablamos
ayer con Pablo.

Voy a probar que funque bien, que pueda trabajar con un par de preguntas de 7x7, 6x7 y 6x6.
Por lo pronto está corriendo, así que eso es bueno. Ahora me voy a poner a hacer lo que sigue,
que es mandarle un mail a Hugo con los archivos para correr las simulaciones.
Estoy corrigiendo muchos pequeños errores con esto, y eso me distrae del laburo. Es sólo eso,
nada más lo que me distrae.

Armé un mail lo más prolijo que pude y le envié a Hugo con los archivos para simular, así
como una explicación de qué hacen y lo que espero que haga. Veremos lo que me contesta mañana
y eso.

------------------------------------------------------------------------------------------

13/06/2024

El 12 no anoté nada porque estuve trabajando en el google drive para armar la presentación
de el análisis teórico realizado. La idea era terminar eso el 12 para mandárselo a Hugo.
Logré terminarlo y programé el mail para enviárselo a Hugo temprano.

Hoy en la mañana me puse a estudiar un poco de Alemán. Ale nos contó que nació su beba.
Eso significa que el Lunes no vamos a jugar la partida.

¿Qué voy a hacer hoy? Primero, debería mandar a correr lo que calcula la Distancia Jensen-Shannon
y arma gráficos con promedios de conjuntos cada vez más grande. Pongámosle un nombre. Ranking
de promedios de distancias de Jensen-Shannon.

Hasta ahora no hablé con Capuzzi por el tema de los clusters de Dirac. Tengo que hacer eso hoy si
me lo cruzo en el aula Federman.

¿Qué resultados tengo para mostrar? Hice los gráficos de ranking de promedios de distancias
Jensen-Shannon. Lo que tendría que hacer ahora es lo que me decía Pablo, de intentar ver
cómo se componen los estados del ranking. La pregunta es, ¿cómo lo hago?

La idea es que yo tengo mi matriz de Distancias JS, que su número de filas es la cantidad de Cos(delta),
su número de columnas es la cantidad de Betas, y su tercer coordenada es la cantidad de iteraciones.
Para cada simulación, que se identifica con una combinación de estas tres variables, ubico su
distancia JS en un lugar de esta matriz. Yo quiero agarrar un conjunto de estas simulaciones, ordenados
en distancias de menor a mayor, a ese conjunto quiero identificar qué estados son y obtener algún gráfico
que visualmente me indique qué estoy viendo.
 Ponele que estoy mirando las primeras 10 simulaciones, y quiero que arme dos gráficos de frecuencias de
estados. Uno con lo que es el estado más probable, y otro con el segundo más probable. Se entiende por
más probable el que aparece más veces en TODO el espacio.
 Tengo una idea de cómo hacerlo, vuelvo a recorrer todo el espacio de parámetros, uso la matriz que tiene
las distancias JS y en cada vector asignado a una combinación de parámetros Beta, Cos(delta), busco el
décimo elemento ordenado de menor a mayor y me quedo con las iteraciones ubicadas en las posiciones
de los elementos cuya distancia sea menor o igual a la del décimo elemento. Luego, construyo mi matriz
con las frecuencias de esos estados. Una vez hecho eso, voy contando cuántos estados tengo y listo.
También veo de armar un gráfico de frecuencias de estados y palo y a la bolsa. Planazo.

La siguiente pregunta es: ¿Meto esto también en la función de cálculo de distancias JS? ¿Debería separar
esa función en nuevas funciones? Lo meto en la función, siento que separarlo sería un problema en
sí mismo porque cada una de esas funciones nuevas necesitaría que les pase un montón de cosas, sería
un bardo en sí mismo.

#################################################################################################
#################################################################################################

Lo pensé de nuevo, puedo hacerlo en partes separadas. Pero creo que mejor lo dejo para la próxima eso.
Lo voy a hacer después de la próxima reunión. O en el finde quizás.
.) La idea es que la primer función returnee la matriz ZZ de distancias Jesnsen-Shannon.
.) Armo una segunda función que arme los gráficos de ranking de promedios de distancia JS.
.) Armo una tercera que grafique la simulación más similar y la menos similar. Para eso voy
a tener que agregar de forma externa el cálculo del Dic_total que surge de la función de 
Diccionario_metricas. La idea es que eso lo hago una sóla vez, por fuera del for de cada par
de preguntas.

TODO esto, después de la próxima charla.

#################################################################################################
#################################################################################################

Tengo que guardarme la matriz ZZ así como viene originalmente, así que voy a tener que agregar
una nueva matriz que sea la ZZ_sorted.

Arranqué con esto, mañana lo resuelvo y empiezo a armar la presentación.

------------------------------------------------------------------------------------------

14/06/2024

Primero hice lo de armar los gráficos de Frecuencias de estados para los dos estados más dominantes
en el espacio de parámetros. Ahora estoy intentando lograr que funcione, porque estoy teniendo unos
problemas de tamaños en el archivo, no entiendo bien por qué.

Hecho eso, ya me puedo poner a armar la presentación. ¿Qué voy a mostrar en la presentación?

.) Se me ocurre que puedo arrancar mostrando las preguntas a considerar. Ya lo vimos antes, 
pero para recordar, en especial ahora que estamos fijándonos en las preguntas sin la cruz
del centro.

Termino esto mañana.
##############################################################################################

Por algún motivo raro, el código que hace el mapa de colores de FEF para los dos estados predominantes
está teniendo un problema en un caso particular. Por alguna razón para los valores de Beta=0.4 y
Cos(delta) = 0.1 pareciera que al calcular la Distancia Jensen-Shannon, todas las distancias dan lo
mismo. RARO. Voy a intentar printear esa "fila" del vector ZZ y ver qué pasa ahí.

------------------------------------------------------------------------------------------

15/06/2024

Estoy en casa intentando hacer funcionar el código que arma los gráficos de Frecuencia de Estados
Finales rankeados de las dos distribuciones más dominantes. Estoy teniendo problemas con las matrices
construidas, pero no sé por qué y tarda mucho el código para poder mostrarme cuál es el problema.

Puedo seguir hardcodeando mis problemas, pero hay muchas chances de mandarme cagadas. Mejor me pongo
a revisar las cosas de a poco. Voy a desarmar el código de mapa de colores de Distancia Jensen-Shannon
de forma de correrlo una vez y empezar a trabajar con la matriz ya construida de forma más fácil.

La idea es que la función devuelva la matriz de Jensen-Shannon, después otra función con eso grafica
el mapa de colores, otra arma los ranking y una tercera hace la parte de las simulaciones predominantes.
Haré eso entre mañana y el lunes.

------------------------------------------------------------------------------------------

16/06/2024

Armé la función que construye la matriz de Distancia Jensen-Shannon y la returnea.
Algo que estoy notando es que quizás convenga definir cuál es el código x y el código
y por fuera de estas funciones. Mejor hacerlo en el código de Graficar.

------------------------------------------------------------------------------------------

17/06/2024

Logré separar el código en tres partes. Lo cuál es un golazo. Lo siguiente es ver si puedo
descubrir el error que tiene para abrir la matriz. Si puedo hacer eso, puedo revisar entonces
qué es lo que está pasando que falla en el armado de los gráficos de FEF.

Luego de bastante revisar, al final el error que tenía era una boludez, simplemente pasa que
estaba pasando un float para ubicar filas de un array. La cosa está funcionando ahora. Lo que voy
a hacer ahora es volver a mandar a correr las cosas las pc's de la facultad. Y también voy a mandar
a correr las funciones de Graficación en Algarve, para tener los gráficos hechos a partir de las
simulaciones de 10000 agentes.

.) En Algarve voy a mandar a correr todo a partir de Beta=0.9. Así que después tengo que completar
la segunda simulación de Beta=0 a 0.8.
.) En Coimbra, la primer simulación se terminó y estaban en la segunda. Así que ahora mando todo
a partir de Beta=0.3.
.) En Oporto voy a mandar a correr todo a partir de Beta=1.1. Así que después tengo que completar
la segunda simulación de Beta=0 a 1.

------------------------------------------------------------------------------------------

18/06/2024

Estoy armando la presentación para mañana, ya tengo varios de los gráficos. Tengo que ver
de mandar a hacer gráficos de histogramas con los 10000 agentes. En especial porque necesito
un gráfico de un estado de transición para mostrar.

Como de costumbre, Pablo me tiró una idea de qué hacer ahora, que es un bardo de rearmar. Veamos
qué podemos hacer de acá para mañana. Si tengo esto resuelto para una o dos preguntas, estoy hecho.
La idea es mirar el estado promedio. Juraría que cuando hicimos esto, lo que se veía era una cagada.
Debí haber recordado eso cuando charlaba con Pablo.

Tengo que considerar adaptar el armado de los histogramas 2D de opiniones para que acepte el caso de
que las preguntas tengan 6 respuestas en vez de 7.

------------------------------------------------------------------------------------------

19/06/2024

Notas de la reunión:
-------------------

.) El gran dilema es si deberíamos tomar pocas cantidad de simulaciones o no al hacer
los análisis. David propone que deberíamos complementar el análisis de pocas simulaciones
con un segundo análisis indicando la composición de estados en el espacio de parámetros.
.) Método cuantitativo, tomar las distancias, ver la dispersión de distancias. Podría dar una
idea de la comunidad de distancias.
.) Armar un histograma de distancias de Jensen-Shannon. La idea es ver si se pueden conseguir
histogramas que indiquen comunidades.
.) Gráfico de probabilidad de encontrar una configuración en función de su distancia. Esto sería
el primer paso para algo mejor, que es un segundo gráfico que tenga para cada combinación de
parámetros, la probabilidad de encontrar comunidades de distancias similares.
.) Nos están comentando sobre el trabajo con la persona esta, Pablo Etchenique. Es un trabajo
en que analizan las ideologías 

Luego de la charla me puse a cocinar y mi intención era estudiar Alemán. No estudié un carajo.
Cuatro horas se me fueron entre cocinar, limpiar la cocina, comer y cosas. Definitivamente fue
mucho tiempo. No sé qué pasó.

------------------------------------------------------------------------------------------

22/06/2024

Revisé las pc's, las simulaciones casi terminaron hoy, mañana podría mandarlas a las regiones que
faltan. Oporto podría mandarla hoy. Mañana lo que puedo hacer es revisar un poco
el código y ver de armar algo que construya los histogramas de distancias de Jensen-Shannon.

Tener algún argumento para charlar con Pablo sobre si tomar más o menos simulaciones. Revisar
las otras preguntas para tener más info qué conversar. Podría también considerar actualizar
las documentaciones.

------------------------------------------------------------------------------------------

23/06/2024

Algarve todavía no terminó, mañana mando a correr cosas ahí. Esto es lo que tengo que mandar
a correr nuevamente. Ignorando la región 

.) En Algarve voy a mandar a correr todo a partir de Beta=0.9. Así que después tengo que completar
la segunda simulación de Beta=0 a 0.8.
.) En Coimbra, la primer simulación se terminó y estaban en la segunda. Así que ahora mando todo
a partir de Beta=0.3.
.) En Oporto voy a mandar a correr todo a partir de Beta=1.1. Así que después tengo que completar
la segunda simulación de Beta=0 a 1.

En Coimbra se terminó todo, así que ahora tengo que mandar en la región chiquita que faltaba.
En Oporto falta una buena parte de la segunda simulación, ahí la mando a correr.
Ya mandé a correr todo. Ahora me voy a poner a estudiar un poco de Alemán.

Arranqué a construir una función que arme los histogramas en el punto de distancia mínima. El
problema con eso es que ese punto medio depende de la cantidad de simulaciones que tome. Y hacer
un histograma con 10 simulaciones no tiene mucho sentido. Necesito definir bien qué critero voy a tomar.
Eso lo haré mañana charlando con Pablo. Y también debería tener un buen argumento sobre si tomar
muchos o pocas simulaciones. Igual posiblemente lo mejor sea hacer lo que dijo David,
hacer los dos análisis en simultáneo al mostrar los resultados.

------------------------------------------------------------------------------------------

24/06/2024

Algarve todavía no terminó, así que habrá que ver de mandar a correr cosas el miércoles
o algo así.

Volviendo al tema de los histogramas de distancias, me parece que lo mejor es arrancar con
histogramas de distancias en los puntos de mínima distancia promedio, utilizando 100 distribuciones
para el histograma.
 Ahí tengo armados estos histogramas. ¿Expando la región que observo? Me parece que hacer un histograma
de cada punto en el espacio de parámetros es un montón. Una gran pregunta es después cómo encontrar
máximos o cosas así. Pero para saber qué quiero buscar, voy a necesitar revisar un poco qué forma
tienen los histogramas obtenidos. Quizás quiera revisar ciertos percentiles o cosas así.

Si hoy no llego a reunirme con Pablo, siento que podría ponerme a revisar un rato el tema de
por qué mi código va tanto más lento que el de Hugo y la gente de Gotham. En cuanto logre armar
los histogramas hago eso.
 Viendo los histogramas, tengo un gran pico con distancias grandes, y un pequeño conjunto
de estados con distancias pequeñas. Voy a tener que ver cómo reconocer esos conjuntos de
datos. También considerando que por ejemplo en el caso del par de preguntas no-políticas,
la distribución de distancias es simplemente un único pico.

Ya revisé el código que tengo en el src. Corregí la función de Clasificación para que funcione
correctamente. Ahora puedo empezar a jugar con esto para intentar ver qué es lo que está funcionando
mal. Hice una corrida de 10000 agentes con 500 pasos, tardó 11 minutos y medio. Probemos con 1000
agentes. Esto tarda 139 segundos. Puedo intentar trabajar con esto para intentar reducir los tiempos
de simulación. Veré de trabajar en esto mañana. O quizás hoy lo mire un poco en casa, ya veré.

------------------------------------------------------------------------------------------

25/06/2024

Hoy a la mañana estuve estudiando Alemán. Después fui a cursar, volví de eso y me encontré con Nacho.
Terminado eso, intenté ponerme un poco con el código que hice yo, pero no tuve mucho tiempo. Revisé
de nuevo de a poco las cosas iniciales, es todo igual. Así que debería empezar a retocar el RK4 para
ver qué pasa ahí.

------------------------------------------------------------------------------------------

26/06/2024

En Coimbra tengo todas las simulaciones que quería resueltas. También tengo lo que me pasó Hugo,
puedo empezar a revisar de compilar todos los datos y simulaciones juntos.

En Algarve mandé a correr las segundas iteraciones, barriendo Cos(delta) [0.16, 0.5] y Beta [0,0.8].
Cuando eso termine, voy a tener que mandar el pedazo de región que falta, la región E. Que va
con Beta [1.1,1.5] y Cos(delta) [0,0.14].

En Oporto se están terminando las segundas iteraciones, barriendo Cos(delta) [0.16, 0.5] y Beta [0,1].
Acá también voy a tener que mandar el pedazo de región que falta, la región E.

Veamos de ir armando el conjunto de datos para revisar en Coimbra.

En la carpeta de Barrido_final puse los datos que tenía reformateados, para que coincidan con todos los
datos que vengo construyendo. Hay algo que podría ser un leve problema para el futuro, y eso es que
estos archivos no tienen un espacio final en los datos que sea una tabulación. Mis códigos están armados
contemplando eso. Pero creo que lo puedo solucionar fácil, tengo que considerar simplemente que lo
que levanto son los valores de las distribuciones, las cuales tienen tamaño 42x42. Entonces simplemente
puedo pedir que cuando construya la distribución, esta tome valores del array de 0:42*42 y listo.

Más problemas creo que me va a traer TODO lo demás, como el Diccionario_metricas o el graficado de los
histogramas 2D, los cuales ahora tendría que readaptarlos a las nuevas distribuciones. O, mejor idea,
puedo armar una función que a partir de las distribuciones que estoy observando, reconstruya las opiniones
de los 10000 agentes, de esa manera tengo ese array de opiniones y ya de ahí no modifico nada de todo
lo demás. Soy un genio. Suena como lo más sensato, en vez de cambiar todo el código, cambiar simplemente
eso.

Voy a mandarle un mensaje a Pablo para juntarnos, mientras termino de subir todos mis archivos a Coimbra.
Ya tengo armados los archivos reformateados de la región inicial. Ahora voy a subir lo que ya tengo
a Coimbra. Lo subo a Coimbra porque es ahí donde se terminó el barrido del espacio de parámetros que
queríamos hacer. Faltará combinar las cosas de Algarve y Oporto, cuando esas terminen de correr.

Hablando con Pablo, lo que charlamos es primero arrancar estudiando cuál es la composición de estados
en el primer cluster de distancias. También querríamos ver si el mínimo de las distancias promedio
se encuentra en el mismo lugar al tomar ese pequeño conjunto de distancias.

Arranco analizando la pregunta de Impeachment vs Wall with Mexico. Para esta pregunta considero las
distancias menores a 0.4. Eso es un pequeño conjunto de distancias. Lo primero que quiero hacer
es entonces estudiar la composición de esos estados.
