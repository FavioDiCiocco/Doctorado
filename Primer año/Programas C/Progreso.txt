09/05/2022

Ayer y anteayer estuve mirando el código por arriba. Ya hoy lo agarré y
lo emprolijé borrando varias cosas comentadas. Entre ellas, creo que lo que
estuvo bien borrar son las matrices de agentes activados y la de grado medio
de las redes.

Ahora, yo quiero mandar a correr el sistema, teniendo este opiniones positivas,
un campo externo y un coeficiente que regule la intensidad con la que
las opiniones caen a cero. El coeficiente ya lo agregué y vale 1.
Para que las opiniones sean postivas reemplace el uso de la función Gaussiana
por una distribución uniforme entre [0 y 5]. Me pareció que 5 sería razonable.

Para el campo externo, tengo que agregar un término a la ecuación dinámica que
se sume a todo el resto de agentes, se me ocurre que se sume justo antes
de rearmar la red y que la intensidad del campo externo sea tres o cinco.
Podría barrer en eso también.

¿Qué datos quiero mirar? Me interesa en principio ver las opiniones iniciales
y las finales, la Varprom y el comportamiento de los testigos.

Bien, ya preparé para que se construyan dos archivos, uno que es Opiniones
y toda la perorata, que guarda los siguientes datos:
Opiniones iniciales
Varprom
Opiniones Finales
Semilla

El segundo archivo, que es Testigos y la perorata, guarda las opiniones de
diez testigos de manera que cada columna es la evolución de las opiniones
de un testigo. En realidad cada dos columnas.

Todavía queda agregar el campo externo y ya podemos empezar a ver qué hace esto.
Lo agregué al campo como una influencia que surge en cada iteración previa a que
el sistema rearme la red. 

Creo que está armado el código, lo podemos mandar a correr. Ya revisé y corregí
los errores con el Cygwin. Voy a barrer en el campoext y en lambda, que es la
intensidad del decaimiento.

Ya el programa lo mandé a correr, pero está teniendo problemas. Por algún motivo,
las opiniones no se actualizan. Y además, el cálculo de Varprom me da "inf".

---------------------------------------------------------------------------------

10/05/2022

Hoy es el primer día que vine a la facultad, me instalé en la oficina, descargué
programas, instalé Github Desktop y Notepad ++.

Después pasé el resto del día trabajando en el archivo de main para que el modelo
labure en el caso de tener opiniones positivas en su distribución inicial.
Estuve un buen rato porque había problemas en el código que me costaban encontrar.
Al final lo que ocurría es que estaba pisando un vector no declarado, que es el
vector de los agentes activados. Un vector que había eliminado pero que estaba siendo
llamado en una de las funciones de las redes de actividad. Creo que en la función que
establece los enlaces de los agentes activados.

----------------------------------------------------------------------------------------

16/05/2022

Entre el martes pasado (10/05/2022) y hoy lunes (16/05/2022) no hubo mucho trabajo
porque estuve yendo a la TREFEMAC, mis viejos se fueron y hubo que cuidar a los
perros. Fueron días movidos y complejos. Esta semana me voy a mudar, así que
vamos a ver qué podemos avanzar. Encima está el trabajo del Labo de Datos.
Está fuerte la semana.

Volviendo al trabajo, hoy ya me puse a armar datos del modelo en tres casos:
1) alfa = 0, Cdelta = 0.
2) alfa = 2, Cdelta = 0.
3) alfa = 2, Cdelta = 1.

Lo siguiente es tomar estos datos y armar los gráficos de distribución de opiniones
en el espacio de tópicos.

Ya logré armar los gráficos con los datos que hice. Falta ajustar un poco los gráficos,
pero en líneas generales están. Lo que se observa es un que en los casos de consenso
las opiniones forman una nube cerca del cero en la región positiva del espacio de
tópicos. Algunas opiniones llegan a valores negativos, estudiar eso cómo ocurre.

Por otro lado, los gráficos de distribución de las opiniones 2D en los casos de
polarización descorrelacionada y estado ideológico son indistinguibles. Además,
hay mucha variabilidad en el tiempo. Bah, habría que ver qué significa "mucha".
En principio el programa puede tardar entre 15 y 40 unidades de tiempo.

Lo siguiente sería ver si podemos restringir el espacio de los valores observados
de manera de enfocar mejor el gráfico hacia los datos graficados y que no
sea tanto un gráfico vacío.

Hecho eso, habría que analizar porqué algunos valores se vuelven negativos,
evaluar si los resultados obtenidos tienen sentido con lo esperado y si
habría que hacer nuevos gráficos o modificar el programa.

------------------------------------------------------------------------------

24/05/2022

Lo primero que hice a la mañana fue ajustar los gráficos para poder observar
mejor la distribución de opiniones en el espacio 2D. Si alfa es alto, para casos
de correlación cero o alta correlación, el sistema se comporta exactamente igual
y polariza en el primer cuadrante.

En cambio, para casos de controversialidad negativa el sistema se comporta tal cual
se comportó siempre, polarizando en los cuadrantes 2 y 4. Claramente las condiciones
iniciales no logran determinar el comportamiento del sistema para que este se
encuentre confinado al primer cuadrante.

Luego de eso estuve el resto del día estudiando la expresión de la ecuación dinámica
de la bifurcación transcrítica. Leyendo del Strogatz, básicamente se observa que
esta bifurcación se caracteriza porque dos puntos fijos intercambian estabilidad,
siendo que un punto fijo estable se vuelve inestable mientras que uno inestable
se vuelve estable. Esta ecuación aplicada a nuestro modelo permite que existan dos
puntos fijos, el 0 y el punto fijo (mu/K). Dependiendo del signo de mu, el cero
será un atractor o un repulsor. En particular, si mu es positivo, el cero es un
atractor, mientras que si mu es negativo, el cero es un repulsor. Y respectivamente,
el otro punto será un repulsor o un atractor.

Se me ocurre que esto nos permite proponer un nuevo comportamiento al sistema donde
ahora el primer término de la ecuación dinámica no es únicamente la tendencia de los
agentes a perder su convicción en el tema, sino que puede ser su apatía o interés en
los tópicos y por tanto se puede definir mu positivo para tópicos aburridos y 
mu negativo para tópicos interesantes.

Este análisis junto con la adaptación de la ecuación dinámica del modelo a la
ecuación dinámica de una bifurcación transcrítica están anotados en mi cuaderno.

El jueves sería ideal arrancar modificando la ecuación dinámica en el código,
remover las cosas sobre cálculos de tanh y empezar a observar el comportamiento
del sistema para distintos valores de mu, alfa y cos(delta). Luego de eso,
decidiremos el tema de cómo interpretar este nuevo parámetro. Hagamos la
documentación de esto a medida que voy haciendo los cambios.

---------------------------------------------------------------------------------

25/05/2022

Estuve un rato mirando el tema de la beca y del trámite de título.
Lo importante para hacer es en algún momento charlar sobre anotarme en la 
carrera de doctorado. Eso charlarlo con pablo.

Por otro lado, ya borré todo rastro de la tanh en el código. Modifiqué
la ecuación dinámica para calcular el cuadrado del producto entre la matriz
de superposición y el vector de opinión del agente j. Lo siguiente es empezar
a ver el comportamiento del modelo ante la variación de los parámetros
mu, alfa y Cdelta.

Estuve probando cosas en bash para poder pasar números con fracción directamente
desde Bash a mi programa de C y no tener que andar pasando enteros y sobre esos
armar una fracción para conseguir el número decimal deseado. La cosa es que en
Prueba.sh dejé anotado en los comentarios como se usa el comando bc, por lo menos
de forma rudimentaria. Y también como obtener los datos de un array. Con eso
ya puedo empezar a armar mis arrays en los archivos de Instanciar y desde ahí
pasar directo los valores de alfa y Cdelta deseados.

Con eso hecho, me puse a armar algunos datos del modelo con la ecuación dinámica
adaptada según la ecuación dinámica de una bifurcación transcrítica. Esto salió
mal. El término cuadrático potencia mucho a los agentes con opiniones mayores a
1, y esto hace que refuercen mucho a sus vecinos y el sistema entra en una espiral
de refuerzos que no frena más. Cada vez se aleja más y más. Ese término
cuadrático así pelado no va a servir. Necesitamos algo que contenga a las
opiniones en la región [0,1] para que se pueda usar. Se me ocurrieron formas
de normalizar la opinión, pero creo que hasta charlarlas con Pablo no tiene mucho
sentido explorarlas demasiado.

Hay una en particular que se me ocurrió y creo que estaría interesante. Espero
que Pablo y Sebas opinen lo mismo. Podría mañana mandarle un mensaje a Sebas y
ver qué piensa él al respecto.

----------------------------------------------------------------------------

30/05/2022

Agregué el término de normalización que entiendo es el máximo valor de opinión
que cualquier agente puede alcanzar, y eso sólo es posible si se conecta
con toda la red y además si toda la red está con opinión 1. Claramente este
es un máximo inalcanzable y por eso es que las opiniones de los agentes van
a estar muy tiradas al cero. Podríamos probar y ver si usando valores más
bajos de mu o valores más altos de K permiten que las opiniones de los
agentes despeguen un poco del cero. Pero esto es especulación, hay que hacer
las cuentas.

Para mencionar, también modifiqué en avanzar el Din2, tenía un else para un
if que no hacía realmente nada. Era cuando calculaba la sumatoria de opiniones
del segundo término. Si los agentes estaban conectados aplicaba la función
Din1. Si no estaban conectados, sumaba cero. Pero para sumar cero, directamente
ni pongo el else y listo.

También voy a ir armando carpetas para cada tipo de normalización, cosa de no
confundir los archivos. La carpeta con la normalización sobredimensionada es
Norm_Maxima.

Ya probé el modelo con Norm_Maxima. En el caso de mu positivo el sistema logra
converger y alcanza estados de consenso. Básicamente, lo que está pasando es
que las opiniones son lo suficientemente bajas como para no poder hacerle frente
al término que los tira al cero. En el caso del mu negativo, incluso para el caso
de alfa cero, el sistema eventualmente se dispara. El motivo es que la normalización
con alfa cero es simplemente 1. (Mu = 1). Entonces lo que ocurre es que la derivada
queda igualada a un término que es siempre positivo y por tanto crece en cada iteración.
Las opiniones se escapan.

Me parece que no podemos simplemente plantear el mu en valor positivo. Pero entonces
esta ecuación del punto de bifurcación no sé si tenga sentido de ser usada.

------------------------------------------------------------------------------------

01/06/2022

Lo primero que hice fue copiar el archivo de ActividadConexa.py y lo renombré
estudio_transcritico.py. Este segundo archivo lo usé para armar los gráficos de
Varprom, Tópicos vs T e Histograma 2D de los datos del modelo transcritico normalizado
de forma sobredimensionada. Esto tomó un rato y me hace ver que ese código podría
necesitar una buena pulida para verse mejor. Cosas como el armado del SuperDiccionario
necesitan realizarse mejor. Pero eso quedará para otro momento.

En la tarde estuve haciendo un análisis de las ecuaciones dinámicas del modelo para
el caso de dos agentes. En principio, no llegué a nada todavía. Claramente el estudio de
este tipo de ecuaciones dinámicas es complejo y no se puede encarar directamente.

----------------------------------------------------------------------------------------

02/06/2022

Se me ocurrió probar de estudiar el comportamiento del sistema en el caso de que
los dos agentes conectados tengan opiniones similares. Me suena razonable
como suposición si considero que las opiniones arrancan en el espacio positivo de
valores y que sólo van a reforzarse. Además, cada uno crece en función de la opinión
del vecino, entonces si sus opiniones tienen valores diferentes, el que tenga opiniones
más pequeñas crecerá más en una iteración que su vecino porque el que tiene opinión más
baja recibirá un impulso más grande de su vecino que tiene opiniones más grandes.

Estuve mirando las ecuaciones dinámicas e intentando estudiar el comportamiento del
sistema para el caso de 2 agentes conectados. Llegué a la conclusión de que para 2
agentes conectados, el sistema presenta dos puntos fijos, uno en el cero y el otro
en el borde que separaba las regiones de consenso y polarización según el paper de
Baumann. Al principio me pareció trivial, pero no creo que sea un resultado tan
trivial si consideramos que partí de una ecuación dinámica completamente diferente.
Quedaría estudiar la estabilidad de estos puntos, cosa que no hice todavía.

Antes de eso, me adelanté a estudiar el comportamiento del sistema con tres agentes.
Creo que eso resultaría bastante interesante y daría una buena comprensión de qué
está pasando. Espero sea resoluble. Lo siguiente sería empezar a simular el modelo
en estos casos y ver que los resultados coinciden con lo predicho analíticamente.

Y de paso, hoy a la tarde asistí al coloquio de física de Luz Ricci (nombre de la
expositora). Estuvo interesante, fue sobre análisis de materiales nanométricos
a través de métodos ópticos, algunas aplicaciones y métodos desarrollados. Eso
es un resumen muy breve.

---------------------------------------------------------------------------------------

03/06/2022

Durante la mañana y la tarde continué el análisis de las ecuaciones dinámicas
del modelo para el caso de bifuración transcrítica. Logré llegar a la conclusión
que en ambos casos el sistema presenta dos puntos fijos, si se toma como 
condición que para cada agente las opiniones en ambos tópicos son iguales.

Uno de estos puntos fijos es el cero, el cual puede ser un atractor o repulsor
dependiendo de si el coeficiente mu es positivo o negativo respectivamente.
El otro punto fijo toma el mismo valor que tomaba el modelo de Baumann para
el límite entre la región de consenso y la región de polarización. Este punto
fijo presenta autovalores positivos y negativos, con lo cual resulta un punto 
inestable que aleja al sistema en una dirección y lo atrae en otra. Esto me lleva
a pensar que no hay forma de que el modelo así como está planteado pueda ser
estable o converger a nada. Por tanto necesita fuertemente una revisión.

------------------------------------------------------------------------------------

06/06/2022

Hoy estuve leyendo el paper que Pablo me pasó sobre dinamica de polarización.
Anoté un resumen sobre esto en el archivo de resúmenes en la carpeta de Tesis
en el drive. Debería pasar los resúmenes del resto de papers a ese archivo y
ya usar ese archivo directamente.

------------------------------------------------------------------------------------

09/06/2022

Voy a hacer simulaciones del modelo para el caso de 2 agentes y 3 agentes. Para
eso necesito modificar el armado de la matriz de adyacencia, quitar todo el tema
de las actividades y agregar una condición de que el código corte si las opiniones
se alejan demasiado. También voy a necesitar carpetas nuevas en las que almacenar
estos datos.

Ya tengo armadas las carpetas y actualizada la documentación. Ahora puedo ponerme
a modificar el programa C. Después veré de repasar el archivo py cuando haga los
gráficos de esto y borraré todo lo innecesario, dejando el archivo .py previo
como la versión que contiene todo. Cosa de tener a donde retomar lo que busco.

Las modificaciones que tengo que ahcer son:
.) Cambiar el número de agentes. (Eso se hace en Instanciar.sh) DONE
.) Cambiar el armado de la red de Adyacencia, hacerla de forma manual. DONE
.) Quitar el uso de actividad. Fijarse que eso no genere warnings de 
variables definidas pero no usadas. DONE y DONE
.) Cambiar la carpeta en la cual se guardan los datos. DONE
.) Quitar la normalización de la ecuación dinámica. DONE
.) Agregar una condición de corte de la simulación si las opiniones
crecen demasiado. DONE
.) Modificar a los testigos, porque ahora sólo tengo tres agentes.
Incluso, podría sacarme de encima el guardar datos iniciales y finales
de la matriz porque los testigos tendrían todo. DONE

Creo que eso es todo lo que hay que hacer.

Ahí están anotados los que resolví, mañana continuar con lo de agregar la
condición de corte. Me quedé mirando la parte del while para ver que esto
corte bien. Desconfío de que el sistema cumpla correctamente esa condición.
Probar algo así en Prosem para ver que todo funque correctamente.

(Por alguna razón esa última frase no se guardó bien)

--------------------------------------------------------------------------------

10/06/2022

Terminé de hacer modificaciones al código en C para poder realizar simulaciones
de 2 y 3 agentes. Con esto ahora voy a observar si el sistema converge al cero
o luego diverge. Para los casos que diverge, el código debería cortar si la
norma del vector de opiniones es mayor al valor que tendría si las opiniones
de cada agente en cada tópico fuera 100. Haré 20 iteraciones para 2 y 3 agentes
con controversialidad variando de a 0,05 entre 0 y 0,2, y luego controversialidad
1. El Cdelta será 0 siempre y mu será 1 y -1.


Completó todo en segundos, ya después empezaré a ver los resultados de los datos.
Seguramente lo haga ya el lunes o martes eso.

---------------------------------------------------------------------------------

13/06/2022

En la mañana organicé algunas cosas con el grupo de Labo de Datos, hablé con
Lupi y eestuve armando el archivo de Conjunto_pequeño.py para estudiar los datos
simulados el otro día. Tome el de estudio_transcritico, lo copié y empecé a sacarle
todo lo que no fuera a usar en este caso. Estoy esquilmando el código para arrancar
nuevamente de cero, cualquier cosa que pueda necesitar después la copiaré. Además,
estoy independizándome del SuperDiccionario e implementando el uso de pandas
para manejar los nombres de los archivos.

Todavía queda revisar la mitad del código, pero buena parte de eso hay que simplemente
borrarla.

-----------------------------------------------------------------------------------

23/06/2022

Cómo pasaron 10 días de la última entrada me asusta un poco. Yendo a lo que hice hoy,
estuve revisando la ecuación dinámica e intentando pensar en soluciones para que no
crezcan de forma desmedida las opiniones de los agentes al interactuar. Para esto
se me ocurre modificar la función cuadrática por una función logarítmica. La función
logarítmica tiene una derivada que se hace más chica a medida que x crece, de forma
tal que si las opiniones crecen mucho esa derivada se hace pequeña. 

Para estudiar esto el proceso será parecido al anterior. Primero veré la ecuación
dinámica del punto transcrítico. Luego, pasaré a estudiar la ecuación dinámica aplicada
a nuestro modelo. Habrá que considerar los dos casos de aplicación del logaritmo que se
me ocurrieron y ver las diferencias de cada uno.

Por otra parte, Pablo propone ir pensando en cómo plantear el modelo en versión estocástica.
Para esto podés arrancar tomando parte del código que ya tenés de esto y re adaptarlo. Eso
suena lo más lógico.

También hay que considerar en los planes y en el tiempo el estudio para el final de
la materia de Dorso, que puede o no darse.

--------------------------------------------------------------------------------------

06/07/2022

Entre el 23 y hoy no es que estuve rascándome, el preparar el TP final de Labo de Datos
fue complejo. Al final Cami nos carreó como la mejor. Se merece un chocolatito o algo.

Cuestión que hoy estuve todo el día analizando la ecuación dinámica del punto transcrítico
para el caso en que la ecuación tiene un logaritmo en vez de una cuadrática. Claramente
la ecuación fue más compleja de resolver. Lo que descubrí con esto es primero que 
la aparición de un punto fijo en los valores positivos ya no depende del signo de "r"
sino de su módulo. Para "r" negativo o 0, el único punto fijo existente es el cero y
es un punto estable. En cambio, si "r" es mayor a cero y menor a uno, se tiene un 
punto fijo estable en el cero y un punto fijo inestable en un valor positivo de x. Si bien
no pude despejar el valor de este punto fijo en función de "r", sí pude despejar "r" en
función del punto fijo, de forma tal que puedo construir una ecuación para que tenga
un punto fijo en un valor deseado.

En el caso de "r" igual a uno se tiene que sólo el cero es un punto fijo y que además
es un punto silla, que siempre empuja al sistema hacia los valores positivos.

Finalmente, si "r" es mayor que uno se tiene que el cero es un punto fijo inestable
y por otro lado se tiene un punto fijo entre menos uno y cero, el cual se vuelve
estable. Nuevamente, si bien no se puede calcular el valor de este punto fijo en
función de "r", se puede sí calcular "r" en función del punto fijo y de esa manera
armar una ecuación a medida.

Luego estuve intentando resolver la ecuación dinámica aplicada a dos agentes conectados,
sin embargo esto resultó ser bastante más complicado de lo que parecía ya que al
querer hacer eso me surge una ecuación que tiene un logaritmo dentro de un logaritmo.
Un bardo importante.

Mañana vamos a intentar continuar con esto y resolverlo mejor. Hay que tomar en cuenta
que entonces este modelo no presenta punto fijo estable en los valores positivos de x.
Hay que ver cómo se puede crear un punto estable en los valores de x positivos. Seguramente
la separación de "r" en dos valores "K" y "mu" me permite más fácilmente jugar con la estabilidad
de esos puntos

---------------------------------------------------------------------------------------------------

13/07/2022

No comprendo cómo pasan tan rápido los días, simplemente no sé qué pasó entre 
la última vez que escribí y hoy. Pero vamos a intentar recordarlo. En la libreta
está anotado lo que pasó, ahora todo tiene sentido.

Cuestión que seguí trabajando la expresión dinámica del modelo con el logaritmo
en vez de la tanh. Esta vez los resultados son un poco más prometedores, se
observan dos puntos críticos, uno en cero y el otro en un valor positivo.
La existencia de este segundo punto crítico y el hecho de que sea positivo
no es algo trivial, pero con algo de cuidado creo que se puede lograr que se 
encuentre donde queremos en general. 
  Lo importante entonces es que estos dos puntos fijos obtenidos son o atractivos
  o repulsivos y dependiendo del signo de la variable mu alternan. Igual no hay que
  olvidar que los resultados que obtuve son para el caso más simple de todos, con
  varias aproximaciones. Hay que ver qué pasa después.
  
  
Ayer estuve armando un resumen con todo lo que analicé de esta ecuación dinámica
para poder mostrárselo a Pablo y a Lupi. Tengo hechos los gráficos y toda la
bola. Hoy lo charlamos rápido con pablo y me recomendó que arme una presentación
de esto y de los resultados obtenidos del sistema cuando reemplazamos la tanh por
una cuadrática. Así que ya mañana me pondré con eso.

Por otro lado, estuve trabajando en el archivo de python de Conjunto_pequeño.py.
Logré hacer que grafique, pero luego de mirarlo noté que los gráficos de Varprom
me graficaba una sóla iteración del sistema, cuando debería graficar la de cada
sistema. Esto me pareció raro, así que miré los archivos a ver qué pasaba. Y
por algún motivo desconocidos, pareciera que los datos armados son los mismos
exactos para las simulaciones dado un valor de N, alfa, cdelta, y mu. Es
muy raro eso, tengo que revisar qué pasó.

También noté que no tengo armada una documentación de la carpeta de trasncritico
y de la nueva carpeta que puse en imágenes. Debería corregir eso. Aunque quizás
haga esto a lo último, preparar la presentación tiene prioridad.

Lo otro que estaba pensando es que debe existir un entorno tipo anaconda o spyder
para trabajar python en Linux. Trabajarlo desde línea de comando es una paja, el
programa corre y no tenés mucho para hacer para chequear qué anda mal. Voy
a buscar mañana algo que sirva para eso, así ya trabajo más cómodo en la facultad.

La otra es mudar las cosas a notebooks, ir probando ahí y después descargarlo
en la pc. Pero me parece mucho laburo y una cagada francamente. Además tendría
que organizarme el drive. Y ya tengo tres grandes cosas que organizar, el drive,
el mail y algo más que ya no recuerdo.

Ya tengo marcado el laburo para mañana, mañana lo continuamos. Y mañana a
primera hora me anoto lo que hice en el cuaderno.

-----------------------------------------------------------------------------------

14/07/2022

Revisé los datos y al parecer el problema era que como el sistema tenía pocos
agentes, el código se resolvía muy rápido y no llegaba a cambiar la semilla.
Entonces, todos los archivos tenían la misma semilla y por eso arrancaban
con las mismas exactas opiniones.

Analizando los gráficos, se observa que los de Varprom tienen un pico al principio
de la curva. No puedo comprender qué genera ese pico. Lo miro y simplemente no lo
entiendo.

Dicho esto, ya puedo ponerme a trabajar directo en el tema de armar una presentación
de esto. Así que eso haré.

En la tarde estuve viendo unos videos sobre los temas de clases que iba a dar, así
que no pude trabajar tanto. Lo que hice fue cambiar el gráfico de histograma 2D por 
unos scatter plot, me parece que resultan mejores para observar cómo se distribuyen
las opiniones finales de los agentes en el espacio de opiniones. Podría incluso 
graficar las trayectorias, aunque creo que el gráfico quedaría sobrecargado. Probar
eso mañana, no es muy complejo.

Hecho eso, ya tengo todo lo necesario para hacer la presentación que me dijo Pablo.
Para eso tengo que:
.) Preparar datos para el modelo con la función logarítmica.
.) Graficar esos datos.
.) Actualizar la documentación acorde.
.) Armar la presentación:
  -) Arrancar con la ecuación original y lo que teníamos
  -) Ecuación dinámica transcrítica con función cuadrática
  -) Puntos fijos de la ecuación dinámica transcrítica y su análisis
  -) Análisis de la ecuación para el sistema con dos y tres agentes
  -) Gráficos obtenidos a partir de los datos y su interpretación
  -) Mostrar el código para que no queden dudas sobre lo hecho
  -) Ídem pero con la ecuación dinámica con función logarítmica

Me parece una buena presentación esa.

-------------------------------------------------------------------------------------

15/07/2022

A la mañana me quedé en casa para ir a buscar el DNI y revisé los papeles y cosas
que necesito para inscribirme al doctorado, ya la semana que viene empiezo a juntarlos.

En la tarde vine a la facultad, resolví varias cosas pequeñas como pagar la cuota
del lavarropas, hablar con Pablo sobre la carta a la comisión del doctorado y 
completar la documentación faltante de los archivos de las carpetas de imágenes,
Programas y la de Programas Python. 

Finalmente modifiqué el archivo main cosa de que la ecuación dinámica tenga el
logaritmo en lugar de la tanh. También armé la carpeta Trans_ln para ir guardando
los datos de las simulaciones. Después tendría que simplemente tomar el archivo
de Conjunto_pequeño y modificarlo cambiando la dirección de la carpeta cosa
de que arme los nuevos gráficos. Es una boludez de hacer.

Estuve pensando que si mi objetivo es corroborar las simulaciones con los resultados
analíticos, entonces debería calcular dónde caen los puntos fijos en función
de mis parámetros y ver si tiene algún sentido. Igual, no creo que sea algo tan
directo de ver.

Ya el lunes armo los gráficos y empiezo con la presentación. El martes debería
estar, dale, pongámonos las pilas. Y ya después de eso arrancamos con el planteo
del sistema con una evolución estocástica. Y si se puede, y encontramos un momento,
revisar el archivo de graficación de python. No me gusta cómo está planteada esa
cadena de fors. Tiene que haber algo más sencillo.

------------------------------------------------------------------------------------

18/07/2022

Hoy estuve revisando los gráficos armados por el modelo de la ecuación dinámica
transcrítica para el caso de la función logarítmica. Lo primero que se me ocurrió
es modificar lso valores de mu y de alfa barridos cosa de observar la aparición
de los puntos fijos y el cambio de estabilidad. Para eso modifiqué los mu's para
que varíen de 0.1 a 0.3 y alfa lo hice variar entre 0 y 1 de a 0.2, sin poner
el 0.6 o 0.8. Esto es porque si alfa vale cero, los tres valores de mu son mayores
que alfa, si vale 0.2 tengo un valor de alfa menor, uno igual y uno mayor, y
si alfa es 0.4 tengo tres valores de mu menores que alfa. Todo esto porque la 
relación entre alfa y mu es lo que determina si el punto fijo estable es un
valor positivo de opinión o si es el cero. Para que la aparición de este
punto fijo dependa de una relación 1 a 1 entre alfa y mu le cambié el valor
a K de 3 a 1.

Los gráficos de esto muestran que el valor del punto fijo estable positivo
depende de la relación entre mu, alfa y la cantidad de agentes del sistema.
Para 2 agentes, si mu < alfa el punto fijo estable es el cero, mientras que
en caso contrario el sistema busca un punto fijo positivo. En cambio para
3 agentes mu apenas más pequeño que alfa no parece alcanzar para que el
sistema decaiga completamente al cero.

En todo este análisis también observé que el armado de la matriz de Adyacencia
estaba mal hecho. Eso hacía que algunos agentes se conecten pero otros no.
Eso afectó tanto a estos datos, cosa que corregí al final, como a los datos
de Conjunto_pequeño, los cuales todavía no corregí. Esos necesitan ser corregidos
mañana para observar mejor qué pasaba con ese sistema.

Otra cosa que observé es que el Spyder me come completamente la RAM. No comprendo
por qué. Tendré que ver otra forma de hacerlo funcionar o si existe alguna solución
para que esa memoria se libere, porque se vuelve insostenible. Básicamente si dejo
abierto el spyder mucho tiempo, la RAM se llena progresivamente. Al cerrarlo, 
se libera todo de una. Es una bosta eso.

Ya mañana debería ver de armar la presentación completa. Para eso sólo faltan
los gráficos de la ecuación transcrítica con función cuadrática. Mañana
concentrémonos en eso únicamente, así ya tengo la presentación hecha y se
lo menciono a Pablo.

Después veré de solucionar el tema del consumo de la RAM. Quizás use algún
notebook como hacen los demás.

También modifiqué los gráficos de opiniones finales en el espacio de tópicos
para que ahora agreguen las trayectorias de las opiniones de los agentes.
Eso soluciona el tema de las escalas raras y además permite seguir mejor
el comportamiento del sistema.

--------------------------------------------------------------------------

19/07/2022

Al principio en la mañana me pelee con el hecho de que los gráficos del
sistema con la ecuación dinámica transcrítica y función cuadrática no
se veían. Cuestión que el número de Itextra y pasosprevios eran más
que suficientes para que el sistema explotara y no pudiera ni ser
graficado. Reduje bastante eso, y aún así el sistema alcanza valores
de opiniones o de variación promedio absurdamente altos. Pero por
lo menos son graficables.

Después estuve retocando los gráficos del modelo de baumann original
aplicado sobre valores iniciales de "interés" distribuidos sobre una región
positiva. Esto lo hice para que se vea un poco mejor al ponerlo en la presentación.

Le mandé un mail a Mariano respecto del tema de la carta que tiene que
mandar Pablo a la comisión de Doctorado. Esta semana tengo que tener
eso hecho.

Avancé con la presentación. En sí sólo hice el primero de los ítems que
tengo anotados, sin embargo me gusta cómo está quedando y es cierto que
el armado de la presentación toma tiempo por la parte de seleccionar los
gráficos, emprolijarlos, organizar la data. Creo que trabajé bien.

Mañana tengo que seguir con eso. Y ya el jueves me pongo con lo de la 
inscripción al doctorado cosa de tener eso hecho este mes.

-------------------------------------------------------------------------

20/07/2022

Hoy trabajé el armado de la presentación de la parte de la ecuación transcrítica
con función cuadrática. Preparé las primeras partes de la teoría y presentación
de la ecuación y después revisé los gráficos que podrían ser interesantes de
mostrar, así como los intenté emprolijar un poco para que se vean más fácilmente
interpretables. Tardé bastante más de lo que esperaba, la verdad que para hoy
yo pensaba ya tener terminada la presentación. El viernes no podré trabajar en
esto, así que quedará para la semana que viene. Mañana me quedo en casa para
preparar la documentación de la inscripción al doctorado y revisar el tema
de la RAFA.

La presentación está subida así como todos los archivos, podría probar si
tengo tiempo de ver de avanzar esto.

----------------------------------------------------------------------------------

26/07/2022

Hoy continué con el armado de la presentación. La verdad no avancé mucho, pero
es que me detuve a revisar algunos detalles, definir cuáles datos usar, rearmar
algunos gráficos para que se vea mejor las marcas en los ejes, revisar por qué
en unos gráficos la opinión daba mas grande que en otros, parchar los gráficos
de variación en los que el primer valor tiene ese salto raro y feo.

Cuestión que me enganché con cosas que no debí haber mirado y me consumieron más
de lo necesario. Ya mañana hago la parte de la ecuación transcrítica con función
logarítmica, no debería tardar mucho, sería algo muy similar a lo hecho en el
caso de la función cuadrática. Los gráficos además ya debería tenerlos. Si veo
que la representación logarítmica no va, lo vuelvo a pasar a representación
lineal. Después mañana pensaré si tengo algún lugar para poner los gráficos
de Varprom. O si son necesarios. Después podría poner algunos más en un apéndice.

Terminado esto, lo siguiente ya sí sería empezar a pensar el modelo en términos de
interacción de a pares, plantear una evolución estocástica del modelo. Para eso
tengo que utilizar el código que tenía en su momento sobre esto. Lo tendré que
actualizar. Y al hacer eso, puedo aprovechar y actualizar otros códigos.

-----------------------------------------------------------------------------------

27/07/2022

Estoy armando la parte de la presentación de la bifrucación transcrítica logarítmica.
Ya creo tener hecha la parte teórica, quedaría entonces armar la parte de los gráficos
y resultados. Cuando me puse a ver eso, me dió la sensación de que quedarían mejor
los gráficos en escala lineal y no logarítmica. Eso implica volver a correr
el archivo de Conjunto_pequeño.py. Eso me hizo pensar en que la Varprom está
corrida en el primer valor para no ver ese feo e inexplicable pico inicial. Pero en
el caso de las funciones logarítmicas ese pico no aparece en el primer paso, sino
20 pasos más adelante. Ese número no es casualidad, hay algo claramente mal en el
cálculo de la Varprom en el programa de C, y tengo que corregirlo.

Es así que miré el archivo de C y noté cosas que no me gustan. Primero, los
datos de las opiniones que se guardan en el archivo correspondiente lo hago
a partir de usar fors, siendo que existe y queda más prolija la función
Escribir_d. Lo segundo, está claramente mal la utilización del 
i_IndiceOpiPasado. Se nota que está corrido, creo que el problema surge en
la parte de las iteraciones, que el indice se adelanta un lugar y eso deja
que el punto inicial quede sin compararse. Eso es lo que produce el cambio
brusco en la Varprom en el paso número i_pasosprevios. Cambiaré eso
del main de la transcrítica cuadrática y de la transcrítica logarítmica.

Esto queda como trabajo pendiente para mañana, o en su defecto el viernes.
Continuando con la presentación, agregué unas cosas que me parecen importantes
a la parte teórica de la transcrítica logarítmica. Y también seleccioné y
agregué los gráficos pertinentes para estudiar el comportamiento del sistema.
Ahora sólo queda agregar las conclusiones, poner la ecuación dinámica junto al
resto de los gráficos y creo que eso es todo. (Conclusiones engloba lo que
aprendí de la transcrítica logarítmica así como lo que se me ocurre hacer
a continuación)

-----------------------------------------------------------------------------

28/07/2022

En la mañana terminé la presentación, completé lo que faltaba de las conclusiones
de la transcrítica y la revisé para ver que no faltara nada o que no hubiera errores
en las ecuaciones.

Ahora en la tarde voy a revisar el archivo main de la transcrítica cuadrática y la
logarítmica para corregir las cosas que vi ayer. Hecho eso y comprobado que los gráficos
de Varprom salen bien, voy a ponerme con el tema de plantear el modelo como una evolución
de interacciones de a pares. Luego de eso podría revisar los archivos de Python para
ver si los puedo emprolijar u organizar mejor. Y después me pondré a preparar el
barrido del espacio de parámetros del modelo.

Ya revisé el archivo main.c de la transcrítica cuadrática y lo emprolijé todo, reorganicé
el código para que las cosas referentes a una parte del código estén todas juntas y
no vayan todas desperdigadas. También agregué una función para armar matrices de adyacencia
completamente conectadas. Corregí la parte de las cosas que hago a mano que ya tienen
funciones y organicé todo el tema de la iteración para que primero se hagan las
cuentas y después se guarden los datos.

Queda para mañana probar si con esto se resuelve el tema del pico feo que se
observa en los gráficos de Varprom.

--------------------------------------------------------------------------------

29/07/2022

Calculé nuevos datos para la ecuación transcrítica cuadrática usando el código
de C reordenado y emprolijado. Los resultados dan bien y ya no tiene el problema
de que la Varprom tiene ese pico feo al principio.

Visto que esto funciona bien reorganicé el código de la ecuación transcrítica
logarítmica y me armé nuevos datos. En este caso también funcionó bien y con esto
ya tengo estos dos códigos completos y funcionando.

Lo siguiente sería entonces armar un barrido en el espacio de parámetros para
caracterizar mejor los valores a los cuales tienden los agentes. Para eso 
lo ideal sería pasar los datos a Algarve u Oporto y empezar a correr. Pero
antes de eso estaría bueno hacer una limpieza de esas computadoras, ver lo que
sirve y lo que no sacarlo así no ocupa espacio ni confunde. Pablo también propuso
el normalizar la suma dentro del logaritmo por el número de agentes. Si bien
el número de agentes me parece un poco grande como normalización, me parece una
buena idea poner algo para que los intereses dentro del logaritmo no
crezcan tanto y los valores de interés se encuentren más contenidos.

Dicho esto, pasaré el resto de la tarde preparando el código de la evolución
del sistema con interacción de a pares. Para eso voy a tomar el código que
tenía de antes de la tesis, el de interacción de a pares.

---------------------------------------------------------------------------------

01/08/2022

Hoy estuve con varias pequeñas cosas que me consumieron todo el día, así
que no avance demasiado. Igualemente, resolví el tema del RK4. Lo revisé
y efectivamente estaba mal, estaba básicamente trabajando como un Euler
ineficiente. Así que para solucionar eso, hice un recambio de cosas en
el código. El RK4 particularmente lo dejé tal cual estaba, ese no necesitaba
cambios. Modifiqué el nombre del vector PreOpi a OpiPosterior. Este vector
ahora lo que hace es ir guardando los valores de opiniones que voy calculando
del próximo paso. De esta manera voy usando el RK4 para calcular los nuevos
valores de opiniones de mis agentes, los guardo en OpiPosterior y una vez
terminado de evolucionar a todos los agentes y todos sus tópicos, paso a 
reescribir el vector de Opi con los resultados de OpiPosterior.

Tambien modifiqué los input de Iteracion, porque me parece que si bien
funcionaba el mandarle el puntero del struct directo, creo que resultaba
oscurecedor respecto de cuál es el puntero que se estaba evolucionando.

Lo puse a prueba. Para empezar las 4 pendientes calculadas son distintas.
Si bien la segunda y tercera parecen iguales, se distinguen en los últimos
decimales. Por otro lado lo comparé con esos valores calculados a mano
con la calculadora, la primer pendiente me dió lo mismo, así que parece
que está bien.

Me queda mañana usar esto para corregir la transcrítica logarítmica, luego
corregir la transcrítica cuadrática y ver si los resultados son distintos o
no. También revisaría el RK4 para ver si se me ocurre hacer más sencillos los
comentarios. Terminado eso, ahora sí, laburo el caso de evolución estocástica
del sistema.
-----------------------------------------------------------------------------

02/08/2022

Probé las correcciones hechas sobre el RK4 al detalle con un archivo de
Python aparte. Por lo visto, funciona perfecto. Ahora lo que debería hacer
es correr los datos de la transcrítica logarítmica y ver que los gráficos
no sean muy distintos a lo que yo puse, sino caer en la desesperación.

Hecho esto, podemos finalmente pasar a estudiar la evolución del modelo
en el caso estocástico. Bueno, quizás no pasemos tan diréctamente a esto.
De Secretaría Académica me mandaron un mail diciendo que le hable a un cierto
profesor para que él me tome el final de Física Computacional. Si sumo eso
con el hecho de que voy a empezar a cursar el cuatri que viene, voy a
avanzar lento con esto, por lo menos el primer mes de cursada.

Pero superado eso, le entramos duro al armado del modelo estocástico.
Igual mañana ya podría ponerme con eso y tener algo ya funcional, digo yo.

-------------------------------------------------------------------------------

03/08/2022

Lo primero que hice fue plantear el cómo modificar el modelo para que realice una
evolución estocástica. Esto lo escribí en el cuaderno y ahí lo que menciono es el
tema de que necesito definir cómo es la selección del segundo agente para la
interacción del modelo. Esta selección actualmente se resuelve de manera
equiprobable, tomando al segundo agente del conjunto total de agentes. Esto
puede cambiarse en cambio y para mi hay dos opciones características. La primera
es que el segundo agente se tome del subconjunto de vecinos del primer agente
de forma equiprobable, entendiendo que la red del primer agente se construye
siguiendo el protocolo de construcción de la red de actividad que planteamos
en la tesis. La otra opción es desligarse completamente de la red y tomar
al segundo agente del conjunto total de agentes pero eligiéndolo con una
probabilidad condicionada por la homofilia entre los agentes, usando la
misma probabilidad que se usaba en el modelo.

Teniendo esto planteado, para comenzar a probar el código simplemente realicé
una evolución estocástica en la cual el segundo agente se selecciona equiprobablemente
de conjunto total de agentes de la red. Y el resto de la tarde me la pasé
probando manualmente que las cuentas realizadas sean las correctas. Haciendo
esto por suerte detecté que tenía mal planteada la función Din1 y estaba
aplicando la transcrítica cuadrática, no la logarítmica.

Ya mañana o pasado queda charlar con Pablo al respecto y tomar una decisión
sobre cómo evolucionar este sistema. Para mi lo ideal sería considerar 
un sistema sin una red compleja, directamente mandar el sistema a correr.
Quizás incluso podríamos proponer una red que se forme estocásticamente,
asignando actividad a los agentes y activándolos al azar, luego registrando
enlaces entre aquellos agentes que interactúan. Finalmente, estudiamos las
redes que surgen de estos comportamientos, sus propiedades y conectividad.
Me gusta la idea.

-------------------------------------------------------------------------------

04/08/2022

Hoy decidí primero dedicarme a armar un barrido de los datos del modelo
con la ecuación transcrítica logarítmica. Para eso necesito definir la 
región del espacio de parámetros que voy a recorrer y también necesito
tener acceso a las computadoras de Algarve y Oporto. Hablé con Pablo y
me explicó como entrar. Desde la computadora del DF es tan simple como
apretar ssh faviodic@computadora.df.uba.ar, reemplazando algarve u
oporto por computadora. Con Setubal todavía tengo que revisar el
tema de la contraseña porque no pude entrar.

En Oporto y Algarve borré todo lo viejo e innecesario, así no me estorba
ni me confunde. Lo siguiente será empezar a copiar cosas de la máquina
principal a las pcs para empezar a laburar. Eso lo haré a la tarde.
Espero no haber borrado nada útil. Confío en que no

Terminado eso empecé a preparar el archivo para caracterizar el modelo con
la ecuación logarítmica. Partí del códgio de trans_ln y fui agregando
cosas para el guardado de datos de Modelo_estocástico. Aún así faltan
cosas por hacer en el código. Son:
  .) Agregar una parte en la cual el sistema evolucione hasta que la
  red se vuelva conexa. Una vez que la red es conexa el código avanza
  a la siguiente etapa en la cual corre hasta alcanzar un estado estable.
  .) Modificar la ecuación dinámica para que dentro del log haya una 
  sumatoria y no una productoria. Este es en realidad un cambio doble,
  porque al hacer esto también tengo que cambiar el logaritmo natural
  por un logaritmo en base 10.

Hechas esas cosas, estamos en condiciones de mandar a correr el código
para probar cuánto podría tardar. Teniendo una idea del tiempo que 
tardaría en resolver para 1000 agentes, ahí sí lo mandamos a Algarve,
que creo están libres las máquinas, y corremos sobre los 20 hilos.

La idea es hacer eso mañana y ya la semana que viene arrancamos con la
materia de Dorso a Full.

-------------------------------------------------------------------------------

05/08/2022

Hoy trabajé dese casa, se puede laburar perfecto. Quién diría que cargar
todo a Github sería tan cómodo para laburar.

Cuestión, modifiqué varias cosas del código para la etapa Din_log.
Lo que hice en el main fue agregar una sección donde el sistema evoluciona hasta
generar una red conexa. También me aseguré de guardar las redes de Adyacencia
armadas, cosa de poder ver que efectivamente sean redes conexas.

En avanzar modifiqué el tema del logaritmo para que ahora el programa calcule
un logaritmo en base 10 y que además haga el logaritmo de la sumatoria
y no el de la productoria.

En inicializar aproveché y emprolijé la función de Conectar Adyacencia. Eso
necesitaba un retoque porque era muy molesto cada vez que necesitaba revisar
algo de ahí. Además había partes en las funciones donde buscaba elementos de
la matriz de adyacencia y utilizaba el número de filas en vez del de columnas,
un error feo pero que no se notaba porque el número de filas y de columnas
de la matriz de adyacencia son iguales.
Lo que sí noté es que había una parte del código que era una complicación al
pedo, algo sobre calcular distancias usando la norma, cuando en realidad dado
el espacio no ortogonal el tamaño de un vector se calcula distinto. Encima,
ni siquiera era necesario calcular esa distancia, no comprendo qué pasó.
Seguro el quilombo de cuentas me re confundió.

Ahora que suba esto a Github, ya probé y lo pasé a la pc de Oporto sin problemas.
La semana que viene puedo descargarlo en la facu, pasarlo a Oporto y de ahí
mandarlo a correr. Una vez que tenga eso, ya nos concentramos a full con el final
de Dorso. Esa cosa va a ser un bardo si no me dedico. Así que la semana que viene
haré poco y nada de la tesis. Igual, hace una semana que tengo cosas para mencionarle
a Pablo y charlarlas. Bueno, eso ya se verá. Me alegra haber podido laburar desde
casa, hoy me concentré bastante más en el laburo.

--------------------------------------------------------------------------------

08/08/2022

En la mañana bajé en la facultad el código que hice en casa. Le corregí dos
tonterías que faltaban, declarar unos ints y cositas. Terminado eso, cargué
el código en Oporto y actualicé el Instanciar.sh para que cree los arrays de
parámetros con valores floats y que no se mueva sólamente en valores enteros.
(Eso era un bardo extra). Lo probé y me fijé que todo funque bien.

Después revisé el Metainstanciación, esa cosa no necesitaba mucho laburo.
Corregidos y actualizados los programas de Instanciación, mandé a correr
en la pc de Oporto, ocupando los 20 hilos. Son 80 iteraciones de un barrido
en todo el espacio de parámetros, variando alfa, cdelta y mu en [0,1] de a
0,1. Si mis cuentas están bien, esto va a tomar tres días y medio en terminar
de correr. Así que un día antes del DDF va a estar terminado. Espero que nadie
necesite usar las pcs de Oporto.

Como soy un ansioso volví a mirar la pc de Oporto, ahí vi que el programa se
había trabado. Claro, había mandado a correr casos en los que el Mu es cero.
El sistema no tenía fuerza que lo tire hacia el centro, era puramente repulsor.
Nunca dejaba de crecer. Ahora modifiqué para que lo mínimo que mu pueda valer
sea 0,1 y lo volví a mandar. Esto estará para el 12/08, seguro.

En el entre tiempo, y hoy también lo hice, vamos a estudiar puro y duro la
materia de Dorso. Es un parto leer esos powerpoints. Y eso que estoy con los
primeros. Dios se apiade de nosotros.

--------------------------------------------------------------------------------

14/09/2022

El final de Dorso fue un evento complejo. Sin embargo, valió la pena, o eso
espero. Ahora voy a retomar esto y al mismo tiempo voy a estar trabajando en
pasar a Python el código de Fortran sobre el paper de Pablo. Tanto el de
"Los indecisos tienen la clave", como uno sobre modelo del votante. Así que
no sólo trabajaré en esto.

Acabo de revisar los archivos de datos y me estoy dando cuenta de que había
unos saltos de línea mal en el main, porque debía hacer que el salto de línea
sea en el archivo 2 y los hacía en el archivo 1. Eso llevó a que el archivo 2
que debería tener una línea por cada iteración del sistema tenga una sola línea,
mientras que en el archivo 1 debía tener toda la Varprom en una sola fila y la
tenía en múltiples filas con una sola columna. Un bardo. Esto significa que de nuevo
tengo que mandar a correr todo, otra vez. Lo bueno es que tengo laburo para hacer
mientras esto corre, así que no es taaan malo. Podría ser peor, podría estar necesitando
que esos datos estén perfectamente armados y querer cortarme las bolas.

Mandemos a correr eso de nuevo y mientras ir trabajando el armado de un archivo
de Python que prepare gráficos en base a esos datos, que estarán hechos cuando estén.
Ya hice las correcciones de los archivos main.c tanto en pc de Oporto como en 
mi pc.

------------------------------------------------------------------------------------

26/09/2022

Estos días estuve trabajando en armar el código que me pidió Pablo sobre el modelo
del Votante y el modelo de Deffuant, en preparar el TP de la materia de Mininni
y en la charla que di en la reunión de grupo.

Y además en un principio pensé que para no mezclar cosas lo mejor era no anotar
el trabajo hecho sobre lo de Pablo acá, pero siendo que eso también lo voy a
estar subiendo al Github, me parece que no tiene sentido no ir agregando notas.

Cuestión, arranqué por el modelo del votante, el cual estoy armando sobre una grilla
cuadrada. Este modelo evoluciona tomando un agente al azar y luego a un vecino de
este agente. Definidos los dos agentes, al segundo le pongo el valor de opinión del
primero. Este sistema evoluciona hasta que una sóla opinión domina.

Actualmente el código genera la grilla, la puebla y evoluciona el sistema seleccionando
agentes y copiando las opiniones. Para eso estoy usando el generador de grilla de 
networkx y el generador aleatorio de Numpy. Estas cosas nuevas que estoy aprendiendo
las estoy registrando en el código.

El miércoles seguiré con esto. La idea es armar un gráfico que se actualice a medida
que el sistema evoluciona y que eso me muestre el estado del sistema. Y después meter
todo en funciones que estarán en un archivo separado que importaré usando import.

--------------------------------------------------------------------------------------

28/09/2022

En la mañana trajeron los muebles nuevos, así que estuve trabajando en el cuaderno.
A la tarde me puse a pasar el código hecho a la pc. Ya con eso logré hacer que las
bases para el modelo del votante estén terminadas. Puedo generar la grilla, asignar
las posturas de los agentes, evolucionar sus posturas, calcular la fracción de agentes
activos y graficar el sistema. Lo que sigue es pasar todo el código a funciones
externas que después importare. 

Por otra parte, queda pendiente el desarrollar la parte del código donde se calcula
la fracción de enlaces activos porque esta sección es sin duda la que más tarda del
código. Se me ocurre que para que vaya más rápido podría la primera vez hacer la
cuenta, con todo lo que eso puede tardar, y después a partir de ahí simplemente
voy actualizando sólamente a los sujetos que flippean. De esta manera no necesito
hacer tanto laburo de calcular, porque en cada iteración sólo cambia un sujeto a
lo sumo. Además esa matriz eventualmente la necesito para graficar.
Por el lado de armar un gráfico animado, eso va a ser un tema en sí mismo.

Después quiero remarcar que debería actualizar las documentaciones de las carpetas
que hay en Programas Python y en Imágenes porque hay más carpetas que las que dan 
cuenta los archivos y pronto me voy a empezar a olvidar que etapa del trabajo iba
antes de cuál.

----------------------------------------------------------------------------------

29/09/2022

La mayor parte del día me dediqué a la materia de Mininni. A lo último dediqué
unos minutitos a seguir un poco con el código del modelo del votante.

----------------------------------------------------------------------------------

30/09/2022

En la mañana terminé el código del votante y lo mandé a correr. Al principio
tardó mucho en converger, tenía una gran cantidad de agentes. Por consejo
de Pablo le reduje el número de agentes y lo mandé a correr de nuevo y ahora
sí converge. Cuestión, es un código que no va en una única dirección sino que
oscila entre caer a un estado o al otro.

Sobre el modelo de Deffuant, en el día varias cosas diversas me distrajeron un
poco de eso, pero en definitiva ya tengo el esqueleto del código armado, ahora
sólo tengo que organizarlo para que las funciones estén prolijamente anotadas y
el código funcione correctamente. En este caso sólo armaré un gráfico del sistema,
así que ese gráfico tiene que construirse con el array de opiniones que se va
construyendo. Es decir, el armado del gráfico tiene que ser parte de la iteración
del sistema, no puede estar al final.

El lunes termino esto y ya veo que tal queda.

------------------------------------------------------------------------------------

03/10/2022

Le hice los retoques finales al código del modelo del votante y lo mandé a correr
con 10000 agentes. Estuvo toda la tarde corriendo y no convergía, el sistema osciló
entre la solución de todos +1 a todos -1 y a todos +1 de nuevo, no parecía que fuera
a converger nunca. Hablando con pablo me dijo que pruebe ponerle menos agentes y
también le cambié para que no arme tantos gráficos. Esta vez convirgió rápido
dentro de todo.

Mientras todo esto corría, trabajé en el modelo de Deffuant y lo preparé para que
corra y se vea el gráfico similar a lo que se ve en el paper de Statistical
Physics. Logré que el programa funque, las funciones están dentro de todo prolijas
y arma los gráficos.

Por último actualicé la documentación de la carpeta de Programas Python y la de
Imágenes.

--------------------------------------------------------------------------------------

04/10/2022

Estuve toda la mañana leyendo el paper sobre Social Physics que me pasó Pablo, 
la sección sobre Metapoblaciones. Leí la primera parte, mañana termino lo que
falta. Tomé algunas notas en el archivo de Resumenes que está en la carpeta
de Tesis Favio en el Drive. Mañana terminaré esto.

----------------------------------------------------------------------------------------

05/10/2022

Terminé de leer la sección de metapoblaciones del paper de Social Physics y
tomé unas notas en el archivo de Resumenes.

Habiendo terminado el trabajo que me pidió Pablo y leído lo de Metapoblaciones
ahora tengo los siguientes caminos para tomar en el trabajo:

1) Revisar los archivos de simulaciones que armé y que están en la pc de
Oporto.
2) En sintonía con lo anterior, podría intentar emprolijar el archivo de
Python que uso para armar gráficos y demás cosas. En particular considerando
que no me gusta cómo funca la cosa porque siento que hay muchas indentaciones
innecesarias.
3) Modificar la función del segundo término de Baumann usando una función
logística. Esto está bueno porque es una función en la que podemos modificar
la pendiente más fácilmente.
4) Repensar el modelo desde cero, partiendo en un modelo de un tópico.
Pensar en las reglas que quiero que el modelo cumpla, luego pensaré en
cómo armar una ecuación en torno a eso.
5) Leer la bibliografía que tengo, ya sea el archivo de Social Physics completo,
el de Statistical Physics o el de Detección de Comunidades.
6) Armar algún concepto de un modelo o un estudio en base al Republia.

Tengo las siguientes respuestas para organizar cuál debería ser el próximo trabajo
a realizar.

El 1 y 2 no los haría ahora, me parece que ya establecimos que la idea es volver
a la mesa de trabajo y empezar el modelo de nuevo, arrancar con eso sería muy
cercano a una pérdida de tiempo.
La 3 creo que es un paso avanzado, lo haría si ya repensé el modelo primero.
Ya estuve leyendo bibliografía, me gustaría tener algo sobre el modelo para
conversar la próxima vez, así que quizás hasta la semana que viene dejaría
el punto 5 de lado.
El 6 lo empezaré a pensar en mi tiempo libre la semana que viene. Quizás me
traiga el manual del juego a la oficina.

Por tanto, lo razonable es empezar con el punto 4, y en la medida que tenga algo
coherente pensado, intentar involucrar el punto 3 en eso. Ya la semana que viene
volver a tomarme momentos para ir leyendo papers y traerme el manual para
ir leyendo y pensando algo al respecto.

---------------------------------------------------------------------------------

13/10/2022

El 06/10 fue el jueves antes del feriado largo de 4 días de octubre. Creo
que arranqué con la guía 3 de Temas avanzados de fluídos. O quizás
estuve charlando con Pablo sobre los modelos de votante y de Deffuant.
Hice cosas pero no terminé nada.

A la tarde fui a cursar la materia de Fluidos.

Del 07/10 al 10/10 fue feriado y me dediqué a intentar preparar mi prueba
de oposición para el concurso de ayudante de primera.

El martes 11/10 a la mañana el internet no funcaba y la puerta hacia las oficinas
estaba cerrada. Trabajé lo que pude en la prueba de oposición. Después fui al
club de conversación de inglés en la mañana.
Terminado eso, fui a dar una mano para preparar las cosas para la Giambiaggi.
No fui a cursar la práctica pero sí la teórica de la materia de fluídos de Mininni.

El miércoles 12/10 a la mañana ayudé con las acreditaciones de la Giambiaggi,
estuve en las charlas de la mañana y después al mediodía me volví a casa
para terminar durante el resto del día la prueba de oposición. Terminé
la prueba a la noche.

Hoy vine a la Giambiaggi durante la mañana y a la tarde fui a cursar la materia
de Mininni, tanto la teórica como la práctica.

-------------------------------------------------------------------------------------

14/10/2022

Hoy fue el último día de la Giambiaggi. Asistí a las charlas, ayudé con la
organización y cosas y ya me fui a casa después de el brindis final.
No hay mucho que decir, intenté prestar atención a las charlas y listo,
eso es todo.

-------------------------------------------------------------------------------------

17/10/2022

En la mañana llegué y me puse a leer mails. Después fui al curso de listening
y speaking de inglés de los lunes. Más tarde seguí con lo que tenía para hacer 
de mi trabajo.

En la tarde resolví los temas pendientes del modelo del votante y de Deffuant
que me había pedido Pablo. Al modelo del votante le agregué la posibilidad de
usar diversos tipos de redes iniciales y modifiqué el gráfico para que se
forme usando networkx de forma tal que se grafiquen todas las redes, no sólo
la grilla 2d. A los nodos les agregué un nuevo atributo que es el color y usé
eso para irlos coloreando en la red. Y también agregué una condición que se
encargue de asegurarse que la red de Erdos-renyi sea conexa.

En el modelo de Deffuant agregué el armado de un gráfico de las densidades
de agentes que se suma al gráfico ya existente.

---------------------------------------------------------------------------------------

19/10/2022

Hoy estuve haciendo trabajo en la carpeta, puramente lapiz y papel. Plantee
las bases que me parecen correctas para el modelo en 1D y reduje los parámetros
que me resultaron redundantes. Esto llevó a una simplificación del modelo
con una implementación de la función logística en el segundo término. La forma
de esta función y el hecho de que tenga un valor de "activación" permite agregar
control sobre el desencadenamiento que lleva a que los agentes se sobre interesen
en un tema.

--------------------------------------------------------------------------------------

21/10/2022

Hoy hice muy poco, estuve con cosas de inglés, me reuní con Nahuel y fui a la charla
de Serge Haroche, no estuve trabajando mucho. Anoté algunas cosas más en mi carpeta,
pero poco más.

--------------------------------------------------------------------------------------

24/10/2022

En la mañana fui al taller de inglés para el TOEFL. En la tarde estuve trabajando
en el tema de tesis, le sigo dando vueltas al tema de la ecuación dinámica. No me
cierra del todo el cómo usar la función logística. Estaba viendo que por como plantee
la ecuación, una baja controversialidad no me lleva el interés de los agentes al
0, sino al 0,5. Eso no sería tan malo, pero encierra un segundo problema más preocupante,
los únicos temas en los que es accesible el interés nulo es en aquellos en los que
la controversialidad es alta. Eso es totalmente raro.

Lo que me tiene particularmente en conflicto es que aparece un nuevo parámetro que 
un poco me da la sensación de que puedo absorberlo con el alfa, pero si lo pienso mejor
es más útil al modelo si lo dejo como un parámetro aparte. La pregunta es cómo interpretarlo.
Ese parámetro que multiplica a todo el argumento de mi función logística es un poco
como un término de aprendizaje que define cuánto el agente absorbe de su entorno.
Se parece a la influencia social, pero lo raro de eso es que si la suma de las opiniones
de los vecinos es menor al umbral beta, entonces esa influencia social no potencia
el efecto, sino que lo atenúa. Entonces ya no es tan claro el concepto de su funcionamiento.
Aún así, sin interpretación, puede seguir siendo funcional al modelo y quizás valga la
pena no absorberlo en el alfa.

------------------------------------------------------------------------------------------

28/10/2022

Hagamos un repaso rápido.

El 25/10 estuve trabajando en cosas de la materia de fluidos.

El 26/10 me quedé en casa, organicé las carpetas de bibliografía en el drive.

El 27/10 seguí con cosas de la materia de fluidos.

Hoy a la mañana hice ejercicios de inglés, repasé un poco la ecuación dinámica del modelo,
fui a la charla de Pablo, después al DF abierto y listo, fin del día.

Espero la semana que viene dedicarme más. Pero estos talleres y cursadas están comiendo
bastante del tiempo de trabajo. Tengo que considerar que si estuviera dando clases, la cosa
sería así o peor.

------------------------------------------------------------------------------------------

31/10/2022

En la mañana fui a la clase de inglés del curso de speaking para el TOEFL.

En la tarde estuve planteando diferencias entre las dos ecuaciones dinámicas para
el modelo de dinámica de opiniones. Pablo me dijo que me decante por la ecuación
que tiene la sumatoria afuera de la función logística. Esa se tiene que normalizar
usando el grado de cada agente. Dicho eso, el miércoles entonces puedo ponerme a revisar
la estabilidad de los puntos fijos del sistema, arrancando por mirar el 0,5. (Creo que
es el único punto fijo que fácilmente puedo mirar) Igual, sabiendo la estabilidad de
este punto fijo puedo inferir la estabilidad de los otros dos. Lo importante es que
este sistema tiene tres puntos fijos, uno cerca del cero, uno cerca del 1 y el otro
en 0,5.

--------------------------------------------------------------------------------------------

01/11/2022

Hoy me voy a poner a trabajar en cosas de la materia de Mininni. Tenemos hasta el 15/11
para hacer el TP4. Esta vez voy a arrancar a armar datos. De los cuatro puntos, sólo
el cuarto es con simulación, los otros tres son teóricos. Mi idea hoy es ir probando armar
datos y levantarlos, tomar notas de lo que estoy obteniendo y ya con eso ver que puedo
hacer tres cosas importantes:

1) Simular un sistema como el que pide el enunciado, logrando usar correctamente GHOST.
2) Levantar los datos y analizarlos en Python.
3) Armar gráficos e interpretarlos.

Arranquemos por hacer las simulaciones para el punto 4 de la guía 4.
Me tome un buen tiempo y fui cambiando uno por uno los parámetros al tiempo que iba anotando
esos cambios. Para esta primera vez me parece bien, después no lo haré. Tengo consultas que
revisar al respecto.

Logré hacer que el código corriera. Lo que sí, el output del código no va todo a un único lugar,
una parte se genera en la misma carpeta en la que se encuentra el binario del solver, la otra va
al output designado. Mejor dejarlo todo junto y después moverlo.

Después fui a la materia de Minnini e intenté prestar toda la atención posible.
El jueves me conviene ponerme a trabajar sobre los ejercicios teóricos. Para eso estaría
bueno descargar alguna bibliografía asociada para interpretar mejor esos ejercicios.

--------------------------------------------------------------------------------------------

02/11/2022

Hoy voy a continuar con el trabajo del 31/10. La idea entonces sería ya definir la ecuación
dinámica y empezar a armar un código en C que corra eso.

Me parece que es una buena decisión en este momento aprovechar y cambiar los floats que
tengo a doubles. No hay razón para seguir laburando con floats.

Estuve queriendo armar una función que calcule la exponencial de la logística. Me acabo
de dar cuenta que es al pedo, porque hacer eso como quería implica sobreescribir y borrar
el Din1, lo cual es una mala idea. El viernes cuando arranque con esto, voy a ver si
incorporo esto todo de una en Din2 o si hago una función aparte que sea la exponencial.
Esa exponencial debería usar la sumatoria que aparece en Din2.

---------------------------------------------------------------------------------------------

03/11/2022

Estuve intentando leer el libro de P.A. Davidson sobre turbulencia y fluidos. La verdad,
no es sencillo de leer. Y además, estoy con un sueño terrible hoy.

Increíble pero lo que leí me quedó y logramos con Mauro hacer buena parte del TP 4 en la
clase de la matería de Minnini. Después de eso fui a la teórica y listo.

---------------------------------------------------------------------------------------------

04/11/2022

En la mañana estuve anotando las cosas en la "burocracia infinita" y después me puse a 
resolver tarea de inglés. Aunque como en realidad la profe todavía no subió tarea
al campus, lo que hice fue mirar los videos que me propuso para mejorar mi pronunciación.

Ahora a la tarde voy a intentar ponerme a hacer algunas cosas con el código de python.
Voy a empezar por la parte de cambiar los floats por doubles. Los floats que voy a
cambiar son : f_K -> d_K; f_alfa -> d_alfa; f_dt -> d_dt, f_Cosangulo -> d_Cosangulo.

Bien, ya cambié los floats. Ahora vamos a la parte de ir colocando la ecuación dinámica
en la función Din1. Ya agregué en la función Din1 lo necesario para calcular la función
logística. Ahora queda retocar la función Din2. Y después cambiar el armado de los parámetros
en main. Porque nunca asigno valores a m o umbral.

---------------------------------------------------------------------------------------------

07/11/2022

A la mañana hice una parte de la ejercitación del taller de inglés y después fui
a cursar el taller.

Antes de ponerme a trabajar en algo nuevo, voy a ponerme a anotar las ideas que
tuve sobre el armado de un experimento en base al Republia.

######################################
######################################
EXPERIMENTO EN BASE AL REPUBLIA
######################################
######################################

Concepto base:
--------------

El Republia es un juego en el que cada jugador representa a un parlamentario
asociado a un partido político. Estos parlamentarios deben debatir leyes
y votar respecto a su aprobación. Las leyes que se voten modifican la tendencia
política de la sociedad. Para ganar un jugador debe lograr que la tendencia
política de la sociedad se alinee perfectamente con su partido, puede conseguir
las condiciones necesarias para que su partido se alce al poder o puede
ser el partido con mejor imagen pública llegado un caso de extremo descontento
en el cual la población se vuelca hacia el partido político con el que presentan
la mayor afinidad respecto del resto.

Características:
----------------

El juego presenta varios elementos de juego. Por un lado tiene una punta de 
manejo de recursos, representado por el uso de los puntos de poder político
y las cartas de acción, las cuales indirectamente representan el uso de
puntos de corrupción. El manejo de recursos sube un escalón de complejidad
al habilitarse el canje en una partida.

Luego hay una competencia por elementos comunes, como son la tendencia pública
y la presión social. Estos dos elementos no afectan a un jugador por sí mismo,
pero son algo que definen el estado de la partida y son elementos centrales
en la definición de las condiciones de victoria.

Como tercer elemento, y la característica que más nos interesaría estudiar,
el juego propone interacciones de persuasión/negociación, en las cuales cada
jugador presenta, si quiere, una ley al grupo. Al hacerlo debe dar un discurso
que contenga como mínimo ciertas palabras, pero fuera de esto el o la expositora
tiene total libertad de presentar la ley como desee o hablar tanto como quiera
de ella, dando más o menos información. Luego de una primera exposición, el resto
de jugadores y jugadoras tienen la opción de hacer una pregunta que el o la
expositora original responderá. Luego las leyes se votan y de ser aprobadas, se
agregan a la pila de leyes.

Enfoque del experimento:
------------------------

Lo primero sería recortar los elementos que podrían empantanar el experimento
al volverlo demasiado complejo. Por tanto yo propondría concentrarse en la 
parte discursiva del juego, la parte de persuasión/negociación, y dejar de
lado la parte de manejo de recursos, así como minimizar la parte de competencia
sobre los elementos comunes a los jugadores.

El experimento debería consistir en una interfaz en la cual los participantes
puedan escribir sus discursos de negociación para convencer a los contrincantes.
Cada participante empezaría siendo asignado a un partido de izquierda o derecha
y luego le serían presentadas un conjunto de leyes, las cuales debe decidir
cómo presentar a sus contrincantes. Los contrincantes así como el juego no
serían reales, la idea es justamente quedarnos únicamente con el input de
texto presentado por la persona. Para esto estaría bueno primero un cuestionario
que ayude a definir la orientación política de las personas y luego a partir
de eso que se le presenten un conjunto de leyes de izquierda y de derecha para 
que la persona proponga.

¿Qué nos interesaría medir de estos datos?:
-------------------------------------------

Los datos que obtendríamos entonces serían un custionario respondido por los
participantes, así como una serie de discursos. El cuestionario debería servir
para construir un perfil político del participante, mientras que de los discursos
podríamos medir el tamaño de los discursos (cantidad de palabras), el sentiment,
la percepción de los grupos representados a partir de las palabras utilizadas en
los discursos.

De esta manera podríamos intentar responder las siguientes preguntas:

.) ¿Las personas de algún tipo de perfil son más tolerantes? Esto lo podríamos
medir en términos de qué tan cómodas las personas se sienten al expresar ideas
de facciones con un perfil político distinto al propio. Esa comodidad se puede
intentar medir en términos de la sofisticación de los discursos o el tamaño
de los discursos.

.) ¿La gente debate temas con una violencia que depende del contexto? Podríamos
dar ejemplos de discursos, a algunos grupos darle discursos con un mensaje más
extremo y agresivo y ver si sus discursos responden a eso.

.) ¿Cuál es la opinión que tienen de los grupos de perfil político contrario al
propio? Para esto podemos usar un análisis de sentimiento de cada discurso asociado
a los distintos perfiles políticos. De esta forma podríamos comparar si presentan
un bias respecto de algunos ideales u otros.

.) ¿Podemos entrenar a la gente para que realice mejores discursos, de forma que
los debates que generen sean más interesantes y mejor fundamentados? Esto es algo
más complejo, requeriría poder establecer un feedback respecto a cada discurso
de cada uno de los agentes. Y está quizás la pregunta de si esto es del todo
correcto o no.

---------------------------------------------------------------------------------------------

08/11/2022

En la mañana los trenes no funcionaban, tuve que venir en bondi a la facultad. A la mañana
revisé algunas facturas que tenía que pagar y después fuí al Conversation Club.

En la tarde fui a la materia de Minnini y trabajamos con Mauro. Resolvimos lo que faltaba
del TP4 y después fui a cursar la teórica. Nos contó que la Fanta la hicieron los nazis.

---------------------------------------------------------------------------------------------

09/11/2022

A la mañana fui a comprar el colchón y también llevé la bici a reparar y a ponerle un
portaequipajes. Cuando llegué a la facultad me puse a hacer la burocracia infinita,
terminé de armar el concepto del experimento basado en el Republia y después me puse
a revisar el TP4 de la materia de Minnini.

Ya terminé de revisar el TP4. Subí los archivos al Github. Ahora tengo una hora antes
de que vayamos a Blest a la salida del grupo. Voy a intentar ponerme a revisar el
código del trabajo de la tesis. De mis notas anteriores, dice que ya tengo armado el
Din1, ahora debería retocar la función Din2 y el armado de los parámetros en el main.

La función Din2 está retocada. Hagamos el tema de los parámetros. Ya actualicé el
tema de los parámetros. Y saqué muchas cosas que no estoy usando. Si después
las volviera a necesitar, puedo agregarlas o buscarlo de código viejos.

Lo siguiente que debería hacer es revisar el código una vez más para asegurarme
que todo funcione. Cambiar los path de la escritura de los files, armar las carpetas
correspondientes, actualizar la documentación, ver que los archivos generados sean los
deseados, ver que los inputs estén bien numerados, mandar a correr una vez el programa
antes de usar el instanciar. Esas cosas.

Y por otro lado, debería revisar la estabilidad de los puntos fijos. Ver cómo esto
va a ciertos valores u a otros.

---------------------------------------------------------------------------------------------

10/11/2022

A la mañana llevamos el colchón al departamento. A la tarde fui a la materia de fluidos.
Terminamos de revisar el TP 4 y lo enviamos. El martes que viene arrancamos con la guía 5.

---------------------------------------------------------------------------------------------

11/11/2022

Lo primero que hice a la mañana fue resolver ejercicios de inglés. Hice una parte de
la tarea que había en la solapa Week 3. Todavía falta que la profe suba la tarea de
la solapa Week 4. Ya veremos cómo laburar eso. Antes de ponerme a trabajar en lo
de la tesis, voy a revisar un segundo si hay un segundo tema de Matthew Colville
que me parezca copado tratar, algo más allá de Cargo-Cult Thinking. Al final voy a
hacer una charla sobre Gell-Mann amnesia. Me parece copada la idea.

Para trabajar en el tema de la tesis debería preparar las cosas y mandar a hacer una
corrida, mientras que a la tarde me pondré a hacer el análisis sobre la ecuación
dinámica. 

Estoy notando ahora que las redes que estuve utilizando antes eran redes de actividad.
Y ahora yo quiero trabajar con redes estáticas. Así que vamos a tener que sacar
todo lo asociado a redes de actividad y arrancar desde cero. Tenemos que volver a usar
el leer adyacencia.

Estuve haciendo bastantes cosas, así que intentemos dejar un buen registro. Como dije,
me di cuenta que seguía usando funciones que implicaban redes de actividad, así que
saqué los parámetros asociados a eso y las funciones del main. Pero no borré las funciones
en inicializar, sólo las comenté.
Después me di cuenta que no tenía redes estáticas armadas, así que armé un archivo de
python para fabricarlas. Esto requirió por tanto armar una nueva carpeta en Programas
Python en la cual guardar estos archivos .file. Aunque ahora que lo pienso, podría
guardarme eso en la carpeta de Programas C. En especial porque son simplemente
archivos que después tengo que ir a buscar hasta ahí, y eso no tiene mucho sentido.
####################################################################################
De paso, no actualicé la documentación hablando sobre esa carpeta, tengo que agregar
esa info después.
####################################################################################
Hecho eso, agregué la función para leer archivos y armar la función de adyacencia
a partir de eso. Revisé nuevamente el código y ahora parecer estar bárbaro listo para
funcionar. Habría que hacer una prueba y después mandarlo con el instanciar, aunque
también está bueno tener en cuenta que las redes de Erdös-Renyi que armé son redes 
con 1000 agentes, y mi idea es arrancar con dos agentes. Eso será un problema.

Cuestión, cosas a hacer el lunes:
1) Comentar la lectura de la matriz de Adyacencia, arrancar armando una matriz de
Adyacencia a mano para 2 agentes.
2) Mover la carpeta MARE a la carpeta de Programas C. Me parece que tiene mucho más
sentido eso.
3) Actualizar la Documentación pertinente sobre la naturaleza de esa carpeta.
4) Modificar el Instanciar para que esto se pueda realizar en masa.
5) Hacer un análisis de los puntos fijos del sistema, ver qué pasa con eso y comparar
con los datos.
6) Ahora sí, volver a laburar con los archivos de Python para que quede todo funcionando
finalmente.

---------------------------------------------------------------------------------------------

14/11/2022

A la mañana revisé el segundo video de Laura sobre las palabras más comunes de
inglés, fuimos al curso a estudiar y practicar cosas. Para la próxima clase practicar
lo que mande la profe, hacer algunos de los ejercicios de present y past perfect,
y preparar la charla sobre Gell-mann amnesia.

Continuar con el trabajo anotado en la última entrada sobre el trabajo de tesis.
Ya comenté la función de lectura de la matriz de adyacencia y coloqué la función
de armado de una matriz de adyacencia conectada. También moví la carpeta MARE a
la carpeta de Programas C y cambié el path de s_mady. Actualicé la documentación
de la carpeta de Programas C.

Estoy viendo de modificar el Instanciar, estoy considerando qué valores debería
tomar el alfa o el amplificador. Si tomo alfa cero, el segundo término se vuelve
costante porque ya no depende de las opiniones de los vecinos. Si tomo el amplificador
igual a cero es igual, el segundo término se vuelve constante, pero ahora sé que
suma 1/2. Así que lo razonable es que esos parámetros no sean nunca cero. Yo diría,
para probar, que el amplificador vaya hasta 20 y el alfa vaya de 0.5 a 2.

Hice unas pruebas, el código parece funcionar y generar correctamente los datos.
Los .file generados van bien y parece tener el formato deseado. Lo menciono porque
en un punto al principio parecía haber un pequeño error con un título que estaba
mal posicionado, faltaba un salto de línea.

Ya que voy a mandar a correr esto en el caso de dos agentes, tengo que asegurarme
de ponerle un pause de un segundo en el final del código para que no me corra con
la misma semilla constantemente.

Hice correr el instanciar, tengo 20 simulaciones para un total de 24 combinaciones
de parámetros, es decir, un total de 480 archivos con datos. Antes de ponerme a analizar
los datos, yo primero haría el análisis de la ecuación dinámica, así sé qué espero
ver.

Hice un primer análisis de la ecuación dinámica y puedo observar que el sistema posee
tres puntos fijos, pudiendo saber únicamente que hay un punto fijo en alfa*x = 0.5.
Los otros dos son incalculables. La idea igual es que si conozco la estabilidad de ese
punto fijo, conozco si el sistema tiende a él o si tiende a los otros dos. Ahora que lo
pienso, hice un análisis malo de la estabilidad, revisalo haciendo derivadas sólo en x,
ahí voy a conseguir una respuesta más razonable.

También hice una revisión por arriba de los resultados, me armé una tabla en el cuaderno
de los parámetros y los valores a los cuales tiende el sistema. Lo que puedo ver es que
para bajos alfas, el sistema sólo decae, porque siempre se encuentra por debajo del punto
fijo medio, el cuál, además, para alfa chicos es un repulsor. Y después, a medida que alfa
crece, ocurre que en ciertas combinaciones de parámetros el sistema converge al punto central
como un atractor y para otras combinaciones converge al punto fijo más grande o al más chico
en función de las condiciones iniciales.

---------------------------------------------------------------------------------------------

15/11/2022

En la mañana estuve analizando la ecuación dinámica del modelo y después fui a 
al Conversation Club. Hablé con la chica Carolina, que al parecer está trabajando
en el labo de Goyanes, pero juraría que me dijo con Famá como directora. Justo
leí que recibió un premio de L'oreal Unesco. ¿Me estaré confundiendo los nombres?

En la tarde fui a cursar la materia de Minnini con Mauro, arrancamos el TP5
y después fuimos a la teórica. Esta es la anteúltima teórica.

---------------------------------------------------------------------------------------------

16/11/2022

En la mañana continué con el análisis de la ecuación dinámica del modelo.
Charlé un poco con Pablo, me recomendó deshacerme del parámetro de la amplificación
y quedarme sólo con la controversialidad y el umbral. Estos dos parámetros que
van en el exponente de la función logística. A partir de eso estudié el punto fijo
del sistema y su estabilidad. Con el cambio hecho, ahora la estabilidad del punto
fijo (umbral/alfa) depende únicamente de alfa. Por otro lado, ese punto es fijo
sólo si umbral/alfa = 1/2. En ese caso, el punto es atractor si alfa es menor
a 4, pero es repulsor si alfa es mayor a 4. Me parece interesante estudiar entonces
el comportamiento del sistema en el espacio de parámetros alfa - umbral. El sistema
va a tener tres opciones. Converger a 1/2 si alfa = 2 umbral y alfa es menor a 4,
o converger a uno de los dos puntos fijos extremos si resulta que alfa es mayor a 4.
Para poder diferenciar estos dos casos, creo que lo mejor va a ser estudiar
la dispersión de los datos. Eso va a ser importante. ¿Dispersión o varianza?

Por otro lado, Pablo hablaba de incorporar un mecanismo para que los agentes
pierdan su interés en una noticia o en un perfil, usando el modelo de Fitzhugh-Nagumo.
Después tendré que leer al respecto y ver cómo se puede agregar eso.

La idea del barrido de parámetros ya la tengo. Voy a mover el umbral entre 0,5 y 3
de a 0,5. Mientras que el alfa lo voy a barrer entre 1 y 8, así para cada umbral
paso por el valor en el cual alfa es el doble del umbral. Esto debería dar una buena
primera idea del comportamiento del sistema, porque además alfa se vuelve mayor a
4 y de esa forma el punto fijo de 1/2 pasaría en algún punto de ser estable a inestable.

Mientras el código corre y arma los datos, me queda ponerme a preparar el archivo
de Python. Empecemos armando el gráfico del mapa de color en función de alfa y el umbral.

Me estoy dando cuenta que ya no es conveniente guardar todos los archivos de Python en
una sola carpeta, así que lo mejor será ir armando carpetas con los programas de Python
dentro de las carpetas con datos.

Logré que el archivo de Python levante los datos de las simulaciones y realice unos
gráficos de opinión versus tiempo. Para esto me armé una carpeta en imágenes donde
poner los resultados. Arranqué con esto porque la función estaba básicamente armada
y quería ir probando que el levantar datos estuviera funcionando bien. Todavía estoy
acostumbrándome al uso de Pandas. En los gráficos hay una curva rara e interesante.
Como que el sistema tiene dos regímenes diferentes de crecimiento. Eso me interesa.

Ya que agregué una nueva carpeta en Imagenes, después tengo que actualizar la
documentación. Lo siguiente ahora sería lograr armar un mapa de colores con los datos.
El armado de esta función la dejé a mitad de camino. El viernes continuaré con esto.
Hechos estos gráficos lo que quedará será ir planeando ya una corrida para realizar
un barrido más fino. Y debería borrar los datos de la simulación con el término logarítmico.

---------------------------------------------------------------------------------------------

17/11/2022

En la mañana me puse a preparar las simulaciones para el trabajo final de la materia de
Minnini. Realmente no creo que sea necesario esto, porque Mauro hará las simulaciones
y trabajaremos sobre su pc al respecto. Pero confío en que es una buena práctica
practicar simular los datos.

Logré hacer correr todo en mi pc, aunque tomó bastante tiempo que los datos terminen de
calcularse. Lo que sí vale destacar, me fui y la pc sigue laburando sin problemas
si me voy por menos de tres horas. Creo que no suspende, sólo bloquea la pantalla.

A la tarde laburamos en la práctica y después tuvimos la última clase teórica con Pablo.

---------------------------------------------------------------------------------------------

18/11/2022

En la mañana me quedé en lo de mis viejos a abrirle a Lorena. Al mediodía fue a la facultad
y ya me puse a laburar en el tema de tesis. Lo primero que tengo que hacer es armar
el mapa de colores de la varianza en función de los parámetros alfa y beta.

De paso, ayer hablando con Mauro me dijo sobre el uso de la función Path. Debería revisar
eso a futuro.

Ya tengo la función que arma los gráficos de los mapas de colores. Lo que se observa es que
a partir de alfa 4, y para ciertos valores de umbrales a partir de 2, el sistema empieza a
tener valores de varianza en su distribución mayores a cero. En particular el máximo de 
varianza está en alfa 6 y umbral 3. Pero es importante notar que para ciertos valores
de umbral, si el alfa se hace muy grande, la varianza vuelve a caer a cero.
¿Tiene sentido todo esto? Por un lado, se cumple mi predicción, que a partir de alfa
igual a 4 el punto fijo en el medio deja de ser estable. Por eso, todos los sistemas tienden
al mismo valor y la varianza tiende a cero para los sistemas con alfa menor a 4. Hasta ahí
explico la mitad de abajo del gráfico. ¿Pero por qué para alfas grandes y umbrales chicos
el sistema tiene baja varianza? Bueno, para alfas grandes el punto fijo del medio deja de
ser estable, pero lo que ocurre ahí es que el alfa es demasiado grande para que el umbral
compita, entonces el sistema tiende a manijearse y converge siempre a altos valores de
interés. Es decir, el punto fijo cambió, pero el sistema converge siempre a un mismo lugar
y por eso la varianza es chica. Bien, vamos 3/4 de gráfico. ¿Y qué pasa en la región
de alto alfa y alto umbral? Bueno, para empezar destacar que el sistema tiene su máximo
de varianza en el caso de 2*alfa = umbral. Eso es porque en ese caso el punto fijo del
medio es 0,5, entonces eso significa que el sistema tiene igual probabilidad de converger
a cualquiera de los dos extremos, ya que esto depende únicamente de sus condiciones iniciales,
las cuales son totalmente aleatorias. Por tanto lo que ocurre es que en función de la relación
de alfa y beta, el punto fijo medio separa la región [0,1] en dos regiones de tamaños diferentes.
Esto condiciona la relación de convergencia a cada uno de los puntos fijos.

Espero que esto tenga sentido a futuro. Estaba pensando igual que quizás mejor que mirar la
varianza es mirar la entropía. Para eso tendría que distribuir las opiniones en la región [0,1]
y calcular la probabilidad de caer en cada región.

Lo otro que estoy pensando que debería hacer es mandar a correr el programa y hacer un barrido interesante
en alfa y umbral. Tipo barrer el umbral entre 0.1 y 10, barriendo de a 0.1 y alfa entre 1 y 30 de a 0.1.
¿Aunque debería hacerlo de una o debería hacerlo ya con 1000 agentes?

Antes de mandar a correr esto, pensémoslo un poco. Me parece que recorrer la región propuesta es mucho.
Primero que nada, pensá que tenés 300 simulaciones por el alfa, eso multiplicado por 100 por el umbral,
luego eso multiplicado por 5 por las iteraciones en cada hilo y eso multiplicado por tres por el tiempo
que tarda cada simulación. Eso da un total de 450000 segundos. Dividido por 3600 da exactamente 125 horas,
es decir 5 días aprox. Por no hablar de una cantidad absurda de archivos a revisar. Me parece importante
que como esto va a ser un barrido más fino, pero tampoco la locura, debería no ser tan gede. No debería
tomar más de dos días de corrida, y eso es mucho. Pensémoslo mejor y el lunes o martes venimos ya con una
idea más clara de cuál es la región que queremos recorrer y de qué forma. Fuera de eso, está todo cargado
en la pc de Oporto. Está el código en c, están las matrices de Adyacencia, está la nueva carpeta Logistica_1D
en la cual van a ir los datos, está actualizado el archivo de Instanciar.sh y el de Metainstanciacion.sh
tiene actualizadas las salidas. Básicamente si le mando un ./Metainstanciacion.sh 19 habré mandado a correr
20 programas, que ocuparán los 20 hilos, y en total serán 100 iteraciones para cada combinación de alfa y
umbral.

En el main que subí a Oporto, y en el que tengo la pc de la facultad, ahora cargo la matriz de adyacencia
de los archivos en la carpeta MARE, ajusté para que se guarden datos de seis testigos y creo que nada
más.

¿Que debería hacer el próximo miércoles? Podría por un lado ponerme a trabajar en la función de Python para
que sea un poco más cómodo el armado del mapa de colores. Básicamente armar correctamente las tuplas para que
no tenga tantos for molestos que dificultan la lectura del código. Lo otro que pensaba hacer es modificar
el armado del mapa de colores para que en vez de graficar la varianza grafiquen la entropía de la distribución
de opiniones. Resuelto eso, quedaría estudiar los resultados de mis simulaciones y ver el comportamiento del
sistema en la región de estudio. A partir de eso, el siguiente paso sería intentar incorporar un mecanismo
que permita que un agente pierda su interés en un tema luego de cierto tiempo.

Y si aún así tuviera tiempo de sobra, podría ponerme a leer algunos papers como para no dejar que se me
acumulen.

---------------------------------------------------------------------------------------------

22/11/2022

La mañana arranca con la derrota de Argentina contra Arabia Saudita 2-1. Si no entramos por la ventana,
no sería Argentina. Bueno, volviendo al trabajo, antes de ponerme a trabajar en la guía de fluidos,
corresponde que primero me ponga a revisar el tema de mandar a correr los datos en la pc de Oporto.
Fede mandó a correr algo y está ocupando 3 hilos. Debería no ocupar todo entonces, sólo 15 hilos.

Pensándolo de nuevo, no necesito barrer un espacio taan grande. Podemos reducir el umbral para que
barra entre 2 y 6. Cualquier cosa después le agrego datos. Por el lado del alfa, lo puedo hacer
correr entre 3 y 14, y en vez de moverme de a 0.1, moverme de a 0.2. Eso significa que tengo
41 valores de umbral y eso lo multiplico por 56 valores de alfa. Luego eso lo multiplico
por 90 iteraciones y lo multiplico por 10 segundos cada una. Aproximadamente eso me da
38 horas. Razonable.

Mandé a correr esto, ahora a esperar los resultados. Me hago un café y arranco con el trabajo del
punto 1 de la guía 5. Después fui al curso con Sofi, comimos con Lupi y me fui a trabajar con Mauro.
Me re dormí durante la primer hora, no puedo creer que haya sido tan fuerte. Una re vergüenza.

---------------------------------------------------------------------------------------------

23/11/2022

Ya revisé y los datos que mandé a correr ayer siguen calculándose. Parece que ya se completaron la
mitad de los datos, así que para mañana deberían estar. Lo que puedo hacer hoy es dejar preparado
el archivo que arma los gráficos cosa de que ya mañana simplemente me paro en la pc de oporto y lo
mando a correr. Me dejo anotado el instructivo de cómo mandarlo a correr de paso.

Ahí estoy viendo el tema del pahtlib, voy a ver de implementarlo al código de funciones. Ese es un
primer objetivo de hoy. Lo segundo es armar una función que grafique usando la entropía de las
distribuciones. Lo tercero sería que el gráfico de las cosas no tenga tantos for, sino que itere
sobre una tupla que ya organice los parámetros. El objetivo de eso es que el código sea más legible.
Lo cuarto sería pensar un poco el tema de cómo visualizar a donde converge el sistema en general. Se
me ocurre armar histogramas para eso, suena lo más razonable. ¿Pero pretendo tener taaantos histogramas?
Lo quinto sería estudiar la dinámica de la ecuación, pero para muchos agentes. Mi intuición es que
no va a cambiar la dinámica del sistema para nada, porque como normalizo por el número de vecinos de
cada agente, siempre la opinión de un agente se ve afectada por un mismo factor. Quizás, como sexto,
podemos considerar la idea de usar la opinión varios pasos en el pasado como mecanismo para que 
el interés decaiga luego de cierto tiempo.
  También hay que actualizar la documentación en la carpeta de imágenes.
  
Punto 1 hecho, incorporé Path al código, todo parece funcionar igual. Lo siguiente es una función que
grafique el mapa de colores usando la entropía. Para hacer los gráficos de entropía, decidí armar una
función que calcule la entropía dado un array. Esa función la acabo de probar y funca perfecto. Podría
ver de cambiar el tamaño del bineado a gusto. Eso cambiaría el valor de la entropía, ¿Pero lo hace mejor?
No estoy seguro de la respuesta, no creo que valga la pena preocuparse por eso ahora. Ya logré que
se grafique la entropía. 

Eso es bueno, y lo interesante es que la entropía muestra otras cosas que la varianza no. 
Por ejemplo, el punto con alfa = 4 y umbral = 2 es el de máxima entropía, mientras
que el punto de máxima varianza es el de alfa = 6 y umbral = 3. Eso tiene sentido, en alfa = 4 está
la transición entre un punto fijo estable y un punto fijo inestable. Por eso la entropía es máxima
ahí, porque en ese valor el sistema tiende al punto fijo central y a los extremos en iguales cantidades,
en cuanto alfa sale de ese punto, tiende a un conjunto de los puntos fijos por encima de los otros.
En cambio, el punto de alfa = 6  y umbral = 3 es el de máxima varianza porque como los parámetros son
más grandes en ese caso, los puntos fijos extremos están más separados.

Pensando el tema de los histogramas, se me ocurren tres ideas al respecto. La primera, y que la voy a implementar,
es armar gráficos de histograma para un dado alfa y para todos los umbrales. Esto sería un problema
si tengo muchos umbrales, porque entonces al poner el plt.legend, se me llena el gráfico. Si no lo pongo, 
no sé qué son esas curvas. Aunque quizás la idea sea simplemente observar este gráfico como complemento al
gráfico de entropía. La segunda idea es que esta función arme los gráficos de histogramas para un dado valor
de alfa y umbral determinado previamente. Eso lo agregaría después. La tercera es que los gráficos no sean
gráficos de barras, sino curvas. Eso hará que sea más fácil de seguir. Hagamos eso.

Me quedé trabado en el tema de los histogramas, porque quiero que se armen los histogramas para un dado valor
de alfa, pero para todos los umbrales. Eso me significa que necesito de alguna manera que se armen y guarden
gráficos cada vez que se cambia el valor de alfa. Esto no es tan natural en el caso de tener un for que
integra en un único elemento los alfa, n y los umbrales. Se me ocurre formas de resolver eso usando ifs,
pero siento que si lo hago medio que es un bardo y para hacer eso puedo simplemente meter fors separados
y listo. Después lo pensaré mejor. 

---------------------------------------------------------------------------------------------

24/11/2022

Primero armé el speech sobre amnesia de Gell-mann, así ya lo tengo listo para charlar mañana con Sofi.
Después me iba a poner a leer el libro de turbulencias de Davidson, pero Pablo y Seba me llevaron a
la celebración de recibida de Ariel. Agradable sujeto. Morfé y me dejé mi comida en la heladera de la
facultad para mañana. Después de comer fui al coloquio de Machine Learning. Que nadie se entere que
no presté tanta atención como me hubiera gustado.

A la tarde fui a cursar la práctica y charlamos con Mauro sobre el TP. Yo estuve leyendo el libro de
Davidson para sacar ideas. A la vuelta estaban reunidos en la oficina Pablo, Sebas y Fede, así que 
me fui a dar una vuelta y tomar un café.

---------------------------------------------------------------------------------------------

25/11/2022

Hice un poco de Burocracia infinita a la mañana y después me puse con el código de Python para
estudiar los datos del sistema para la fase de la función logística_1D. Es importante mencionar
que ayer hubo un problema con el aire acondicionado del cluster de pcs, así que ahora están inaccesibles.
Los datos se calcularon bien, pero igual no puedo utilizarlos. Habrá que ponerse con otra cosa.
Más tarde vendrá Sofi e iremos arreglando para charlar.

Ahora voy a continuar con las cosas que quedaron pendientes del 23/11. Para arrancar, el tema de los
histogramas. Después vemos el resto de los puntos anotados, que están en el segundo párrafo.

Nos juntamos a charlar con Sofi sobre lo de inglés. Mientras tanto estuve trabajando sobre la función
de histogramas. Ya grafica correctamente, junta varias curvas y guarda bien. Lo que sí,
tuve unos problemas en la visualización al principio porque se superponían varias curvas y entonces
no se entendía que pasaba con ciertas simulaciones. Aumenté el tamaño del binneado y la cosa dió
mucho mejor. Claramente lo que se ve es que los picos de convergencia se acercan al cero a medida
que el umbral crece, mientras que los picos se corren a la derecha si crece alfa.

Lo siguiente es estudiar la dinámica del modelo para muchos agentes. Arranquemos con eso.
Anoté la ecuación y revisé lo que me queda luego de tomar la aproximación de que los agentes tienden
a los mismos valores. Al hacer eso, la sumatoria en j se anula con el grado del agente i. De esta
manera no necesito tomar la aproximación que hacía en campo medio de que la suma en j era el 
grado medio de la red. Esto lleva a que la ecuación dinámica del sistema es exactamente la misma
que la que tenía en el caso de dos agentes. Lo cual es bueno, porque entonces los valores a los
cuales tienden los agentes ya no dependen de la cantidad de agentes, ahora sólo dependen de los
parámetros, y por tanto el sistema se mantiene en mi región de [0,1].

Voy a dejar el tema de pensar en utilizar la opinión de pasos previos para generar caídas en la opinión
como algo a tratar la semana que viene. Mejor charlarlo con Pablo, para tener una primer idea al
respecto. Así no lo laburo al pedo.

Hagamos lo de actualizar la documentación en la carpeta de imágenes. Hecho, podemos cortar por hoy.

-------------------------------------------------------------------------------------------------

28/11/2022

A la mañana repasé la charla de Gell-Mann amnesia y después me fui a la clase del taller de inglés.
Estuvo linda, dimos las charlas, después hicimos unos ejercicios del TOEFL y revisamos los audios
que armamos de respuesta.

En la tarde me puse con el tema de tesis. La computadora de Oporto parece estar encendida, me parece
una buena idea intentar entrarle y ver qué pasa. Revisando, el archivo de Python parece funcar
bárbaro, así que simplemente tengo que moverlo a la pc de Logistica_1D en la pc de Oporto, armar
una nueva carpeta en imágenes para recibir los gráficos, y listo.

Estuve un buen rato intentando no dormirme, pero al final pude pasar el archivo a la pc de Oporto
y ponerlo a correr. Cuando se terminen de armar los gráficos, me los descargo. Lo siguiente es el
tema de cómo incorporar un mecanismo que haga que las opiniones de los agentes luego de saturar
vuelvan a decaer. Mi primer idea es usar la opinión del agente en un tiempo previo, de manera que
si el agente tenía un interés bajo y saturo, por un período delta ese término no aporta. Luego
del tiempo delta, el interés retardado sube y el interés actual baja. Esto hace que ahora el interés
se mantenga abajo por un rato. La única pregunta al respecto es cómo evitar que esto tire las opiniones
de los agentes a valores negativos. ¿Dependerá de delta?

Hablando con Seba me recomendó un paper llamado "Accelerating dynamics of collective attention". Ahí
utilizan un término no lineal para modelar la saturación del interés en un tópico. Eso suena más
copado y mejor que la idea de usar una opinión retardada. Así que lo próximo que voy a hacer es leer
cómo funca. Después veré de implementarlo en C y ver qué resultados da para el caso de 2 agentes.
Recordá que el caso de muchos agentes con un sólo tópico ya lo tenés visto, los datos están en la
pc de Oporto. Cuando estén listos los gráficos estudiamos ese caso.

También un detalle importante, armaste un environment en la carpeta Python dentro de Logistica_1D.
Fijate que los environment sólo se pueden abrir desde la carpeta indicada, no desde cualquier carpeta.
Para deshacer un environment, simplemente mandá deactivate.

Intenté leer el paper, pero hoy estoy muerto, mañana intentaré prestar más atención. Aunque mañana
voy a estar con el TP de la materia de fluídos. Ojalá terminemos eso temprano.

-------------------------------------------------------------------------------------------------

29/11/2022

Me levanté sintiéndome muy mal, me quedé en casa descansando. En la mañana dormí, en la tarde
me puse a leer un poco del paper que Seba me pasó. Tengo una idea de cómo hacer funcionar este
término de saturación, parece muy simple de implementar. Requeriría tener una segunda ecuación
dinámica que evolucionar con un proceso de un Runge-Kutta. Entonces tendría que primero
evolucionar mi término de saturación y LUEGO evolucionar la dinámica del sistema. La idea es
bastante interesante. Presentárselo a Pablo en orden, contándole primero la idea de la opinión
con retardo y después esta otra opción que me mostró Seba.

-------------------------------------------------------------------------------------------------

30/11/2022

En la mañana me puse a mirar el TP de Fluídos, Mauro había subido al Github todo hecho, dijo que había
que revisar unas cosas del punto 2 pero que los gráficos estaban hechos. Queda justificarlos. Estuve
leyendo el Davidson para hacerme unas ideas de cómo justificar esos puntos. A la tarde posiblemente
me junte con Mauro para resolver esto antes del partido de Argentina.

Logré hacer los gráficos con los datos que simulé en la pc de Oporto y ya me bajé los gráficos a la pc.
Con lo que tengo se observa que el sistema efectivamente tiene una curva que separa dos estados en
el espacio de fases alfa-chi. Esa curva es la de alfa = 2*chi. En esa curva, el punto fijo central
coincide con ser 1/2. Fuera de esa curva, el punto fijo central sabemos que va a depender con alfa y
chi, pero no exactamente cómo. Sé que mientras más grande sea chi, más se corre a la derecha eso.
Y mientras más grande sea alfa, más se corre a la izquierda. Un alfa grande aumenta la pendiente
de la logística, y por tanto una logística corrida a la derecha interseca con la recta antes de lo
que su corrimiento indica.
  Creo que esto explica el comportamiento del sistema. Si alfa es menor a 4, el sistema tiende al punto
fijo central. Si alfa es mayor a 4, el sistema tiende a los puntos fijos extremos. Este segundo caso
se abre en dos opciones más. Si alfa es menor al doble de chi, entonces la tendencia del sistema es
ir al punto fijo menor, ya que el punto fijo del medio está más cerca de 1 que del medio, y esto
establece un bias en las condiciones iniciales de manera que la mayoría de las condiciones iniciales
van a ese valor. Si por el contrario, alfa es mayor al doble de chi, entonces el sistema tenderá en
general al punto fijo mayor porque el punto fijo central se corrió a la izquierda y eso establece un
bias en la dirección contraria de las condiciones iniciales.

Tendría que rearmar esos gráficos de histograma, honestamente se ven re chotos. Podría sacar
el cuadro con la leyenda de los valores de chi. Lo malo ahí es que no tenés idea de qué son
todas las curvas, pero podés mirar todos los gráficos juntos y ver que a medida que aumentás
alfa las curvas se van corriendo a la derecha.

Ya me junté con mauro y repasamos el punto 2. Creo que está bastante bien. Mañana a la mañana le
hago una última repasada antes de mandarlo.

Mandé a rehacer los gráficos, cosa de que no tenga el cuadro de leyendas en el medio. Voy a ver
parte del primer tiempo, después meriendo y después decido con qué continuo. Por ahora el partido
va 0-0, pero con la ofensiva de Argentina todo parece mostrar que el gol argentino es inminente.

Volviendo a lo nuestro, por lo que queda de hoy, me voy a poner a leer un paper o algo. Ya mañana
me pondré con implementar el nuevo término. Aunque para eso, prefiero primero hablar con Pablo.
Veré si viene mañana, si tiene un ratito para charlar al respecto.

-------------------------------------------------------------------------------------------------

01/12/2022

Lo primero que hice a la mañana fue revisar el tp de la materia de Fluidos. No será perfecto, pero
creo que está para entregarse. La verdad, tengo que hacerle un monumento a Mauro, un capo.

Después hablé con Pablo sobre el tema de implementar el mecanismo de saturación a partir del paper
que me mostró Seba y la idea le copa, así que podría ponerme a trabajar en eso ya. Pero antes de
eso Pablo me dijo que haga un mapa de colores del valor promedio al cuál tiende el sistema.

Antes de trabajar en nada de la tesis, me voy a poner a hacer la burocracia infinita, y después a 
laburar.

Ya mandé a armar los mapas de colores en función del promedio de las opiniones de los agentes.
La idea sería poder distinguir las regiones en las que el sistema cae a cero, de las que tiende
a valores altos y de las que tiende al punto fijo central. Veremos qué tal da.

-------------------------------------------------------------------------------------------------

02/12/2022

Empecemos descargando el gráfico del promedio de opiniones en el espacio de parámetros.
Bien, se ve como una bandera. No capta la línea horizontal del alfa = 4. Eso es algo a
preguntarse, cómo lo identifico.

Pero bueno, eso queda para otro momento, pongámonos a trabajar en implementar el término
de saturación. Lo primero que voy a hacer es cambiar el nombre de las funciones dinámicas que
tengo, porque ahora no hay una única función dinámica. Voy a cambiar los nombres de Din2 y
Din1 a Din_interes y Din_sumatoria. Y la nueva función será Din_saturacion.

Ya rearmé la función de RK4 para que evolucione todo el array pasado como sistema y que no
devuelva nada. Tengo que probar si no hay un error de que falte una variable o algo así,
pero para eso tengo que revisar de nuevo el header. El cambio de la función lo hice básicamente
cambiando el puntero double de pendientes y transformándolo en un array de punteros double.
Después me encargué de que en el RK4 se realizaran los for de agentes y tópicos, en vez de
hacerse por fuera. Ahora que volví a metir estos for dentro del RK4 estoy empezando a recordar
por qué la otra versión me parecía más modular. Siempre y cuando no tenga que evolucionar vectores
de tamaños distintos, esto viene bien. En caso de tener que hacer eso, tendré que repensar esto.
Por suerte no borré la función anterior. Con el compilador encontré algunos errores y los corregí.
Parece que funca bárbaro. A lo que sigue.

Cambié la función RK4 para que evolucione todo el vector en una sola vez. También borré la función
Iteración y reemplacé eso por la función RK4 en el main. Lo siguiente ahora es eliminar el array
de Opiposterior e introducir el array de saturación. Hagamos eso.

Con el array de Saturación creado, ahora puedo ponerme a armar la función dinámica de saturación.

También tuvimos una charla con Perotti sobre la idea de poder armar un trabajo en colaboración
tomando en cuenta lo que contó sobre los complejos simpliciales. Esto al parecer mezcla
ideas de geomtería diferencial con redes para poder acoplar las interacciones entre los
agentes de la red utilizando estructura de derivadas. O algo así entendí.

-------------------------------------------------------------------------------------------------

05/12/2022

Lo primero que hice a la mañana fue leer un poco del paper de "Discrete Hodge Theory on Graphs",
después me puse a hacer burocracia infinita y lo siguiente fue ponerme con la implementación
del término de saturación en la ecuación dinámica.

Pongamos en orden lo que tengo:
.) La función RK4 recibe los punteros a structs, un puntero a un array y un puntero a una función.
La idea es que el puntero a array es el que evoluciona y el puntero a función es la función
dinámica que usa para evolucionar el array.
.) Tengo armado el array de Saturación, inicializado y todo. Si no me equivoco, lo razonable es
inicializarlo en 0, y de ahí dejar que la dinámica lo haga crecer. Pero en el paper mencionan
que memoria 0 es hacer que la condición inicial de Saturación sea igual al interés inicial.
Otra cosa que hacen igual es darle un tiempo de evolución al sistema para que termalice.
Así que supongo que eso tengo que hacer ahora, inicializar los valores de Saturación copiando
los valores de interés y después termalizar el sistema. Me parece razonable aprovechar los
pasos previo que ya uso del sistema, así que mezclemos eso.
Pensándolo, lo mejor es no mezclarlo, porque justamente no quiero tomar info de la termalización
hacia el sistema.
.) Cambie los nombres a las funciones dinámicas para diferenciar las que se encargan de la evolución
del interés y las que se encargan de la evolución de la saturación. Lo que no hice todavía es actualizar
las ecuaciones dinámicas para que incorporen el término de saturación.

Bien, la ecuación dinámica ya incorpora el término correctamente. Anotemos lo que falta hacer, porque me
estoy perdiendo mucho en lo que tengo y lo que falta. Agregué el parámetro gamma que mide la memoria
de los agentes respecto de su interés previo. Después inicializo el puntero al vector colocando
ceros en todas las posiciones. Esto después se cambia. Hecho eso, defino los punteros a mis dos
ecuaciones dinámicas, la de saturación y la de interés. Incializo el vector de saturación
copiando los valores de interés en el vector de saturación. Esto es inicializar el vector en
el valor de memoria nula según el paper. Luego itero primero la saturación y luego el interés. Es
importante evolucionar los vectores en ese orden para que la saturación se actualice al valor
correspondiente al paso temporal t, que es lo que necesito para calcular la opinión a tiempo t.

Parece estar todo bien, así que ahora me voy a encargar de hacer unos datos de prueba en la
pc. Primero una sóla simulación, para ver que esto finaliza en algún momento. Luego una tanda
de datos para ver el comportamiento general del sistema. Por último, un barrido más grande
en la pc de Oporto si todo va bien.

Bueno, no estaba todo tan bien como parecía. Las opiniones de los agentes no están evolucionando
como deberían. Si armo un sistema de dos agentes, me evoluciona sólo el primero. Y de tres agentes
el sistema no corre.

Ahí lo corregí, mi problema fue que al recorrer los elementos del puntero pasaba a las distintas
filas haciendo saltos de tamaño "cantidad de filas" en vez de "cantidad de columnas". Eso hacía
que intentar acceder a espacios de memoria que quedaban por fuera del espacio malloqueado del
puntero.

Mañana puedo a primera hora mandar a correr esto con un Instanciar, así ya puedo examinar un poco
el comportamiento del sistema. Después me queda el tema de estudiar el comportamiento del sistema
de forma analítica. Tener una idea de a dónde tiende el sistema con este nuevo término.

Armado con esta información, recién entonces haré un barrido más fino en las pc's de Oporto.
De paso, mañana debería actualizar la documentación en la carpeta de Programas Python. Y además,
debería copiar la carpeta Python al interior de Saturacion_1D, así tengo mis programas de Python
ahí dentro. Y algo que valdría la pena hacer es armar un environment de Python parado en la carpeta
de Programas Python dentro de Oporto. Eso serviría para no tener que armar un nuevo environment
para cada nueva fase. Eso significaría cargar las librerías de Numpy, Pandas, Matplotlib, Networkx
y Pathlib. Ya veré si me acuerdo de alguna otra que necesite.

-------------------------------------------------------------------------------------------------

06/12/2022

Ya mandé a correr el Instanciar, estoy recorriendo el mismo espacio que antes, con alfa entre 1 y 8
y chi entre 0.5 y 3. Lo que queda hacer ahora entonces es realizar un análisis de las ecuaciones
dinámicas y ver a dónde tiende el sistema. Esto es importante para entender los puntos fijos del
sistema. Me alegra que por lo menos las simulaciones terminan y no evolucionan eternamente.

Antes que nada, hagamos la parte de la documentación, así resuelvo esto.
Listo, ya modifiqué todas las documentaciones. Ahora analicemos los datos que tenemos. 
Ocurren varias cosas raras. 

.) Primero, por alguna razón la entropía sólo despega en el valor de
chi = 1.5. No comprendo por qué, pero habrá que ver qué pasa ahí. 

.) Segundo, si bien todos los sistemas
caen a cero, según los valores promedios pareciera ser que el valor promedio de interés al que tiende
el sistema depende únicamente de chi, no depende para nada de alfa.

.) Tercero, la varianza parece ser máxima en el mayor valor de chi. Igual, son todos valores muy
chicos, no sé si es significativo eso.

.) Los histogramas todos van a cero, eso es genial. Y los gráficos de opinión vs tiempo decaen siguiendo
casi el comportamiento de exponenciales. Digo casi porque no me animo a decir otra cosa.

Me parece que el tiempo de termalización es mucho. Probemos a ver qué pasa sin termalización.

Intenté estudiar la dinámica del sistema, pero la verdad no saqué mucho. Lo que pude ver es que
tengo dos estados, puedo tener un punto fijo o tres, eso depende de la relación entre alfa y lambda.
Debería llamar gamma al lambda. O cambiarle el nombre en el programa. Cuestión que si tengo un alfa
grande, la función logística tiene una pendiente más empinada, mientras que si lambda es más grande,
la función cuadrática es más ancha. Entonces, hay un punto en el cual alfa y lambda son tales que
el lo logística corta a la cuadrática abajo, luego la corta al subir y después la vuelve a cortar
cuando va para la derecha. En cambio, si la pendiente de la logística no es muy empinada, puede
pasar que la logística sólo corta a la cuadrática al principio abajo y listo.

Ahora, no tengo idea de la estabilidad de esos puntos fijos. Por lo que se ve en los datos que tengo,
el sistema pareciera simplemente caer a cero, independientemente de todo. Y hablando de eso.

Probé mandar el sistema a correr sin termalización. Creo que lo mejor va a ser tener el programa corriendo
sin termalizar. O podría probar inicializar la memoria en cero. La otra es ir moviendo el parámetro de
lambda y ver qué pasa en esos casos. Me gustaría ver qué pasa si tomo un valor más alto de memoria, tipo
0.1. Ahí quizás la cosa caiga más lento. Si voy a trabajar todas estas ideas. Ahora, necesito organizar
esto para ver qué pasa y poder comparar. Ahora mismo los datos que tengo son sin termalizar.
Necesito dos carpetas más:
.) Datos con memoria en cero
.) Datos con memoria en 0.1

La paja de esto es que tengo que rearmar el código para que levante datos de tres carpetas distintas.
Pero quizás sea para lo mejor, a partir de ahora puedo ir separando los datos en carpertas y eso haría
que esto funque mejor, no tendría que pelearme con los cinco minutos de entrar a las carpetas cuando
están llenas de archivos. Ok, hagamos esto.

Separé los datos en tres carpetas. Recién hice la simulación para el caso de Mem_cero y lo sobreescribí en
los datos de Sin_terma, así que estoy re armando los datos de Sin_terma, después haré Mem_cero y por último
los de Lambda_01. Resuelto eso, compararé lo que tengo y de ahí veré qué barrido particular hago en las pc's
de Oporto.

Dicho esto, ahora mismo no tengo mucho más que hacer. Ya hice la documentación asociada y ya intenté analizar
la ecuación dinámica. Ahora puedo ponerme a leer unos papers mientras se arman los datos faltantes.

Ya armé todos los datos, mañana a la mañana mando a armar los gráficos y ya los empiezo a analizar.
Teniendo eso, ya podré decidir cómo hacer el barrido fino y para dónde ir con esto.

-------------------------------------------------------------------------------------------------

07/12/2022

A la mañana los trenes no andaban por un siniestro. Así que me quedé en casa leyendo el paper de Social
Physics. Leí la sección sobre diseño urbano. Así que eso son 10 páginas adentro. La verdad se lee rápido
eso, no tiene cuentas muy difíciles o complejas, es más que nada una idea bien superficial de muchos temas.
Para saber más hay que leer los papers citados.

Ya tengo todos los datos de ayer del sistema con saturación_1D y las distintas modificaciones, así que
analicemos eso y veamos qué da. Esta es la parte difícil.

Sin_terma:
----------
Por un lado, el no usar termalización no parece tener un impacto negativo de ninguna forma en los datos.
Incluso, es mejor, porque justamente puedo ver todo el comportamiento transitorio del sistema. Antes,
con la termalización, arrancaba en el final del sistema, cuando ya estaba decayendo a cero hace bastante.
Al igual que antes, se observa una disparada de la entropía en el valor de chi = 1,5. Esto indica que el
sistema tiene picos más anchos y petisos. No logro comprender por qué el valor de chi=1.5 es tan especial.
Pero por alguna razón los máximos de entropía se fijan ahí. Igual me parece que no resulta significativo
de los gráficos.

De la misma forma, los gráficos de varianza toman muy valores chicos, lo cuál simplemente habla de que
los picos son angostos en general. El promedio de opiniones muestra que el valor al cual tiende el sistema
no depende de alfa, sino puramente de chi. Esto es cierto, pero al mismo tiempo vale porque las opiniones
promedio del sistema son muy bajas para todo alfa, entonces cambiar el umbral hace que todo el sistema tienda
a valores bajos. Eso es lo que me cambia el valor promedio.

En los gráficos de opiniones versus tiempo vemos que los agentes que arrancan con interés bajo, y por tanto
memoria baja, son los que más logran aumentar su interés, mientras que los que arrancan con interés alto
sólo descienden, nunca incrementan su interés. Esto es porque básicamente la memoria asociada al valor
de su interés mata totalmente el término de la función logística y el interés sólo puede descender.

Mem_cero:
---------
En este caso tengo termalización cero y memoria inicial (vector Sat) cero. Recordá siempre que Sat
está dado por una integral desde menos infinito hasta t, a medida que el tiempo pasa, eso debería
crecer, y sólo si pasa mucho tiempo con valor de interés bajo eso decaería. Entonces la diferencia
clave con el caso anterior es que como esto arranca en cero, en una primera etapa tiene que ir
acumulando saturación para llegar a lo mismo de lo que parte el sistema en Sin_terma.

Dicho eso, la entropía presenta una disparada en el valor de chi = 1.5, pero en este caso lo
hace solo para los alfas menores a 4, que en el caso sin saturación correspondían a los alfas
en los cuales el sistema convergía al punto fijo central.

En los gráficos de Opinión vs Tiempo, creo que se puede afirmar que hay una mayor tendencia de
los agentes a aumentar su interés en los primeros pasos, a diferencia del caso anterior en el
cual los agentes con alto interés directamente caían.
 
Los gráficos de promedio se ven idénticos, pero el de varianza ahora tiene un máximo en el alfa
máximo y máximo umbral. Igual, la varianza considerada es mínima, lo cual es nimio. Incluso, la 
varianza esa habla de una diferencia respecto del valor medio para cada medición que está a
10^(-3) de distancia del valor medio. Lo cual es una tontera, porque el criterio de corte es
que el sistema varíe menos de 10^(-4) entre pasos. Es decir, estoy tan lejos como mi criterio
de corte me lo permite. Un criterio de corte más bajo permitiría ver una varianza más pequeña.

Lambda_01:
----------
En este caso la termalización también es cero, la memoria (vector Sat) se inicializa igual que
el interés de los agentes y el valor del parámetro que regula la variación de la memoria, lambda,
se fija en 0,1 en vez de 0,005. La diferencia entre este caso y los otros es que ahora el valor
de memoria crecería a un valor más pequeño y por tanto la saturación no llegaría a cancelar tanto
el término de la logística. Es decir, los agentes no tienen tanta memoria, no guardan tanta info
del pasado.

El gráfico de la entropía es bastante distinto, no parece seguir un comportamiento claro. Honestamente
no sé qué extraer de eso, cuál es el motivo de que el gráfico se vea de esa forma. Quizás un
barrido más fino ayude a dilucidar lo que pasa ahí.

Los histogramas ya no dan valores tan pegados al cero, lo cual se observa también en el gráfico de
promedios. El valor de lambda alto hace que la saturación no pueda alcanzar valores tan altos,
es decir que la saturación no alcanza valores altos.

El crecimiento de los intereses en función del tiempo no se ven sustancialmente distintos,
los agentes con poco interés logran crecer, los demás no.

La varianza sigue dando muy chico.

############################################

Hablando con Seba, me tiró la idea de que podría estar bueno cambiar el signo del término lineal,
o quizás sacarlo. Total ese término tiraba al agente al cero. Ahora, con la saturación el agente
cae al cero de una. Quizás sin eso o con un término lineal positivo el agente pasa maś tiempo
en valores altos de interés antes de decaer.

Otra idea interesante sería restringir los valores de interés iniciales a 0,5, interpretando que
los agentes no arrancan interesados en el tema de una, sino que ganan el interés.

Una tercera idea copada sería estudiar si en una red de todos con todos o una Random Regular
un único agente interesado dispara una ola de interés en el resto de la red, y luego esa ola
cae a 0 por la saturación.

Charlemos todo esto con Pablo y vamos viendo a dónde dispara esto. Lo importante entonces es
tener una idea de cómo funciona esto, qué quiero ver y cómo lo voy a medir. Yo diría que esto
que hice ahora tiene un propósito. Puedo concluir que:
1) El sistema no necesita termalización, eso va en contra de mis intereses.
2) La Saturación tiene que inicializarse en cero, no en el valor del interés.
3) La memoria, el lambda, (necesita un mejor nombre) debería tener un valor
definido entre 0,005 y 0,1. 0,1 me parece mucho, porque el sistema ya no
tiende tan cerca de 0. Quizás podamos seguir con 0,005. O quizás siga con 0,01.

-------------------------------------------------------------------------------------------------

12/12/2022

Para continuar con el trabajo me parece importante pensar un segundo lo que siempre me dice
Seba sobre qué es lo que espero que hagan los agentes a interactuar y derivar un modelo en
base a lo que espero que ocurra.
 Ante eso, mi respuesta es que mi intención es que los agentes arranquen con un interés bajo
a medio en el tema. Luego interactúen entre ellos aumentando su interés hasta que se saturen
del tema y lo abandonen. Por ahora, como sólo tengo un tópico, ese es todo mi objetivo.

Dicho esto, puedo ver que mi ecuación necesita sólo dos términos, el de generación y
transmisión de interés (la función logística) y el de saturación (el cual se evoluciona
con su propia ecuación dinámica). Por tanto sacar el término lineal suena lo más razonable.
No creo necesario hacer una prueba con el término ese en positivo, porque eso es un deseo
innato de investigar el tema que no me parece que tenga mucho sentido. Es proponer que
la gente quiere buscar el tema sólo porque sí.

¿Cuáles son los cambios que haré sobre el código?
-------------------------------------------------
.) Las opiniones iniciales las voy a restringir a la región [0,0.3]. Eso suena correctamente
a interés bajo a medio.
.) Voy a sacar el término lineal, no tiene que estar en el modelo.
.) Sacaré la parte de termalización del código, definitivamente innecesaria.
.) Inicializo la saturación en cero. Inicializarla en otro lado es arrancar 
creyendo que los agentes ya vienen estudiando el tema de antes, no es mi idea.
.) Fijo el olvido en 0,01, confío en que ese es un valor razonable.
.) Guardar datos de la saturación.


Charla con Pablo:
-----------------

Pablo me dijo que haga dos cosas:
1) Comparar los resultados del modelo sin el término de saturación con el modelo de Baumann
unidimensional. Estaría bueno ver cuáles parámetros utiliza y qué podemos extraer nosotros
de ahí.
2) Que pruebe ver qué ocurre con la idea de un agente esparciendo una ola de interés en el
resto usando redes de Erdös-Renyi (creo que le haré caso a Seba con las Random Regulars)
con 1000 agentes.


Yo propuse poner un parámetro que reduzca el efecto del interés propio en el incremento
de la saturación, pero honestamente me parece que no tiene sentido, para eso simplemente
aumento el valor del olvido y listo.

Hagamos la burocracia infinita mientras mando a hacer los datos para el análisis del código
sin el término lineal. Mandé a hacer los datos de la fase Sin_lineal. 
También armé unas redes de Random Regulars para hacer lo segundo que me dijo Pablo.

Ahora debería actualizar la Documentación en Programas Python, agregar una nueva carpeta
en Imagenes, actualizar la Documentación de Imagenes y armar una función para graficar
la Saturación en función del tiempo.

Mirando los gráficos, parece ir bastante mejor el modelo sin el término lineal. Presenta
una buena curva en la cual arranca abajo, alcanza un máximo y después cae a cero. Creo
que tiene un buen tiempo la curva. Las curvas de saturación muestran que la evolución
de esta variable es razonable. La saturación arranca con un crecimiento fuerte en
la primera parte y después de que el interés alcanzó un pico y cayó, la saturación
empieza a crecer mucho más lento.

La implementación del modelo en esta forma parece funcionar. ¿Qué quiero hacer ahora?
Peguémosle una leída al paper de Baumann unidimensional y anotemos unas conclusiones.

Repasando el trabajo unidimensional de Baumann, podemos arrancar diciendo que su modelo
presenta 4 parámetros principales. Alfa (controversialidad del tópico), K (fuerza de
la interacción social), beta (intensidad de la homofilia) y r (probabilidad de reciprocidad
al interactuar con un vecino). Aunque de estos 4 parámetros, los verdaderamente
principales son alfa y K. Hace estudios variando los otros dos, pero para los estudios que
muestra el barrido se produce en estos dos y los otros dos se dejan fijos.

Lo que obtiene de este modelo son tres tipos de soluciones. Para bajo alfa y homofilia,
los agentes tienden a un consenso neutral, con todas las opiniones cayendo a cero.
Para alto alfa y baja homofilia una de las posturas gana y todos los agentes se
radicalizan en una dirección. Para alto alfa y alta homofilia el sistema presenta estados
finales de polarización.

El pase de estados de consenso neutral a polarización y radicalización depende de la relación
entre alfa y K. Básicamente una influencia suficientemente grande o tópicos suficientemente
controversiales son necesarios para que el sistema no decaiga a consensos neutrales.

Dicho esto, ¿Qué podría hacer con esto para armar una presentación como me pidió Pablo?
Lo que se me ocurre es tomar primero la ecuación dinámica del modelo de Baumann y mostrarla
en comparación con la que tengo yo. Luego, podría mostrar los estados finales que genera
el sistema en el caso de Baumann y compararlo con mis estados finales. Para eso estaría
bueno tener entonces un sistema con muchos agentes para comparar el sistema de Baumann
a igualdad de tamaño. De ahí yo podría armar gráficos similares del comportamiento de
mis agentes a lo largo del tiempo. En ese caso necesitaría armar archivos de testigos 
mucho más grandes. Quizás tomar 100 agentes como testigos sería razonable.
Y por el otro lado comparar el gráfico de valor promedio de opiniones de Baumann contra
el de valor promedio de intereses. Eso daría una buena muestra de la diferencia de los
sistemas.
Y creo que la mayor conclusión a sacar acá es que Baumann no tiene tantos parámetros 
en este modelo y habría que considerar cómo haría para que nuestro modelo tenga más posibilidades
de algo interesante para ver.

Entonces el plan mañana es primero armar el código para estudiar el esparcimiento de la ola
de interés. Después estaría bueno ver de ir armando la presentación que me dijo Pablo.
Hechas esas dos cosas, lo siguiente sería... ahora no se me ocurre. Pensaré eso más tarde.

-------------------------------------------------------------------------------------------------

13/12/2022

Primero quiero hacer una prueba más sin el término lineal. Quiero ver si puedo manipular 
mejor el término de saturación. Se me ocurre que podría meter un factor 1/2 en la ecuación
dinámica de la saturación. Eso podría hacer que la campana se estire al doble de tiempo.
Me interesa. ¿Lo pongo en una carpeta aparte? Dale, hagamos eso.

Usé el factor de 1/2, pero esto me generó un problema. El problema es que como la saturación
crece lento, los intereses de los agentes logran cruzar el valor de 1. Esto me marca la
importancia del término lineal en la ecuación dinámica. Ese término se asegura que el sistema
nunca pueda pasar el valor de 1, porque para cuando llega a esa zona, la lineal le gana
a la logística. Entonces reincorporarlo suena razonable. Y la lógica detrás de ese
término tiene sentido, un agente dejado por sí sólo va perdiendo interés o memoria
de un tema. Pero justamente, la pérdida de interés o memoria es el término de saturación.
Proponer que la pérdida de memoria en el tema genera desinterés mientras que al mismo tiempo
propongo que la saturación por tener al tema muy presente en la memoria TAMBIÉN genera
desinterés es contradictorio como mínimo. Se me ocurren otras formas menos generosas de
describir eso.

No, sostengo que si tengo el término de saturación, entonces no necesito el término lineal
que hace que la persona pierda interés. Pero esto me lleva a una gran duda. ¿El término
de saturación me garantiza no cruzar el valor de interés de 1? Por lo visto, claramente
no, porque si la saturación crece lento, entonces el interés va a crecer hasta que la
saturación compense y de ahí cae. ¿Tengo una forma de agregar un término que garantice
que esto no cruce el 1? Se me ocurre que podría poner un término lineal que esté afectado
por cuánto vale la saturación y que al principio del modelo el término lineal domine y
a medida que la saturación crece, este término se ve reducido mientras la saturación
pasa a dominar el decaimiento. Suena lindo y hasta realizable, tipo con una tanh().
Ahora andá a encontrar una justificación de eso. Me parecería más razonable justificar
que la saturación arranque con cierto valor.
 Todo esto gira sobre el hecho de que el término logístico que hace crecer a los agentes
como mucho vale 1, en cambio la saturación puede alcanzar valores cercanos a 6. Entonces,
una vez que la saturación supera el 1, estamos tranquilos que no hay chances de que no
hay forma de que esto cruce. La pregunta es: ¿Hay un valor correcto de olvido que usar
para garantizar que el sistema no cruza el 1 de interés? ¿Hay una franja? Podría estudiar
eso y ver qué pasa en la medida en que varío el olvido. Hagamos eso y veamos el gráfico
del promedio del sistema en función del olvido.

Para estudiar esto voy a definir una nueva etapa del trabajo. Me parece importante esto
separarlo de lo anterior porque ahora sí voy a hacer un cambio importante en el código,
cambiando los inputs y los tipos de archivos que voy a generar, por eso me parece
importante diferenciar esto.

Me armé los datos, los analicé y grafiqué. Efectivamente al aumentar el olvido, el sistema
alcanza cada vez valores más grandes de interés como valor máximo. Esto me marca que con
los términos que tengo, sólo tengo al sistema confinado por encima del cero. Ahora mismo
no tengo forma de evitar que cruce el 1. El término lineal se encargaba de eso antes.
¿Es una buena idea sacarlo y deshacerse de eso? Voy por un café y después haré lo que
sigue, dejándome esto para charlarlo con Pablo o Seba en la próxima. No creo que haya
sido tiempo desperdiciado esto, el gráfico va a ser una forma fácil y rápida de transmitir
esta idea de que fijado el olvido en algún valor, el sistema crece hasta que la saturación
lo alcanza.

Arranquemos con el esparcimiento de la ola de interés. Para eso tengo que hacer algunos
cambios en el código:
.) Usar redes Random Regulars. Confío en que redes de grado 4 están bien. Podría hacerlas
de mayor grado, pero no sé si valga la pena.
.) Cambiar la inicialización del interés para que todos menos uno estén en 0.
.) Dirigir los resultados a una nueva carpeta
.) Guardar las opiniones de todos los agentes en el archivo de testigos. ¿Eso sería un
problema para la función que grafica Opi_vs_tiempo?

Hice estos cambios, comentando varios elementos que preferí no borrar, como el sleep al
final, el armado del archivo de Saturación y el automatizado de la cantidad de testigos.
Voy a hacer una prueba del código y mañana veo qué tal

El código corrió sin problemas, una sola iteración tardó 14 segundos. Después veo si lo
hago acá o en la pc de Oporto.

-------------------------------------------------------------------------------------------------

14/12/2022

Ya sé qué voy a hacer. Antes de empezar con el tema de la ola de interés, en el cuál podría
variar alfa y el interés inicial del agente, creo que lo mejor será armar la presentación 
que me dijo Pablo. Además puedo en esa presentación plantear el tema de si debería o no
mantener el término lineal. Más que nada en términos de cómo justificarlo digo.

Creo que tengo la mitad de la presentación hecha. Hoy estuve con varias cosas que me fueron
frenando, más que anda firmar cosas y mandar mails o mensajes. Veré de tenerla terminada mañana.

Me parece importante agregar algunas diapositivas explicando el razonamiento de lo que hice,
y quizás repasar si las cosas que estuve viendo son interesantes de contar. De paso, me 
parece que no es mala idea mostrar por qué empecé a hacer cuentas sin el término lineal y
por qué diría de reincorporarlo, incluso haciendo uso del término con tanh() para que 
el término lineal pese al principio y desaparezca al final.

Mañana pensaré sobre la ola de interés, cuánto tiempo puede tardar y qué quiero medir de eso.

-------------------------------------------------------------------------------------------------

15/12/2022

En la mañana terminé de preparar la presentación que le iba a mostrar a Pablo. Seba me dijo
que prepare algunas diapos con cómo se comporta la función logística en función de alfa y
chi. Me parece razonable, y veré de hacerlo mañana.

Sobre el tema de estudiar la ola de interés, 
se me ocurren dos gráficos que serían interesantes para mirar. Uno sería el de tiempo que 
tarda el agente en alcanzar el interés máximo en función de que tan lejos está del agente
originalmente interesado. El segundo sería un gráfico en el cual graficaría la curva de
interés en función del tiempo para un agente (al azar) según la distancia con el agente
original. Mi idea es a esto incorporarle el término lineal, pero no sé si sea buena idea.
Quizás necesite tantear un poco más el terreno para eso. Porque con el término lineal 
presente quizás la función no crezca tanto, entonces convendría tener un mayor olvido,
o meter un factor que haga más lenta la variación del término de saturación.

Hablando con Seba, él me propone que por ahora descanse un poco de la idea del término
de saturación y que vuelva al término lineal. Su argumento es que el término de saturación
lo que hace es simplemente romper el estado final de los agentes con un alto nivel de interés
eterno que no decae. Esto se condice con lo que uno espera ver, porque tarde o temprano
el interés en un tópico termina cuando surge un nuevo tópico. Es decir que es un término
que importa a largo plazo, pero no hace falta preocuparse por eso ahora y que más importante
es volver a trabajar con más de un tópico. Tendré eso en cuenta, así que ahora tengo que
tomar el código y preparar una etapa nueva para la ola de interés. Podría armar una primer
etapa 1D y luego una de 2 tópicos. Con término lineal, sin saturación. Luego la idea
es medir lo que dije que iba a medir, eso me cierra bárbaro.

Ahora, hay que pensar en términos de cuántos datos almaceno, cuáles son los parámetros que
voy a variar y qué pretendo mirar exactamente. Para la última pregunta tengo un poco
una respuesta, considerando los dos gráficos que dije de armar. Esos me van a dar una
buena idea de los tiempos que el sistema tarda en esparcir el interés.

.) En lo que a los parámetros respecta, razonable es que voy a variar la controversialidad
de los tópicos. Si no hay término de saturación, el olvido se descarta como opción. Pero
lo otro que puedo manejar es el interés inicial que presenta el agente que empieza interesado.
Podría variarlo entre 0,5 y 0,9, por ejemplo. También podría variar el chi supongo. Entonces
ahí tengo tres parámetros para variar, eso significarían tres gráficos para armar mapas
de colores. Yo empezaría variando la controversialidad y el interés inicial.

.) La pregunta final es la más difícil. ¿Cuánto tiempo va a tardar esto y cuántos datos
almaceno? En tiempo, una simulación que hice ayer tardó 14 segundos. Supongamos 80 o 100
simulaciones, variando alfa entre 2 y 6 (pondría chi = 1) tomaría 50 valores de alfa y
variando el interés inicial entre 0.5 y 1 tomaría 13 valores. Eso serían 65000, con un
promedio de 25 segundos cada una, eso es un total de 451 horas. Si divido eso en 20
hilos me da 22 horas. Tranca.
¿Pero y qué pasa con el almacenamiento? El tema es que para medir cuánto le tarda a 
cada agente en llegar el interés, necesito tener el registro de la evolución de las
opiniones de todos los agentes en todo momento. Pero un sólo archivo de Testigos me
consume 25 megas. Así que necesito 1000 gigas y cosa para guardar todo eso. Imposible.

Se me ocurre algo más razonable, que es guardar datos de agentes según su distancia con
el agente inicial. Para eso tengo dos posibilidades. Una es cambiar el código de Python
para que al armar las matrices de Adyacencia, en el mismo archivo txt se me guarden los
conjuntos de agentes según su distancia al agente principal. La otra opción es armar una
función en C que a partir de la matriz de Adyacencia identifique los grupos a los que
pertenece cada agente, luego con eso me arme un vector que cataloga a los agentes y otro
vector que marque cuántos agentes hay en cada conjunto. Luego, la idea sería tomar dos o
tres agentes de cada conjunto. Eso sería mucho mejor, total en una red de grado medio 4
Random Regulars, a distancia 3 del primer agente hay 4³ agentes, es decir 64. A distancia
5 ya considero 1024 nodos. No confundir con que sean todos distintos, podría pasar que haya
muchos repetidos. Pero la idea es que tengo como máximo esa cantidad, y ya a distancia 6 son 
4096 nodos. Creo que puedo razonar fácilmente que no puede haber nodos con distancia mayor
a 6, porque aunque haya dos nodos que tengan los mismos 4 vecinos, eso haría que en vez
de sumar 8 sujetos reales a ese conjunto, en realidad sumo 4. Me estoy enredando con esto
al pedo, porque técnicamente podría pasar que varios agentes sumen unos mismos pocos
agentes, entonces no es tan sencillo. No sé si más razonable sería pensar en términos
de todos los grafos posibles armables y de ahí calcular la distancia media del sistema.
Pero dejemos eso para alguien que entienda más de esto, vamos a los bifes.

La conclusión es que suponiendo que el tamaño de la red sea con 6 agentes de distancia,
si tomo dos o tres agentes por cada anillo, eso serían 18 agentes como máximo. Eso
es un 2% de los 1000, lo cual es algo mucho más razonable para guardar de datos.
Armemos el plan para mañana:

.) Iniciar una nueva etapa. Borro la carpeta de Ola_interes en Saturacion_1D y me preparo
una nueva etapa que sea Ola_interes con una primera carpeta que sea topico=1.
.) Me aseguro de tener bien guardado el código de Saturacion_1D.
.) Armo una nueva función en C que catalogue a los agentes según su distancia a un cierto
agente y que me marque cuántos agentes tengo en cada conjunto.
.) Agrego líneas para modificar la escritura de mis archivos de forma tal que se 
guarden los intereses de un pequeño grupo de agentes.
.) También tengo que guardarme los agentes y los conjuntos a los que pertenecen
cada uno. Eso va a estar bueno para comparar con Python que realmente están
bien catalogados los agentes antes de mandar esto a funcionar largo.
.) Desconecto las funciones dinámicas asociadas al término de saturación.
.) Vuelvo a agregar el término lineal.
.) Redirijo los archivos a la carpeta correcta.

-------------------------------------------------------------------------------------------------

16/12/2022

A la mañana no hice mucho, como ayer hicimos la jutnada de fin de año, llegué tarde
a casa y hoy me levanté más tarde.

Ahora a la tarde voy a seguir con lo que anoté ayer. Armé una función que categoriza
a los agentes según su distancia al nodo inicial, que es el primero. Ahora lo que 
me queda es ver si esta categorización está bien hecha. Para eso lo que haré es comparar
lo obtenido con lo que pueda hacer en Python al respecto.

-------------------------------------------------------------------------------------------------

19/12/2022

Lo primero que voy a hacer hoy es armar un código que levante el archivo de la
red de adyacencia de Random Regulars y que levante los datos del archivo que armé
y me permita compararlos. Ahora que lo pienso, no necesito un código para eso,
tengo simplemente que levantarlos y empezar a compararlos.

Logré armar un código en Python que a partir de la matriz de Adyacencia cataloga
a los agentes según su distancia al agente inicial. A partir de esto, puedo
ver que el código hecho en C no funciona tan bien. Para arrancar, el código
en C registra a distancia 2 53 agentes, cuando lo máximo es 16, y a distancia
3 registra 618, cuando lo máximo es 64. Hay que revisar el código en C.

El error en el código C está en que estoy visitando a los agentes a distancia
2, marcando para visitar a sus vecinos, y antes de terminar la segunda pasada
de agentes a visitar, estoy visitando los vecinos que estarían a distancia
3. Entonces no estoy siendo organizado. Creo que lo mejor sería agregar un
segundo vector que sea inmutable durante las pasadas, y que por otro lado
el vector de Visitar sea el que acumule las marcas de a quién ir a visitar
más tarde. Pero hagamos eso en casa mañana, hoy volvamos temprano.

-------------------------------------------------------------------------------------------------

20/12/2022

Hoy es feriado, voy a laburar un poco en casa y después mañana sigo con lo que tengo.
Hoy quiero solucionar la función que cataloga a los agentes en C.

De paso, creo que comitee pero no pushee los datos ayer, por eso no están las últimas
actualizaciones subidas. Definitivamente eso fue lo que pasó. Bueno, voy a solucionar 
el tema de la función en C para que correctamente catalogue a los agentes, luego voy a 
revisar el archivo y después tomaré notas. Mañana antes de commitear y pusehar o pullear
nada, corregiré esas cosas a mano y revisaré que el pull no arruine nada.
Cosas que se me ocurre puede arruinar:
.) Las notas de Progreso
.) El archivo de Python armado en la carpeta "Ola_interes"

Modifiqué el código y ahora parece funcionar bien. Por lo menos ahora reconoce que la
red tiene distancia máxima 9, que es creo que la misma que vi en el archivo de Python
de la facultad. Pero no puedo saber si funciona bien porque no tengo el código de Python
que armé ayer. Y además la red que tengo ahora en mi pc es distinta que la red en la
facultad. Mañana tendré que comprobar que todo funciona bien.

No voy a avanzar más hoy para no complejizar las cosas que se van a sobreescribir.

------------------------------------------------------------------------------------------------

21/12/2022

Lo primero que hice fue organizar el pusheado y pulleado de github. Intenté que no se perdiera
nada. Lo interesante es que descubrí que Github en estos casos guarda los archivos que no
son compatibles en un "stash", y luego te permite volver a pushearlos más tarde.

Hecho esto, simplemente puse a prueba mi código en C y el código en Python. Ahora sí los dos
catalogan lo mismo. Ven los mismos agentes a las mismas distancias. Ahora que esto funciona,
puedo implementar la función y avanzar con la etapa de Ola_interes.

Lo siguiente a hacer es esto:
.) Agregar unas líneas que marquen cuántos agentes tengo en cada conjunto.
.) Agrego líneas para modificar la escritura de mis archivos de forma tal que se 
guarden los intereses de un pequeño grupo de agentes.
.) También tengo que guardarme los agentes y los conjuntos a los que pertenecen
cada uno. Eso va a estar bueno para comparar con Python que realmente están
bien catalogados los agentes antes de mandar esto a funcionar largo.
.) Desconecto las funciones dinámicas asociadas al término de saturación.
.) Vuelvo a agregar el término lineal.
.) Redirijo los archivos a la carpeta correcta.

Al mediodía fui a comer al brindis que dió el rector en frente del Pabellón 2 y después
me fui temprano porque tenía turno con la dentista. Antes de irme completé la burocracia
infinita. Mañana continuo con esto.

------------------------------------------------------------------------------------------------

22/12/2022

En la mañana no hice nada del proyecto porque cuando llegué me di cuenta que me había olvidado
la llave en la oficina, así que estuve esperando hasta que abrieran secretaría, les pedí
una llave, conseguí la mía y me puse a trabajar.

Pero antes que nada, estuve anotándome en cosas, con lo que me anoté para lo de la ley Yolanda
(en lo cual completé un código que saqué del Sigeva de Conicet) y después me anoté para dar
curso de matemática para ingresantes del CBC.

Ahora sigamos con lo de ayer. Agregué el nuevo vector pi_Cant, el cual voy a usar para llevar
cuenta de cuántos agentes tengo a distancia d de mi primer agente. Para poder inicializar este
vector, necesito conocer la distancia máxima a la cual un agente puede encontrarse del agente
inicial. Estaba teniendo problemas porque la función Distancia_agentes returneaba ese número,
pero el número me quedaba uno por encima y no entendía por qué, pero ahora me di cuenta.
Lo que pasa es que, supongamos que mi sistema tiene agentes con separación siete del primer
agente. Entonces, cuando estoy revisando los agentes a distancia seis, el código está marcando
a los agentes a distancia siete. Antes de hacer una nueva pasada, el while avanza la distancia
a ocho. Luego revisa a los agentes a distancia siete y revisa si alguno de sus vecinos está
a distancia ocho para marcarlo, pero como todos los vecinos de los agentes a distancia siete
ya fueron marcados, no marca a nadie. No queda nadie más por visitar, así que el while va
a terminar acá, pero antes de terminar, vuelve a incrementar la distancia a nueve. Y por eso,
a pesar de que sólo hay agentes hasta distancia siete, el sistema termina con el valor de distancia
en nueve. Veré de cambiar eso, creo que es complejo al pedo. Es una cuestión de prolijidad.

Después de comer fuimos a una charla sobre el modelo de Ising. Al final hoy no pude trabajar
mucho, pero puedo decir que avancé en ir organizando el código para la medición de la Ola de interés.
Me quedé en armar un vector que registre quiénes van a ser los agentes testigos. Es decir, el
vector tiene que tener en sus coordenadas los números de los agentes testigos. Para eso mi idea
es seleccionar a los agentes en orden recorriendo el vector de Sep. En principio tomaría tres
agentes testigos, después podría intentar tomar más. Pero nada me garantiza que siempre
tenga suficientes agentes para lo que yo quiera. Tipo, tengo un sólo agente a distancia cero
y 4 a distancia 1, eso siempre. Entonces tengo que considerar que si i_testigos es la cantidad
de testigos que planeo tomar de cada conjunto, entonces puede que no haya esa cantidad de agentes
en ese conjunto. Mi mayor miedo con esto es que después a la hora de levantar datos, no va a ser
tan obvio si la cantidad de agentes a cada distancia es variable. Pero bueno, eso será un problema
para el Python. Confío en que si soy prolijo y coherente, eso tiene solución.

------------------------------------------------------------------------------------------------

23/12/2022

Hoy declararon asueto en la facultad, así que hoy laburo desde casa. Lo primero
que quiero hacer es armar una función que tome el vector de testigos y le asigne los
agentes que serán los testigos de los cuales guardaré los datos de interés durante la
evolución del sistema.

Armé el código, ahora estoy listo para implementarlo. De paso, estoy considerando tomar
el consejo de code aesthetic a la hora de nombrar variables y agrandar los nombres de
las variables y empezar a usar un poco más el auto completar. Aunque tendré que ver que
tanto me acostumbro a eso. Y si vale la pena hacer eso con un código tan largo como el que
ya tengo. Supongo que podría ir cambiando estas cosas en Enero, que no voy a estar yendo
a la facultad.

Una vez que implemente esto, no me tengo que olvidar de pasar el código que acabo de armar
al Archivo, así lo tengo ahí guardado todo esto.
Ya guardé el código en Archivo, así como ya implementé la función de Lista_testigos en el
main. Ahora queda seguir con las demás instrucciones. Y por último queda revisar que toda la
dinámica se resuelva con sentido.

------------------------------------------------------------------------------------------------

26/12/2022

En la mañana llegué y charlé con Pablo. El plan es poner en StandBy el trabajo de Ola_interes.
Vamos a ponernos a laburar reduciendo los parámetros en el exponente de la función logística
y agregando un parámetro que regule la relación entre el término lineal y el logístico.

Ya registré las cosas de Ola_interes, empecemos con el trabajo de lo que dijo Pablo. Para eso
tengo que tomar el código que tengo y hacer lo siguiente:
.) Voy a unir el alfa y el chi en un único parámetro. Eso es un cambio que haré en la ecuación
dinámica, pero en principio no voy a borrar el parámetro alfa, ese lo voy a usar para separar
los conjuntos de datos y así hacer una primera verificación del comportamiento del sistema
con alfa. Básicamente veré qué tal se comporta con alfa en 2, alfa en 4 y alfa en 6.
.) No voy a borrar, pero sí a comentar todo lo relacionado con el término de saturación.
Esto implica la matriz de Saturación y los parámetros asociados a su ecuación dinámica.
.) Voy a sacar los vectores asociados al tema de seleccionar testigos específicos, eso
es innecesario por ahora y complejiza el código al pedo.
.) Tengo que redirigir los archivos a la nueva carpeta de la nueva etapa.
.) ¿Hago esto con dos agentes o con 1000? Lo haré con dos en mi pc. Así que tengo que comentar
la parte donde tomo la red de MARE.
.) Donde pueda, voy a empezar a cambiar nombres de las variables y punteros para que sean más
claros.

El nombre de la nueva etapa es Cambios_parametros. Hice bastantes cambios respecto de los códigos,
escribiendo de forma completa los nombres de las cosas, como por ejemplo llamando las matrices
Ady->Adyacencia, Ang->Angulos, etc., también cambié los punteros a struct para que sean 
ps_var->ps_variable y ps_par->ps_parametro. Espero que no haya "s" dando vueltas por ahí.

Ya armé el código para estudiar el comportamiento del sistema en el espacio de amplitud vs epsilon.
Lo probé y parece funcionar sin problemas, mañana lo mando a correr con Instanciar, la idea sería
estudiar el comportamiento del sistema para dos agentes con tres valores de alfas distintos y ver
a qué valores tiende el sistema. Visto eso, se lo comento a Pablo y vemos si lo mando a la pc de
Oporto o si le implemento el término de saturación.

------------------------------------------------------------------------------------------------

27/12/2022

Hoy me sentía medio mal, creo que comí demasiado en las fiestas y me indigesté. Así que ahora
queda esperar a que se me pase. Estoy revisando el archivo para ver que todo funcione bien.
Debería mandar a correr Instanciar.sh. Para eso, necesito definir los valores que voy a barrer.
Voy a usar alfa {2,4,6} para probar que con alfa por debajo de 4 el sistema sólo tiende al punto
fijo intermedio, mientras que para alfas por encima el sistema puede tender a los puntos fijos
extremos.
 El epsilon, que es el umbral, lo tengo que mover entre valores que sean menos que el alfa y el
doble de alfa. Yo diría de mover el umbral de a 1, entre 1 y 14.
 La amplitud en cambio la voy a mover entre varios valores, algunos por debajo de 1 y otros por
encima. Tomaré {0.5,0.6,0.75,0.9,1,1.25,1.5,2,3}
Al final reduje un poco eso, porque era mucho para simular en algo que sólo quiero hacer una
inspección rápida. El Epsilon lo muevo entre 1 y 14 pero saltando de a números pares y tomando
el 1. Para la amplitud tomé {0.5, 0.75, 1, 1.5, 2, 3}

Ya mandé a correr y obtuve los datos. Ahora mi interés sería primero armar gráficos de promedio
de los valores finales en el espacio de fases. Para eso tengo que considerar que los archivos ahora
tienen otros nombres y por tanto otros parámetros.

Ahora me doy cuenta que pensé el tema de los epsilon totalmente al revés. La idea es que alfa sea más
del doble de epsilon. Entonces debería manejar un alto valor de alfa y de ahí mover el epsilon por debajo
de la mitad de ese alfa.

Me parece que antes de mostrárselo a Pablo, debería armar datos nuevos. Así que mañana, si voy a la
facultad, me voy a poner a armar una nueva tanda de datos y simplemente mando a correr el archivo
que los grafica.

------------------------------------------------------------------------------------------------

28/12/2022

Sigo sintiéndome medio mal, así que me quedé en casa. En la mañana descansé, a ver si así para
mañana me encuentro mejor. Pablo me dijo que directo me reincorpore en Enero, pero veré
qué hago mañana.

Volviendo al trabajo, ayer me di cuenta que hice un montón de simulaciones mal armadas, porque
moví al epsilon hasta valores que son el doble de alfa, pero la curva que determina si el sitema
cae al punto fijo mínimo o al máximo es 2*alfa = epsilon. Es decir, epsilon debería moverse entre
valores que son la mitad de alfa como máximo. Entonces voy a cambiar los valores de epsilon para
que se muevan entre 0.5 y 3 de a 0.25.

Ahora se están armando los datos. Esto podría tomar una o dos horas. Debí haberlo pensado y mandar
a hacer esto durante el almuerzo. ¿Qué hago ahora? Puedo ponerme a corregir los nombres en las
funciones de Python; leer el paper de SocioPhysics; o intentar planear el próximo paso de este
modelo cuando los datos estén hechos. Intentemos arrancar con esto último.

El modelo tiene dos términos, uno lineal y uno logístico. Vimos que en el logístico, la relación
entre la controversialidad alfa y el umbral de interés epsilon es lineal en lo que la tendencia al
último punto fijo del sistema refiere. Por tanto, decidimos fijar alfa en un valor y variar sólo el
epsilon. Pablo estaba un poco en duda de por qué armé mis simulaciones a partir de alfa=4, así que
para mostrar que en casos de alfa bajos el sistema tiende al punto fijo intermedio, es que armé 
simulaciones con alfa=2, y también armé otras con alfa en 4 y en 6 para ver que ahí el sistema sí
converge al punto fijo máximo.
 ¿Qué espero ver en el mapa de color de promedios? Lo que espero sería que para alfa menor a 4 el sistema
converja al cero o cercano para cualquier epsilon y que si la amplitud es muy grande entonces logre
converger a un valor alto. Entonces, sólo espero que la franja de arriba logre converger a valores
"lejanos" del cero. En cambio, para alfa=4 espero que el sistema converja al punto fijo máximo para
epsilons menores a 2 y al punto fijo mínimo para epsilons mayores a dos. Entonces la franja inferior
y una parte un poco más grande de la derecha debería converger al cero o cercano. Finalmente, en
alfa 6 el esta franja inferior debería achicarse y la región a la derecha debería correr su inicio más
a la derecha.

Suponiendo que esto se ve bien, sería una buena idea mandar a hacer unos gráficos más copados en Oporto.
Pero hecho eso, ya tendré entonces una buena idea de cómo funciona el modelo. La pregunta importante es:
¿Qué quiero hacer con esto? Llevamos un año preguntando cómo modificar el modelo para "mejorarlo", pero
nunca queda claro qué sería "mejorarlo". Probamos hacer lo de la transcrítica con una función cuadrática,
después probamos con una logarítmica, después probamos con la logística, metimos un término de saturación,
sacamos el lineal, metimos el lineal y sacamos el de saturación y después volvimos a reducir los parámetros
para reinsertar un parámetro de presión social que otra vez nos saca de la región [0,1]. ¿Qué estamos
queriendo ver acá? ¿Qué datos usaríamos para contrastar si el modelo va bien? Creo que la constante
sensación de: "Implementé en el código el nuevo término, ¿Ahora qué?" viene de acá. La idea de medir la
Ola de interés, que posiblemente no es una idea tan interesante, aún así era una primera idea de algo
qué hacer con el modelo. El término de saturación también proponía una idea viable en medición al poder
intentar medir cuánto tiempo tarda en decaer el interés e intentar ajustar al sistema con eso.

Por enésima vez, volvamos a las bases. Tengo un modelo multidimensional de interacción entre agentes.
Estos agentes poseen "interés" (voluntad de investigar o consumir contenido de un tópico) para cada uno
de los tópicos considerados. Como el interés en un tema sólo puede ser positivo, al interactuar dos
agentes lo único que pueden hacer es potenciar su interés en el tópico, o si ambos tienen un bajo
interés se mantienen desinteresados porque ninguno está lo suficientemente manija para motivar al
otro. Los agentes interactúan con todos sus vecinos de forma simultánea y el interés de cada uno
se promedia para generar un único término que acrecenta el interés. Este término compite con un término
lineal que impulsa el interés a cero si el agente no interactúa con nadie.

Actualmente estoy haciendo pruebas del modelo en una dimensión, es decir un sólo tópico, y por tanto
tengo dos parámetros para variar. El umbral de interés Epsilon y la presión social Kappa (Debería 
cambiarle el nombre en el código). Ahora estoy explorando el comportamiento del sistema en función de
estos dos parámetros. Pero la idea sería entonces pasar a dos agentes y ver el comportamiento del
sistema también en función de Delta. Lo que me imagino que hará esto será trastocar la relación de
valores de epsilon para los cuales el sistema converge al punto fijo mínimo, ya que entonces la suma
de intereses de los vecinos se verá potenciada por los intereses en otros tópicos. OK, tendré eso,
¿y después qué? Me parece importante tener un objetivo para después, porque en definitiva lo que veré
es un decepcionante (suelen serlo) mapa de colores del sistema en función de Delta y Epsilon (me parece
que son los parámetros que vale la pena utilizar como variables). Esta es una charla para llevarle a 
Pablo de a qué queremos ir, porque sino siento que vamos a seguir haciendo cosas y buscando "soluciones"
sin conocer nuestro problema.

Creo que esta es una charla para tener con Pablo, pero me parece que si voy a la charla debería ir con
una propuesta, no simplemente caer con: "Tengo un problema, ¿Ahora qué hago?" Me parece importante entonces
tener armados datos del sistema tanto para un tópico como para dos. Así que yo diría de mandar a
armar datos en Oporto. Si eso va bien, ya mañana empezaré a modificar el código para dos agentes.
Luego de eso, empezaremos la charla con Pablo sobre esto.

Antes de terminar el día mi plan es mandar a correr el código en Oporto. Para eso necesito definir
la región en la cuál voy a variar mis parámetros. Usaré tres alfas igual que antes, armaré 80
iteraciones, y queda definir de dónde a dónde se mueven los kappa y los epsilon.

Voy a mover Kappa y Epsilon en la región entre 0.5 y 3 de a 0.1, lo cual significa 26 valores para
cada uno. Luego, voy a hacer 100 iteraciones, y por lo que vi cada simulación debería tardar entre
3 y 4 segundos. Entonces si mis cuentas están correctas, hacer esto debería tardar aproximadamente unas
nueve horas. Si lo mando ahora, ya mañana está. Listo, ya está mandado a correr. Mañana vemos qué pasó.

------------------------------------------------------------------------------------------------

29/12/2022

Hoy me siento mejor, así que vine a la facultad. El código en Oporto sigue laburando. Parece que va a tomar
dos días en terminarse. El problema está en que yo planee que cada código tardara unos cuatros segundos
aprox. en terminarse, sin embargo están tardando entre tres y cinco veces ese tiempo en promedio, por eso
es que va a tardar el triple o cuádruple de tiempo en terminarse. Y considerando que mis cálculos daban
9 horas, entonces en realidad esto tardará entre 27 y 36 horas más o menos. Quizás más cerca de 40.

Así que no voy a poder analizar esos datos hoy. Lo que puedo hacer es mandar a correr datos en la pc de
la facultad para poder analizar eso y mostrárselos a Pablo. Después podría preparar el tema del environment
y de los archivos de Python para tener en la pc de Oporto cosa de que una vez que los datos estén listos poder
sacar los gráficos de una. Y si me sobra el tiempo, suena a que sí, debería intentar ponerme a preparar el 
código para estudiar el caso 2D.

Ya preparé el environment en la carpeta de Programas Python y me pasé la carpeta de Python a la pc de oporto.
Igual posiblemente la tenga que modificar para observar algunos otros datos. Mientras las demaś cosas terminan
de armar datos, voy a hacer la burocracia infinita y la documentación de los datos.

Ya revisé los datos que tengo en la pc de la facultad, efectivamente pareciera observarse que a medida que alfa
aumenta se va generando una curva en el espacio de parámetros que separa el sistema que tiende al punto fijo
máximo del que tiende al punto fijo mínimo. Quizás convenga observar algunos valores más altos de epsilon para
ver mejor la curva en los alfas más altos.

Mirando el gráfico, creo que podría tomar entonces alfa=4, kappa=1 (no tengo un gran motivo para hacerlo, pero
creo que esos datos podrían llegar a ser más generales en ese caso), y pasar el sistema a un caso de dos tópicos,
así vemos como el sistema varía en función de cos(delta) y epsilon.

Hagamos el pase a 2 tópicos. Para eso primero guardemos el código de 1 tópico. El pase a 2 tópicos únicamente
requiere cambiar la dirección en la que se guardan los archivos, verdad? Porque en el código es tan simple como
cambiar i_T=2.

Bien, ya hice las correcciones para que el sistema labure en el caso de 2 tópicos. Ahora queda esperar que se
armen los datos y después empezar a analizarlos. Si esto funca, podría mandar a revisar esto en Oporto. Si
estuviera apurado de datos, mandaría a hacer esto en otra pc, tipo Algarve o Setubal, así labura todo junto.

Ahora me voy a dedicar a leer el paper de Sociphysics. Quizás eso me ayude a abrir la mente y ver ideas de
a dónde guiar el tp.

Lo último que me dijo Pablo es de agregar el término de saturación a esto. Creo que también dijo de usar más
de un tópico. O no estoy recordando que fue lo segundo que me dijo. Igual ya tendremos para hablar.

------------------------------------------------------------------------------------------------

09/01/2023

Luego de mi semana de vacaciones, vuelvo al trabajo. Lo primero que estoy haciendo es revisar en qué quedé
y continuar desde ahí. Había mandado a hacer unos datos, y creo que faltaba armar los gráficos a partir
de eso, así que estoy en eso, en armar los gráficos.

También había dejado armado el código para correr datos para el caso del sistema en 2D. Así que tendría
que mandar a correr esos datos también en Oporto. Eso va a tomar unos días, pero sería importante entonces
revisar bien la región en la que voy a analizar esto.

Teniendo esos datos armándose, lo siguiente sería lo de incorporar el término de saturación a la ecuación.
Y finalizado eso, simplemente puedo ponerme con preparar una charla con Pablo al respecto, sobre qué
hacer con el trabajo a partir de ahora.

El código está listo para correr con 2 tópicos. Así que lo que voy a hacer es mandarlo a Oporto y pensar
la región en la cual voy a correr los datos. Pero tengo un hilo que está siendo usado para graficar los
datos de la tanda anterior. Así que voy a tener 19 hilos. Entonces armaré un poco menos de las simulaciones
que pensaba, cualquier cosa después agrego. Después me armaré el código agregando el término de saturación
y mandaré a correr los datos en Algarve, así no tengo que esperar a que esto termine y para el fin de la
semana ya tengo todo visto y preparado.

Pensemos la región que voy a explorar. Tengo kappa definido en 1, así que los intereses se encuentran
restringidos a la región [0,1]. Los parámetros que voy a variar son Epsilon y Cosdelta. Alfa está
definido en 4. Por los datos que armé en la pc, para Epsilon con 1 o menor, el sistema prácticamente
llega a 1, no vale la pena mirar esa región. Recién en Epsilon=2 el sistema empieza a caer al cero.
Yo movería el epsilon entre 1.5 y 3.5, creo que esa sería una región interesante.
El cosdelta lo voy a mover entre 0 y 1 de a 0,05, mientras que el epsilon lo voy a mover entre 1,5 y 3,5
de a 0,1. De esa manera, ambos parámetros recorren 21 valores. Luego voy a querer hacer 6 iteraciones
por cada hilo, y siendo que tengo 19 hilos disponibles, eso son 114 iteraciones. Y cada simulación en
promedio parece tardar 10 segundos. Mandar a correr esto parece que va a tardar unas 8 horas aprox.
Agrandemos epsilon para que vaya hasta 4 entonces, total el tiempo lo tenemos.

Ya lo mandé a correr, aunque parece que va a tardar entre tres y cuatro veces lo que dije, porque 
pareciera que las simulaciones no tardan 10 segundos, sino 30 en promedio. Así que eso es un día entero,
mañana a la tarde estarán esos datos. Ahora que eso ya está corriendo, lo que queda es pasar el archivo
de Python a Oporto para realizar el análisis de los datos. Pero mejor dejar esto para después, porque
no creo que sea buena idea modificar el archivo de Python mientras está siendo ejecutado.

Por tanto, lo siguiente es reincorporar el término de saturación y hacer unas mediciones con eso. Si
lo que veo en la pc va bien, podría empezar a subir esto a Algarve, cosa de tener estos datos armándose
en simultáneo a los del sistema en 2D. Y ya que estoy, si hago eso, debería separar los códigos en
la carpeta y realizar la documentación correspondiente.

Mandé a correr unos datos en la pc de la facultad, pero cada simulación tarda un minuto aproximadamente.
Es complicado armar suficientes datos rápido. Hoy voy a armar algunos y mañana armo el resto. Ahora me
parece que lo mejor es empezar a subir los archivos a la pc de Algarve para poder correr los datos ahí.

Creo que tengo todo lo necesario en Algarve como para empezar a simular datos en Algarve. Aunque ahí
está Ale también, debería no consumir todos los hilos.

Ahora que tengo mi café, pongamos las cosas en orden. Creo que estoy muy lentamente recuperando ritmo
y me parece que hay que acelerar un poco las cosas. 

.)Por un lado tengo los datos que mandé a hacer para el modelo con el termino logístico, 
parámetros Kappa y epsilon y con un único tópico. Cuando tenga armados los gráficos, los
paso a la pc para mirarlos. Parece que se va a tomar unas buenas horas en eso.
.) Por otro lado, también en Oporto, estoy armando datos para el sistema en el caso de dos tópicos.
En esta fase los parámetros son el Cosdelta y Epsilon. Kappa está fijado a 1 y alfa está fijado a 4.
Los datos deberían terminarse para mañana.
.) También estoy armando unos datos del sistema con el término de saturación, eliminando el término
lineal. Esto lo tengo en la pc de la facultad, pero como en el resto de la semana no voy a venir, creo
que son bastante al pedo. Me parece que mejor va a ser mandar a correr esto en Algarve directo y ya ahí
ver que hacer con eso.
.) Dicho esto, necesitaría tener en Oporto el archivo de Graficar.py para graficar los datos de mi sistema
con dos tópicos. Entonces primero tendría que cambiar mi archivo de graficación en la pc y después pasar
el archivo cambiado a Oporto.
.) Además, como voy a tener un poco de tiempo en lo que se terminan estas corridas de datos, podría mandar
a hacer una corrida más que si contenga el término lineal. Creo que eso valdría la pena mandarlo en Oporto.
.) Teniendo todo esto, creo que corresponde entonces armar una presentación donde junte los gráficos importantes
y ya decirle a Pablo de tener una charla, primero para hablar de los resultados obtenidos, y segundo para
hablar de a dónde llevar el trabajo, porque siento que si resuelvo estas cosas, entonces no sé a dónde quiero
ir después.

No puedo mandar a correr en Algarve, Ale está usando todos los hilos. Podría pensar de quizás mandar a correr
en Setubal sino. Mañana lo que voy a hacer es cargar los datos del modelo con Saturacion en Oporto y mandar
a correr eso mientras grafico los datos del sistema con 2 tópicos y me descargo los gráficos del sistema con
1 tópico. Hecho eso, me pondré con los archivos de Python. Y luego, arrancaré con el tema del final de Minnini.


------------------------------------------------------------------------------------------------

10/01/2023

Lo primero que hice fue descargar los gráficos de mapas de colores de Promedio de Opiniones.
Se ven bastante bien, pareciera haber una relación no lineal entre el Kappa y el Epsilon.

Después seguí con el armado de los archivos de Python para graficar los datos del sistema en el
caso de dos tópicos. La mayor diferencia que tengo que hacer es que ahora el kappa está fijo, entonces
mi parámetro es el Cosdelta y tengo que cambiar los nombres de todo. Acabo de pensar esto y me di cuenta
de la boludez que es. Lo que se me ocurre es primero hacer los gráficos. Si eso va bien, pasaré a cambiar
esto considerando que mis gráficos van sobre dos parámetros. Entonces, haré esto de forma genérica llamando
a las columnas del DF "parametro 1" y "parametro 2". Luego con eso armo las funciones de python, y después
le voy pasando los nombres que poner en los gráficos, eso funciona mejor.

Ahora que lo pienso, no puedo probar el archivo de Python porque no tengo datos en los cuales probar esto.
Bueno, voy a guardar mi código de saturación, armaré datos para la fase 2D y pruebo el código de python.
Si eso funca, probaré mi idea de renombrar esto de forma más general.

Quise probar entrar a la pc de Setubal, pero no estoy encontrando la clave. Eso es una paja.
Voy a ver si puedo recuperar esa clave de alguna forma.

Por otro lado, cambié los archivos de Python para que tomen en cuenta que los datos que armo
se hacen sobre la base de que haya dos parámetros que se barren. Por tanto decidí nombrar de
forma general a estos parámetros 1 y 2, y ahora los archivos reciben unos strings que utilizan para
poner nombre en los gráficos y en los archivos a estos parámetros, pero en el interior operan
considerándolos simplemente parámetro 1 y 2. Lo importante es que el parámetro 1 es el que se
grafica en el eje Y mientras que el 2 es el que se grafica en el eje X. Lo interesante es que podría
cambiar cuál va al eje X y cuál al eje Y simplemente cambiando los nombres en el armado del dataframe.
Creo, podría eventualmente probarlo.

Y ahora no puedo mandar a correr nada en Oporto tampoco, Fede copó el cluster.

------------------------------------------------------------------------------------------------

11/01/2023

Ya subí los archivos para armar la simulación en el caso de Saturación a la pc de oporto. Actualmente nadie está
usando el cluster. Así que mi plan es ocupar 19 hilos en simulaciones y uno en armar los gráficos con los datos
que estuve armando el otro día. Mi pregunta ahora es: No tengo una buena idea de cuál es la región en la que
debería investigar el modelo con saturación. Además, no creo que vaya a ver algo interesante en el gráfico
de promedio de opiniones, lo más seguro es que vea que todo cae a valores bajos de opinión, independiente
de todo. Voy a mover Epsilon entre 1 y 3 esta vez, porque como sé que el sistema va a caer, o por lo menos
sospecho que los intereses van a ir a cero, entonces me parece que lo mejor es usar un epsilon más chico
para permitir que el sistema pueda crecer un poco más. Y de ahí vemos qué pasa. Y el cosdelta lo voy
a mover entre 0 y 1 de a 0,1, como para medir qué pasa nomás. Debería tardar menos que lo anterior, porque
es bastante menos cantidad de simulaciones. Después si necesito, expando los datos haciendo los cálculos
para los cosdelta intermedios.

Ya mandé a armarse los datos en la pc de Oporto. En total estoy haciendo 11 (Cosdelta) * 21 (Epsilon) * 6 (iteraciones)
simulaciones en cada hilo. Eso significa 1386 simulaciones por hilo. Como cada simulación en promedio está
tardando 180 segundos, entonces eso son 70 horas aprox. Así que va a estar unos 3 días haciendo esto.
El finde tengo que mandar a hacer los gráficos, así el lunes ya tengo esto visto. Y también tengo que mandar
a hacer los datos en el caso con el término lineal. Podría hacer algunos de esos datos en la pc ahora.

Mientras se resuelven las simulaciones que mandé a hacer, lo que queda es ponerme a preparar el final de Mininni.
Avancé hasta la página 4 con lo de Mininni.

Por otro lado estuve hablando con Pablo sobre el TP, me dijo de armar una presentación para el lunes con el trabajo
hecho, hablamos sobre revisar el tema de por qué para alfa menor a 4 el sistema tiene una región de transición que
supuestamente no debería tener y también me propuso lo de ver si hay una relación entre kappa versus epsilon sobre alfa
o alfa a la algo. Esto está mejor anotado en el cuaderno. La pregunta resulta interesante, porque estaría bueno ver
si en verdad las tres curvas de transición que observo en realidad colapsan a una misma curva.
